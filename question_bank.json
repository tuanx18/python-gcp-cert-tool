[
  {
    "question_id": 1,
    "question": "A developer is planning a mobile application for your company’s customers to use to track information about their accounts. The developer is asking for your advice on storage technologies. In one case, the developer explains that they want to write messages each time a significant event occurs, such as the client opening, viewing, or deleting an account. This data is collected for compliance reasons, and the developer wants to minimize administrative overhead. What system would you recommend for storing this data?",
    "choice_a": "Cloud SQL using MySQL",
    "choice_b": "Cloud SQL using PostgreSQL",
    "choice_c": "Cloud Datastore",
    "choice_d": "Stackdriver Logging",
    "choice_e": null,
    "choice_f": null,
    "correct_answer": "D",
    "chapter": 1,
    "chapter_name": "Selecting Appropriate Storage Technologies",
    "topic": "Business Requirement for Storage System > Application Data",
    "reason": "Stackdriver Logging is a database to write application data. The data of event such as client opening or viewing is “Transactional” data, Stackdriver logging is one system to store this kind of data.",
    "point": 1,
    "timer": 90,
    "multiple_choices": false
  },

  {
    "question_id": 2,
    "question": "You are responsible for developing an ingestion mechanism for a large number of IoT sensors. The ingestion service should accept data up to 10 minutes late. The service should also perform some transformations before writing the data to a database. Which of the managed services would be the best option for managing late arriving data and performing transformations?",
    "choice_a": "Cloud Dataproc",
    "choice_b": "Cloud Dataflow",
    "choice_c": "Cloud Dataprep",
    "choice_d": "Cloud SQL",
    "correct_answer": "B",
    "chapter": 1,
    "chapter_name": "Selecting Appropriate Storage Technologies",
    "topic": "Business Requirement for Storage System > Data Transformations > Cloud Dataflow",
    "reason": "IoT sensor data is a kind of streaming data. Cloud Dataflow is well suited to transforming both stream and batch data. Cloud Dataflow supports both real-time and batch processing. This flexibility allows you to handle both streaming data from IoT sensors in near real-time and process late-arriving data as batches",
    "point": 2,
    "timer": 120,
    "multiple_choices": false
  },

  {
    "question_id": 3,
    "question": "A team of analysts has collected several CSV datasets with a total size of 50 GB. They plan to store the datasets in GCP and use Compute Engine instances to run RStudio, an interactive statistical application. Data will be loaded into RStudio using an RStudio data loading tool. Which of the following is the most appropriate GCP storage service for the datasets?",
    "choice_a": "Cloud Storage",
    "choice_b": "Cloud Datastore",
    "choice_c": "MongoDB",
    "choice_d": "Bigtable",
    "correct_answer": "A",
    "chapter": 1,
    "chapter_name": "Selecting Appropriate Storage Technologies",
    "topic": "Technical Aspects of Data > Volume",
    "reason": "Each data in the file is treated as an atomic unit that is loaded, so it counts as Unstructured data in the storage, so the Cloud Storage would be the best option",
    "point": 1,
    "timer": 90,
    "multiple_choices": false
  },

  {
    "question_id": 4,
    "question": "A team of analysts has collected several terabytes of telemetry data in CSV datasets. They plan to store the datasets in GCP and query and analyze the data using SQL. Which of the following is the most appropriate GCP storage service for the datasets",
    "choice_a": "Cloud SQL",
    "choice_b": "Cloud Spanner",
    "choice_c": "BigQuery",
    "choice_d": "Bigtable",
    "correct_answer": "C",
    "chapter": 1,
    "chapter_name": "Selecting Appropriate Storage Technologies",
    "topic": "Storage Decision Tree",
    "reason": "If the data is structured and it is the analytical data type, the best storage would be BigQuery",
    "point": 1,
    "timer": 90,
    "multiple_choices": false
  },

  {
    "question_id": 5,
    "question": "You have been hired to consult with a startup that is developing software for self-driving vehicles. The company’s product uses machine learning to predict the trajectory of persons and vehicles. Currently, the software is being developed using 20 vehicles, all located in the same city. IoT data is sent from vehicles every 60 seconds to a MySQL database running on a Compute Engine instance using an n2-standard-8 machine type with 8 vCPUs and 16 GB of memory. The startup wants to review their architecture and make any necessary changes to support tens of thousands of self-driving vehicles, all transmitting IoT data every second. The vehicles will be located across North America and Europe. Approximately 4 KB of data is sent in each transmission. What changes to the architecture would you recommend?",
    "choice_a": "None. The current architecture is well suited to the use case",
    "choice_b": "Replace Cloud SQL with Cloud Spanner",
    "choice_c": "Replace Cloud SQL with Bigtable",
    "choice_d": "Replace Cloud SQL with Cloud Datastore",
    "correct_answer": "C",
    "chapter": 1,
    "chapter_name": "Selecting Appropriate Storage Technologies",
    "topic": "Storage Decision Tree",
    "reason": "High velocity data such as IoT data sent in term of minutes, are not suitable for Cloud Spanner, even if it is globally distributed. It should be BigTable, because BigTable can handle large amount of data and also in global",
    "point": 1,
    "timer": 90,
    "multiple_choices": false
  },

  {
    "question_id": 6,
    "question": "As a member of a team of game developers, you have been tasked with devising a way to track players’ possessions. Possessions may be purchased from a catalog, traded with other players, or awarded for game activities. Possessions are categorized as clothing, tools, books, and coins. Players may have any number of possessions of any type. Players can search for other players who have particular possession types to facilitate trading. The game designer has informed you that there will likely be new types of possessions and ways to acquire them in the future. What kind of a data store would you recommend using?",
    "choice_a": "Transactional database",
    "choice_b": "Wide-column database",
    "choice_c": "Document database",
    "choice_d": "Analytic database",
    "correct_answer": "C",
    "chapter": 1,
    "chapter_name": "Selecting Appropriate Storage Technologies",
    "topic": "Schema Design Considerations > NoSQL > Document Databases",
    "reason": "A document databases store allow complex data structures called documents, to be used as values and accessed in more ways than simple key lookup. Consider an online game that requires a database to store information about players’ game state.",
    "point": 1,
    "timer": 90,
    "multiple_choices": false
  },

  {
    "question_id": 7,
    "question": "The CTO of your company wants to reduce the cost of running an HBase and Hadoop cluster on premises. Only one HBase application is run on the cluster. The cluster currently supports 10 TB of data, but it is expected to double in the next six months. Which of the following managed services would you recommend to replace the on-premises cluster in order to minimize migration and ongoing operational costs?",
    "choice_a": "Cloud Bigtable using the HBase API",
    "choice_b": "Cloud Dataflow using the HBase API",
    "choice_c": "Cloud Spanner",
    "choice_d": "Cloud Datastore",
    "correct_answer": "A",
    "chapter": 1,
    "chapter_name": "Selecting Appropriate Storage Technologies",
    "topic": "Schema Design Considerations > NoSQL > Wide-Column Databases",
    "reason": "Bigtable is GCP’s managed wide-column database. It is also a good option for migrating on-premises Hadoop HBase databases to a managed database because Bigtable has an HBase interface",
    "point": 1,
    "timer": 90,
    "multiple_choices": false
  },

  {
    "question_id": 8,
    "question": "A genomics research institute is developing a platform for analyzing data related to genetic diseases. The genomics data is in a specialized format known as FASTQ, which stores nucleotide sequences and quality scores in a text format. Files may be up to 400 GB and are uploaded in batches. Once the files finish uploading, an analysis pipeline runs, reads the data in the FASTQ file, and outputs data to a database. What storage system is a good option for storing the uploaded FASTQ data?",
    "choice_a": "Cloud Bigtable",
    "choice_b": "Cloud Datastore",
    "choice_c": "Cloud Storage",
    "choice_d": "Cloud Spanner",
    "correct_answer": "C",
    "chapter": 1,
    "chapter_name": "Selecting Appropriate Storage Technologies",
    "topic": "Storage Decision Tree",
    "reason": "Unstructured data that goes in large volume should best be stored in Cloud Storage service",
    "point": 1,
    "timer": 90,
    "multiple_choices": false
  },

  {
    "question_id": 9,
    "question": "A genomics research institute is developing a platform for analyzing data related to genetic diseases. The genomics data is in a specialized format known as FASTQ, which stores nucleotide sequences and quality scores in a text format. Once the files finish uploading, an analysis pipeline runs, reads the data in the FASTQ file, and outputs data to a database. The output is in tabular structure, the data is queried using SQL, and typically queries retrieve only a small number of columns but many rows. What database would you recommend for storing the output of the workflow?",
    "choice_a": "Cloud Bigtable",
    "choice_b": "Cloud Datastore",
    "choice_c": "Cloud Storage",
    "choice_d": "BigQuery",
    "correct_answer": "D",
    "chapter": 1,
    "chapter_name": "Selecting Appropriate Storage Technologies",
    "topic": "Storage Decision Tree",
    "reason": "The data is 'analytical' and in 'tabular structure' (structured data). The only service for this would be BigQuery",
    "point": 1,
    "timer": 90,
    "multiple_choices": false
  },

  {
    "question_id": 10,
    "question": "You are developing a new application and will be storing semi-structured data that will only be accessed by a single key. The total volume of data will be at least 40 TB. What GCP database service would you use?",
    "choice_a": "BigQuery",
    "choice_b": "Bigtable",
    "choice_c": "Cloud Spanner",
    "choice_d": "Cloud SQL",
    "correct_answer": "B",
    "chapter": 1,
    "chapter_name": "Selecting Appropriate Storage Technologies",
    "topic": "Storage Decision Tree",
    "reason": "Bigtable is used for storing semi-structured data while all the other 3 options are structured data storage",
    "point": 1,
    "timer": 90,
    "multiple_choices": false
  },

  {
    "question_id": 11,
    "question": "A group of climate scientists is collecting weather data every minute from 10,000 sensors across the globe. Data often arrives near the beginning of a minute, and almost all data arrives within the first 30 seconds of a minute. The data ingestion process is losing some data because servers cannot ingest the data as fast as it is arriving. The scientists have scaled up the number of servers in their managed instance group, but that has not completely eliminated the problem. They do not wish to increase the maximum size of the managed instance group. What else can the scientists do to prevent data loss?",
    "choice_a": "Write data to a Cloud Dataflow stream",
    "choice_b": "Write data to a Cloud Pub/Sub topic",
    "choice_c": "Write data to Cloud SQL table",
    "choice_d": "Write data to Cloud Dataprep",
    "correct_answer": "B",
    "chapter": 1,
    "chapter_name": "Selecting Appropriate Storage Technologies",
    "topic": "Technical Aspects of Data - Velocity",
    "reason": "The weather data is collected every minute from 10000 sensors, which is a giant amount ingestion in a very short amount of time, which called High Velocity. y data and can write up to 10,000 rows per second using a 10-node cluster with SSDs. When high-velocity data is processed as it is ingested, it is a good practice to write the data to a Cloud Pub/Sub topic. The processing application can then use a pull subscription to read the data at a rate that it can sustain. Cloud Pub/Sub is a scalable, managed messaging service that scales automatically",
    "point": 2,
    "timer": 90,
    "multiple_choices": false
  },

  {
    "question_id": 12,
    "question": "A software developer asks your advice about storing data. The developer has hundreds of thousands of 1 KB JSON objects that need to be accessed in sub-millisecond times if possible. All objects are referenced by a key. There is no need to look up values by the contents of the JSON structure. What kind of NoSQL database would you recommend?",
    "choice_a": "Key-value database",
    "choice_b": "Analytical database",
    "choice_c": "Wide-column database",
    "choice_d": "Graph database",
    "correct_answer": "A",
    "chapter": 1,
    "chapter_name": "Selecting Appropriate Storage Technologies",
    "topic": "Schema Design Considerations > NoSQL > Key-Value Data Stores",
    "reason": "A NoSQL where all objects can be referenced by a key is the Key-Value NoSQL Databases",
    "point": 1,
    "timer": 90,
    "multiple_choices": false
  },

  {
    "question_id": 13,
    "question": "A software developer asks your advice about storing data. The developer has hundreds of thousands of 10 KB JSON objects that need to be searchable by most attributes in the JSON structure. What kind of NoSQL database would you recommend?",
    "choice_a": "Key-value database",
    "choice_b": "Analytical database",
    "choice_c": "Wide-column database",
    "choice_d": "Document database",
    "correct_answer": "D",
    "chapter": 1,
    "chapter_name": "Selecting Appropriate Storage Technologies",
    "topic": "Schema Design Considerations > NoSQL > Document Databases",
    "reason": "Only in Document Database can you search for JSON objects by most attributes",
    "point": 1,
    "timer": 90,
    "multiple_choices": false
  },

  {
    "question_id": 14,
    "question": "A data modeler is designing a database to support ad hoc querying, including drilling down and slicing and dicing queries. What kind of data model is the data modeler likely to use?",
    "choice_a": "OLTP",
    "choice_b": "OLAP",
    "choice_c": "Normalized",
    "choice_d": "Graph",
    "correct_answer": "B",
    "chapter": 1,
    "chapter_name": "Selecting Appropriate Storage Technologies",
    "topic": "Schema Design Considerations > OLAP",
    "reason": "OLAP models are designed to facilitate the following:\n■ Rolling up and aggregating data\n■ Drilling down from summary data to detailed data\n■ Pivoting and looking at data from different dimensions—sometimes called slicing and dicing.",
    "point": 1,
    "timer": 90,
    "multiple_choices": false
  },

  {
    "question_id": 15,
    "question": "A multinational corporation is building a global inventory database. The database will support OLTP type transactions at a global scale. Which of the following would you consider as possible databases for the system?",
    "choice_a": "Cloud SQL and Cloud Spanner",
    "choice_b": "Cloud SQL and Cloud Datastore",
    "choice_c": "Cloud Spanner Only",
    "choice_d": "Cloud Datastore only",
    "correct_answer": "C",
    "chapter": 1,
    "chapter_name": "Selecting Appropriate Storage Technologies",
    "topic": "Storage Decision Tree",
    "reason": "Cloud Spanner is a Globally distributed storage. When it comes to building a global inventory database, it would be no option rather than Cloud Spanner",
    "point": 2,
    "timer": 120,
    "multiple_choices": false
  },

  {
    "question_id": 16,
    "question": "Which Google Cloud Storage service can be used for Tabular Structured Data?",
    "choice_a": "Cloud Spanner",
    "choice_b": "Bigtable",
    "choice_c": "BigQuery",
    "choice_d": "Cloud Datastore",
    "choice_e": "Cloud Datashare",
    "choice_f": "Cloud Storage",
    "choice_g": "Cloud SQL",
    "choice_h": "Cloud Firestore",
    "correct_answer": ["A", "C", "G"],
    "chapter": 1,
    "chapter_name": "Selecting Appropriate Storage Technologies",
    "topic": "Storage Decision Tree",
    "reason": "Cloud Spanner, Cloud SQL and Cloud BigQuery are the 3 services in GCP that supports storing structured data",
    "point": 2,
    "timer": 120,
    "multiple_choices": true
  },

  {
    "question_id": 17,
    "question": "Which Google Cloud Storage service can be used for Semi-Structured Data?",
    "choice_a": "Cloud Spanner",
    "choice_b": "Bigtable",
    "choice_c": "BigQuery",
    "choice_d": "Cloud Datastore",
    "choice_e": "Cloud Datashare",
    "choice_f": "Cloud Storage",
    "choice_g": "Cloud SQL",
    "choice_h": "Cloud Firestore",
    "correct_answer": ["B", "D", "H"],
    "chapter": 1,
    "chapter_name": "Selecting Appropriate Storage Technologies",
    "topic": "Storage Decision Tree",
    "reason": "Bigtable, Datastore and Firestore are the 3 services in GCP that supports storing semi-structured data",
    "point": 2,
    "timer": 120,
    "multiple_choices": true
  },

    {
    "question_id": 18,
    "question": "Which services does not come from Google Cloud Platform?",
    "choice_a": "BigQuery",
    "choice_b": "S3",
    "choice_c": "RedShift",
    "choice_d": "Datastore",
    "choice_e": "Oracle",
    "choice_f": "Lambda",
    "choice_g": "Cloud SQL",
    "choice_h": "Firestore",
    "correct_answer": ["B", "C", "E", "F"],
    "chapter": 1,
    "chapter_name": "Selecting Appropriate Storage Technologies",
    "topic": "Storage Decision Tree",
    "reason": "BigQuery, Datastore, Cloud SQL and Firestore are names of GCP services",
    "point": 2,
    "timer": 120,
    "multiple_choices": true
  },

  {
    "question_id": 19,
    "question": "A database administrator (DBA) who is new to Google Cloud has asked for your help configuring network access to a Cloud SQL PostgreSQL database. The DBA wants to ensure that traffic is encrypted while minimizing administrative tasks, such as managing SQL certificates. What would you recommend?",
    "choice_a": "Use the TLS protocol",
    "choice_b": "Use Cloud SQL Proxy",
    "choice_c": "Use a private IP address",
    "choice_d": "Configure the database instance to use auto-encryption",
    "correct_answer": "B",
    "chapter": 2,
    "chapter_name": "Building and Operationalizing Storage Systems",
    "topic": "Cloud SQL > Configuring Cloud SQL",
    "reason": "Cloud SQL Proxy provides secure access to Second Generation instances without having to create allow lists or to configure SSL. The proxy manages authentication and automatically encrypts data",
    "point": 1,
    "timer": 90,
    "page": 62,
    "multiple_choices": false
  },

  {
    "question_id": 20,
    "question": "You created a Cloud SQL database that uses replication to improve read performance. Occasionally, the read replica will be unavailable. You haven’t noticed a pattern, but the disruptions occur once or twice a month. No DBA operations are occurring when the incidents occur. What might be the cause of this issue?",
    "choice_a": "The read replica is being promoted to a standalone Cloud SQL instance",
    "choice_b": "Maintenance is occurring on the read replica",
    "choice_c": "A backup is being performed on the read replica",
    "choice_d": "The primary Cloud SQL instance is failing over to the read replica",
    "correct_answer": "B",
    "chapter": 2,
    "chapter_name": "Building and Operationalizing Storage Systems",
    "topic": "Cloud SQL > Improving Read Performance with Read Replicas",
    "reason": "Sometimes, the read replica is unavailable on no patterns at all, this is due to the Read Replica Maintenance. Maintenance on read replicas is not limited to maintenance windows - it can occur at anytime and disrupt operations",
    "point": 1,
    "timer": 90,
    "page": 64,
    "multiple_choices": false
  },

  {
    "question_id": 21,
    "question": "Your department is experimenting with using Cloud Spanner for a globally accessible database. You are starting with a pilot project using a regional instance. You would like to follow Google’s recommendations for the maximum sustained CPU utilization of a regional instance. What is the maximum CPU utilization that you would target?",
    "choice_a": "50%",
    "choice_b": "65%",
    "choice_c": "75%",
    "choice_d": "45%",
    "correct_answer": "B",
    "chapter": 2,
    "chapter_name": "Building and Operationalizing Storage Systems",
    "topic": "Cloud Spanner > Configuring Cloud Spanner",
    "reason": "When configuring Cloud Spanner, Google recommends keeping CPU utilization:\n■\tBelow 65% in Regional Instance\n■\tBelow 45% in Multi-Regional Instance",
    "point": 1,
    "timer": 90,
    "page": 65,
    "multiple_choices": false
  },

  {
    "question_id": 22,
    "question": "As the founder of a company embarking on the integration of Cloud Spanner for a globally accessible database, you're initiating a pilot project utilizing a multi-regional instance. Seeking adherence to Google's guidelines, what is the recommended threshold for maximum sustained CPU utilization that you aim to achieve for the MULTI-REGIONAL instance?",
    "choice_a": "80%",
    "choice_b": "25%",
    "choice_c": "45%",
    "choice_d": "65%",
    "correct_answer": "C",
    "chapter": 2,
    "chapter_name": "Building and Operationalizing Storage Systems",
    "topic": "Cloud Spanner > Configuring Cloud Spanner",
    "reason": "When configuring Cloud Spanner, Google recommends keeping CPU utilization:\n■\tBelow 65% in Regional Instance\n■\tBelow 45% in Multi-Regional Instance",
    "point": 1,
    "timer": 90,
    "page": 65,
    "multiple_choices": false
  },

  {
    "question_id": 23,
    "question": "Auditors have informed your company CFO that to comply with a new regulation, your company will need to ensure that financial reporting data is kept for at least three years. The CFO asks for your advice on how to comply with the regulation with the least administrative overhead. What would you recommend?",
    "choice_a": "Store the data on Coldline storage",
    "choice_b": "Store the data on multi-regional storage",
    "choice_c": "Define a data retention policy",
    "choice_d": "Define a lifecycle policy",
    "correct_answer": "C",
    "chapter": 2,
    "chapter_name": "Building and Operationalizing Storage Systems",
    "topic": "Cloud Storage > Data Retention",
    "reason": "A retention policies uses the Bucket Lock feature of Cloud Storage buckets to enforce object retention. By setting a retention policy, you can ensure that any object in the bucket or future objects in the bucket are not deleted until they reach the age specified in the retention policy",
    "point": 1,
    "timer": 90,
    "page": 84,
    "multiple_choices": false
  },

  {
    "question_id": 24,
    "question": "A team of machine learning engineers are creating a repository of data for training and testing machine learning models. All of the engineers work in the same city, and they all contribute datasets to the repository. The data files will be accessed frequently, usually at least once a week. The data scientists want to minimize their storage costs. They plan to use Cloud Storage; what storage class would you recommend? ",
    "choice_a": "Regional",
    "choice_b": "Multi-regional",
    "choice_c": "Nearline",
    "choice_d": "Coldline",
    "correct_answer": "A",
    "chapter": 2,
    "chapter_name": "Building and Operationalizing Storage Systems",
    "topic": "Cloud Storage > Storage Tiers",
    "reason": "Regional Storage is suitable for frequently accessed data where low-latency access is essential, and the data can be accessed frequently",
    "point": 1,
    "timer": 90,
    "page": 82,
    "multiple_choices": false
  },

  {
    "question_id": 25,
    "question": "You are running a Redis cache using Cloud Memorystore. One day, you receive an alert notification that the memory usage is exceeding 80 percent. You do not want to scale up the instance, but you need to reduce the amount of memory used. What could you try? ",
    "choice_a": "Setting shorter TTLs and trying a different eviction policy.",
    "choice_b": "Switching from Basic Tier to Standard Tier.",
    "choice_c": "Exporting the cache.",
    "choice_d": "There is no other option—you must scale the instance.",
    "correct_answer": "A",
    "chapter": 2,
    "chapter_name": "Building and Operationalizing Storage Systems",
    "topic": "Cloud Memorystore",
    "reason": "When the memory used by Redis exceed 80%:\n\uF06C\tScale up the instance\n\uF06C\tLower maximum memory limit\n\uF06C\tModify eviction policy\n\uF06C\tSet time-to-live (TTL)\n",
    "point": 1,
    "timer": 90,
    "multiple_choices": false
  },

  {
    "question_id": 26,
    "question": "You are querying a Cloud Firestore collection of order entities searching for all orders that were created today and have a total sales amount of greater than $100. You have not excluded any indexes, and you have not created any additional indexes using index.yaml. What do you expect the results to be? ",
    "choice_a": "A set of all orders created today with a total sales amount greater than $100",
    "choice_b": "A set of orders created today and any total sales amount",
    "choice_c": "A set of with total sales amount greater than $100 and any sales date\t",
    "choice_d": "No entities returned",
    "correct_answer": "D",
    "chapter": 2,
    "chapter_name": "Building and Operationalizing Storage Systems",
    "topic": "Cloud Firestore > Indexing and Querying",
    "reason": "If you attempt to execute a query without having a suitable composite index defined (index.yaml), Firestore will not allow the query to proceed, and you will receive an error.",
    "point": 1,
    "timer": 90,
    "page": 72,
    "multiple_choices": false
  },

  {
    "question_id": 27,
    "question": "A team of game developers is using Cloud Firestore to store player data, including character description, character state, and possessions. Descriptions are up to a 60-character alphanumeric string that is set when the character is created and not updated. Character state includes health score, active time, and passive time. When they are updated, they are all updated at the same time. Possessions are updated whenever the character acquires or loses a possession. Possessions may be complex objects, such as bags of items, where each item may be a simple object or another complex object. Simple objects are described with a character string. Complex objects have multiple properties. How would you model player data in Cloud Firestore? ",
    "choice_a": "Store description and character state as strings and possessions as entities",
    "choice_b": "Store description, character state, and possessions as strings",
    "choice_c": "Store description, character state, and possessions as entities",
    "choice_d": "Store description as a string; character state as an entity with properties for health score, active time, and passive time; and possessions as an entity that may have embedded entities",
    "correct_answer": "D",
    "chapter": 2,
    "chapter_name": "Building and Operationalizing Storage Systems",
    "topic": "Cloud Firestore > Data Model",
    "reason": "Description is a strings because they refers to each character and can’t be grouped so they cannot be entities. Character state can be grouped, so it certainly be an entity with properties for health score, active time and passive time. And possessions should be an entity, but with embedded entities for items within a bag",
    "point": 1,
    "timer": 90,
    "page": 71,
    "multiple_choices": false
  },

  {
    "question_id": 28,
    "question": "A software-as-a-service (SaaS) company specializing in automobile IoT sensors collects streaming time-series data from tens of thousands of vehicles. The vehicles are owned and operated by 40 different companies, who are the primary customers of the SaaS company. The data will be stored in Bigtable using a multitenant database; that is, all customer data will be stored in the same database. The data sent from the IoT device includes a sensor ID, which is globally unique; a timestamp; and several metrics about engine efficiency. Each customer will query their own data only. Which of the following would you use as a row-key?",
    "choice_a": "Customer ID, timestamp, sensor ID",
    "choice_b": "Customer ID, sensor ID, timestamp",
    "choice_c": "Sensor ID, timestamp, customer ID",
    "choice_d": "Sensor ID, customer ID, timestamp",
    "correct_answer": "B",
    "chapter": 2,
    "chapter_name": "Building and Operationalizing Storage Systems",
    "topic": "Cloud Bigtable > Database Design Considerations",
    "reason": "The database is multi-tenant, so each tenant, which is each customer, will perform query on their own data. When designing a row-key in Bigtable, you should consider using non-sequential value in the first part of the row key, which helps avoid hotspots. ",
    "point": 1,
    "timer": 90,
    "page": 70,
    "multiple_choices": false
  },

  {
    "question_id": 29,
    "question": "A Cloud Spanner database is being deployed in us-west1 and will have to store up to 20 TB of data. What is the minimum number of nodes required?",
    "choice_a": "10",
    "choice_b": "20",
    "choice_c": "5",
    "choice_d": "40",
    "correct_answer": "A",
    "chapter": 2,
    "chapter_name": "Building and Operationalizing Storage Systems",
    "topic": "Cloud Spanner > Configuring Cloud Spanner",
    "reason": "Each node in Cloud Spanner database can store up to 2 TB of data. With 20 TB of data, it requires at least 10 nodes for configuration",
    "point": 1,
    "timer": 90,
    "page": 65,
    "multiple_choices": false
  },

  {
    "question_id": 30,
    "question": "For the deployment of a Cloud Spanner database in the us-west1 region, tasked with accommodating a storage load of up to 54 TB, what is the essential minimum count of nodes needed for optimal performance?",
    "choice_a": "54",
    "choice_b": "18",
    "choice_c": "108",
    "choice_d": "27",
    "correct_answer": "D",
    "chapter": 2,
    "chapter_name": "Building and Operationalizing Storage Systems",
    "topic": "Cloud Spanner > Configuring Cloud Spanner",
    "reason": "Each node in Cloud Spanner database can store up to 2 TB of data. With 54 TB of data, it requires at least 27 nodes for configuration",
    "point": 1,
    "timer": 90,
    "page": 65,
    "multiple_choices": false
  },

  {
    "question_id": 32,
    "question": "As a database administrator tasked with migrating a MongoDB instance to Google Cloud, you are concerned about your ability to configure the database optimally. You want to collect metrics at both the instance level and the database server level. What would you do in addition to creating an instance and installing and configuring MongoDB to ensure that you can monitor key instances and database metrics?",
    "choice_a": "Install Stackdriver Logging agent.",
    "choice_b": "Install Stackdriver Monitoring agent.",
    "choice_c": "Install Stackdriver Debug agent",
    "choice_d": "Nothing. By default, the database instance will send metrics to Stackdriver",
    "correct_answer": "B",
    "chapter": 2,
    "chapter_name": "Building and Operationalizing Storage Systems",
    "topic": "Unmanaged Databases",
    "reason": "Stackdriver Monitoring agent can collect application performance metrics, then send to Stackdriver for alerting and enchanting",
    "point": 1,
    "timer": 90,
    "page": 85,
    "multiple_choices": false
  },

  {
    "question_id": 33,
    "question": "A group of data scientists have uploaded multiple time-series datasets to BigQuery over the last year. They have noticed that their queries—which select up to six columns, apply four SQL functions, and group by the day of a timestamp—are taking longer to run and are incurring higher BigQuery costs as they add data. They do not understand why this is the case since they typically work only with the most recent set of data loaded. What would you recommend they consider in order to reduce query latency and query costs?",
    "choice_a": "Sort the data by time order before loading",
    "choice_b": "Stop using Legacy SQL and use Standard SQL dialect",
    "choice_c": "Partition the table and use clustering",
    "choice_d": "Add more columns to the SELECT statement to use data fetched by BigQuery more efficiently",
    "correct_answer": "C",
    "chapter": 2,
    "chapter_name": "Building and Operationalizing Storage Systems",
    "topic": "BigQuery > Clustering, Partitioning and Sharding Tables",
    "reason": "Partitioning the table will enable BigQuery to scan only data in the partition part, and clustering will improve the way column of data is stored",
    "point": 1,
    "timer": 90,
    "page": 76,
    "multiple_choices": false
  },

  {
    "question_id": 34,
    "question": "You are querying a BigQuery table that has been partitioned by time. You create a query and use the --dry_run flag with the bq query command. The amount of data scanned is far more than you expected. What is a possible cause of this?",
    "choice_a": "You did not include _PARTITIONTIME in the WHERE clause to limit the amount of data that needs to be scanned.",
    "choice_b": "You used CSV instead of AVRO file format when loading the data.",
    "choice_c": "Both active and long-term data are included in the query results.",
    "choice_d": "You used JSON instead of the Parquet file format when loading the data.",
    "correct_answer": "A",
    "chapter": 2,
    "chapter_name": "Building and Operationalizing Storage Systems",
    "topic": "BigQuery > Tips for Optimizing",
    "reason": "In BigQuery, when using dry_run without _PARTITIONTIME, all data from the table will be scanned rather than just the limit timestamp. With _PARTITIONTIME, the only data that would be scanned is the data within the time range",
    "point": 1,
    "timer": 90,
    "page": 79,
    "multiple_choices": false
  },

  {
    "question_id": 35,
    "question": "Your department is planning to expand the use of BigQuery. The CFO has asked you to investigate whether the company should invest in flat-rate billing for BigQuery. What tools and data would you use to help answer that question?",
    "choice_a": "Stackdriver Logging and audit log data",
    "choice_b": "Stackdriver Logging and CPU utilization metrics",
    "choice_c": "Stackdriver Monitoring and CPU utilization metrics",
    "choice_d": "Stackdriver Monitoring and slot utilization metrics",
    "correct_answer": "D",
    "chapter": 2,
    "chapter_name": "Building and Operationalizing Storage Systems",
    "topic": "BigQuery > Monitoring and Logging in BigQuery",
    "reason": "Stackdriver Monitoring collects metrics, and the slot metrics are the ones that show resource utilization related to queries.",
    "point": 1,
    "timer": 90,
    "page": 77,
    "multiple_choices": false
  },

  {
    "question_id": 36,
    "question": "You are migrating several terabytes of historical sensor data to Google Cloud Storage. The data is organized into files with one file per sensor per day. The files are named with the date followed by the sensor ID. After loading 10 percent of the data, you realize that the data loads are not proceeding as fast as expected. What might be the cause?",
    "choice_a": "The filenaming convention uses dates as the first part of the file name. If the files are loaded in this order, they may be creating hotspots when writing the data to Cloud Storage.",
    "choice_b": "The data is in text instead of Avro or Parquet format.",
    "choice_c": "You are using a gcloud command-line utility instead of the REST API",
    "choice_d": "The data is being written to regional instead of multi-regional storage",
    "correct_answer": "A",
    "chapter": 2,
    "chapter_name": "Building and Operationalizing Storage Systems",
    "topic": "Cloud Bigtable > Database Design Considerations",
    "reason": "A row-key in wide-column NoSQL should not have a timestamp/date as the first part (date is a sequential value), it could leads to bad performance and create hotspots",
    "point": 1,
    "timer": 90,
    "page": 70,
    "multiple_choices": false
  },

  {
    "question_id": 37,
    "question": "Which GCP services below support 'Streaming Inserts?'",
    "choice_a": "Cloud Dataproc",
    "choice_b": "BigQuery",
    "choice_c": "Amazon Managed Streaming for Apache Kafka (Amazon MSK)",
    "choice_d": "Cloud Pub/Sub",
    "choice_e": "Cloud Memorystore",
    "correct_answer": ["B", "D"],
    "chapter": 2,
    "chapter_name": "Building and Operationalizing Storage Systems",
    "topic": "BigQuery > Streaming Inserts",
    "reason": "BigQuery is a fully-managed, serverless data warehouse that supports real-time analytics. You can stream data into BigQuery tables using the streaming API. \nCloud Pub/Sub is a messaging service that enables you to send and receive messages between independent applications. While it's not a storage service itself, it is often used in conjunction with other services for streaming data",
    "point": 1,
    "timer": 90,
    "page": 77,
    "multiple_choices": true
  },

  {
    "question_id": 38,
    "question": "What is the database that Cloud SQL does not support?",
    "choice_a": "MySQL",
    "choice_b": "Azure SQL Database",
    "choice_c": "PostgreSQL",
    "choice_d": "SQL Server",
    "correct_answer": "B",
    "chapter": 2,
    "chapter_name": "Building and Operationalizing Storage Systems",
    "topic": "Cloud SQL",
    "reason": "The 4 databases that Cloud SQL supports are: MySQL, PostgreSQL, SQL Server and MariaDB",
    "point": 1,
    "timer": 90,
    "page": 61,
    "multiple_choices": false
  },

  {
    "question_id": 39,
    "question": "You are tasked with optimizing your queries in BigQuery to improve performance and manage costs effectively. Which of the following methods would you consider for optimizing BigQuery?",
    "choice_a": "Use --dry-run to estimate the cost of a query before executing it",
    "choice_b": "Set the maximum number of bytes billed to control costs and prevent unexpected expenses.",
    "choice_c": "Increase the number of concurrent queries to utilize available resources better.",
    "choice_d": "Optimize queries by using SELECT * to retrieve all columns in the result set.",
    "choice_e": "Use unpartitioned tables for simplicity and ease of management.",
    "choice_f": "Partition by time when possible to enhance query performance.",
    "correct_answer": ["A", "B", "F"],
    "chapter": 2,
    "chapter_name": "Building and Operationalizing Storage Systems",
    "topic": "BigQuery > Tips for Optimizing BigQuery",
    "reason": "One way to keep costs down is to optimize the way that you use BigQuery. Here are several ways to do this:\n■ Avoid using SELECT *.\n■ Use --dry-run to estimate the cost of a query.\n■ Set the maximum number of bytes billed.\n■ Partition by time when possible.\n■ Denormalize data rather than join multiple tables",
    "point": 1,
    "timer": 90,
    "page": 78,
    "multiple_choices": true
  },

  {
    "question_id": 40,
    "question": "A large enterprise using GCP has recently acquired a startup that has an IoT platform. The acquiring company wants to migrate the IoT platform from an on-premises data center to GCP and wants to use Google Cloud managed services whenever possible. What GCP service would you recommend for ingesting IoT data?",
    "choice_a": "Cloud Storage",
    "choice_b": "Cloud SQL",
    "choice_c": "Cloud Pub/Sub",
    "choice_d": "BigQuery streaming inserts",
    "correct_answer": "C",
    "chapter": 3,
    "chapter_name": "Designing Data Pipelines",
    "topic": "Overview of Data Pipelines > Data Pipeline Stages",
    "reason": "Cloud Pub/Sub is suitable for handling the large volume of messages generated by IoT devices. Furthermore, Streaming ingestion receives data in increments, typically a single record or small batches of records, that continuously flow into an ingestion endpoint, typically a Cloud Pub/Sub topic. \nIn summary, Cloud Pub/Sub is a perfect fit for such task:\n\tImplement notification systems\n\tHandling the large volume of messages generated by IoT devices\n\tCan be part of data integration workflows, allowing systems to communicate and exchange data\n\tEvent-driven systems\n\tReal-time Analytics",
    "point": 1,
    "timer": 90,
    "page": 93,
    "multiple_choices": false
  },

  {
    "question_id": 41,
    "question": "You are designing a data pipeline to populate a sales data mart. The sponsor of the project has had quality control problems in the past and has defined a set of rules for filtering out bad data before it gets into the data mart. At what stage of the data pipeline would you implement those rules?",
    "choice_a": "Ingestion",
    "choice_b": "Transformation",
    "choice_c": "Storage",
    "choice_d": "Analysis",
    "correct_answer": "B",
    "chapter": 3,
    "chapter_name": "Designing Data Pipelines",
    "topic": "Overview of Data Pipelines > Data Pipeline Stages",
    "reason": "In Transformation stage, actions such as filtering, substituting missing data, converting data types, dropping columns and adding columns will be performed",
    "point": 1,
    "timer": 90,
    "page": 94,
    "multiple_choices": false
  },

  {
    "question_id": 42,
    "question": "A team of data warehouse developers is migrating a set of legacy Python scripts that have been used to transform data as part of an ETL process. They would like to use a service that allows them to use Python and requires minimal administration and operations support. Which GCP service would you recommend?",
    "choice_a": "Cloud Dataproc",
    "choice_b": "Cloud Dataflow",
    "choice_c": "Cloud Spanner",
    "choice_d": "Cloud Dataprep",
    "correct_answer": "B",
    "chapter": 3,
    "chapter_name": "Designing Data Pipelines",
    "topic": "Overview of Data Pipelines > Type of Data Pipelines",
    "reason": "For Transformation Stage, the services can be used are Cloud Dataflow and Cloud Dataproc. Cloud Dataflow is a fully-managed service, enterprises do not need to administrate and operate much. While Dataproc required managing efforts.",
    "point": 1,
    "timer": 90,
    "page": 97,
    "multiple_choices": false
  },

  {
    "question_id": 43,
    "question": "You are using Cloud Pub/Sub to buffer records from an application that generates a stream of data based on user interactions with a website. The messages are read by another service that transforms the data and sends it to a machine learning model that will use it for training. A developer has just released some new code, and you notice that messages are sent repeatedly at 10-minute intervals. What might be the cause of this problem?",
    "choice_a": "The new code release changed the subscription ID",
    "choice_b": "The new code release changed the topic ID.",
    "choice_c": "The new code disabled acknowledgments from the consumer",
    "choice_d": "The new code changed the subscription from pull to push",
    "correct_answer": "C",
    "chapter": 3,
    "chapter_name": "Designing Data Pipelines",
    "topic": "GCP Pipeline Components > Cloud Pub/Sub",
    "reason": "If acknowledgments are disabled or not implemented correctly in the new code. The Pub/Sub service may assume that the message was not successfully processed and redeliver it. This could lead to messages being sent repeatedly at regular intervals, such as 10-min intervals mentioned.",
    "point": 3,
    "timer": 240,
    "page": 104,
    "multiple_choices": false
  },

  {
    "question_id": 44,
    "question": "It is considered a good practice to make your processing logic idempotent when consuming messages from a Cloud Pub/Sub topic. Why is that?",
    "choice_a": "Messages may be delivered multiple times",
    "choice_b": "Messages may be received out of order",
    "choice_c": "Messages may be delivered out of order",
    "choice_d": "A consumer service may need to wait extended periods of time between the delivery of messages",
    "correct_answer": "A",
    "chapter": 3,
    "chapter_name": "Designing Data Pipelines",
    "topic": "GCP Pipeline Components > Cloud Pub/Sub",
    "reason": "Pub/sub makes no guarantees that the order of message reception is the same as the publish order. In addition, messages can be delivered more than once. For these reasons, your processing logic should be idempotent; that is, the logic could be applied multiple times and still provide the same output.",
    "point": 2,
    "timer": 150,
    "page": 105,
    "multiple_choices": false
  },

  {
    "question_id": 45,
    "question": "A group of IoT sensors is sending streaming data to a Cloud Pub/Sub topic. A Cloud Dataflow service pulls messages from the topic and reorders the messages sorted by event time. A message is expected from each sensor every minute. If a message is not received from a sensor, the stream processing application should use the average of the values in the last four messages. What kind of window would you use to implement the missing data logic?",
    "choice_a": "Sliding window",
    "choice_b": "Tumbling window",
    "choice_c": "Extrapolation window",
    "choice_d": "Crossover window",
    "correct_answer": "A",
    "chapter": 3,
    "chapter_name": "Designing Data Pipelines",
    "topic": "Type of Data Pipelines > Sliding and Tumbling Windows",
    "reason": "Sliding windows allow the application to get the exact 4 values in the last four messages. While  Tumbling window are used when you want to aggregate data over a fixed period of time.",
    "point": 1,
    "timer": 90,
    "page": 100,
    "multiple_choices": false
  },

  {
    "question_id": 46,
    "question": "A financial institution is analyzing transaction data from multiple branches across the country in real-time. The transactions are processed through a streaming data pipeline, and the goal is to calculate the daily average transaction amount for each branch. The streaming data contains occasional spikes due to large transactions, and the analysis needs to be resilient to outliers. Which type of window would you recommend using to ensure that the daily average is computed effectively while minimizing the impact of outliers?",
    "choice_a": "Extrapolation window",
    "choice_b": "Crossover window",
    "choice_c": "Sliding window",
    "choice_d": "Tumbling window",
    "correct_answer": "D",
    "chapter": 3,
    "chapter_name": "Designing Data Pipelines",
    "topic": "Type of Data Pipelines > Sliding and Tumbling Windows",
    "reason": " In this case, each interval represents a day. Tumbling windows are well-suited for scenarios where you want to perform computations on distinct, non-overlapping chunks of data.",
    "point": 1,
    "timer": 160,
    "page": 100,
    "multiple_choices": false
  },

  {
    "question_id": 47,
    "question": "Your department is migrating some stream processing to GCP and keeping some on premises. You are tasked with designing a way to share data from on-premises pipelines that use Kafka with GPC data pipelines that use Cloud Pub/Sub. How would you do that?",
    "choice_a": "Use CloudPubSubConnector and Kafka Connect",
    "choice_b": "Stream data to a Cloud Storage bucket and read from there",
    "choice_c": "Write a service to read from Kafka and write to Cloud Pub/Sub",
    "choice_d": "Use Cloud Pub/Sub Import Service",
    "correct_answer": "A",
    "chapter": 3,
    "chapter_name": "Designing Data Pipelines",
    "topic": "Type of Data Pipelines > Kafka",
    "reason": "If you are migrating an on-premises services that uses Kafka and you want to replace self-managed Kafka with a managed service, then the Cloud Pub/Sub would meet that need. If you plan to continue to use Kafka, you can link Cloud Pub/Sub and Kafka using the Type of Data Pipelines > Sliding and Tumbling Windows, which is a bridge between the 2 messaging systems using Kafka Connect.\n",
    "point": 2,
    "timer": 120,
    "page": 106,
    "multiple_choices": false
  },

  {
    "question_id": 48,
    "question": "A team of developers wants to create standardized patterns for processing IoT data. Several teams will use these patterns. The developers would like to support collaboration and facilitate the use of patterns for building streaming data pipelines. What component should they use?",
    "choice_a": "Cloud Dataflow Python Scripts",
    "choice_b": "Cloud Dataproc PySpark jobs",
    "choice_c": "Cloud Dataflow templates",
    "choice_d": "Cloud Dataproc templates",
    "correct_answer": "C",
    "chapter": 3,
    "chapter_name": "Designing Data Pipelines",
    "topic": "Cloud Dataflow > Jobs and Templates",
    "reason": "In Google Cloud Dataflow, a template is a pre-defined and reusable pipeline that encapsulates a data processing workflow. Dataflow templates are created to package and parameterize Dataflow pipelines, allowing you to deploy the same data processing logic with different input parameters or configurations. Templates are particularly useful for repetitive or scheduled data processing tasks, as they provide a way to standardize and reuse workflows.",
    "point": 1,
    "timer": 90,
    "page": 107,
    "multiple_choices": false
  },

  {
    "question_id": 49,
    "question": "You need to run several map reduce jobs on Hadoop along with one Pig job and four PySpark jobs. When you ran the jobs on premises, you used the department’s Hadoop cluster. Now you are running the jobs in GCP. What configuration for running these jobs would you recommend?",
    "choice_a": "Create a single cluster and deploy Pig and Spark in the cluster",
    "choice_b": "Create one persistent cluster for the Hadoop jobs, one for the Pig job and one for the PySpark jobs",
    "choice_c": "Create one cluster for each job, and keep the cluster running continuously so that you do not need to start a new cluster for each job",
    "choice_d": "Create one cluster for each job and shut down the cluster when the job completes",
    "correct_answer": "D",
    "chapter": 3,
    "chapter_name": "Designing Data Pipelines",
    "topic": "Migrating Hadoop and Spark to GCP",
    "reason": "On-premises clusters are typically large persistent clusters that run multiple jobs. They can be complicated to configure and manage. In GCP, it is a best practice to use an ephemeral cluster for each job. This approach leads to less complicated configurations and reduced costs, since you are not storing persistent data on the cluster and not running the cluster for extended periods of time.",
    "point": 3,
    "timer": 200,
    "page": 112,
    "multiple_choices": false
  },

  {
    "question_id": 50,
    "question": "You are working with a group of genetics researchers analyzing data generated by gene sequencers. The data is stored in Cloud Storage. The analysis requires running a series of six programs, each of which will output data that is used by the next process in the pipeline. The final result set is loaded into BigQuery. What tool would you recommend for orchestrating this workflow?",
    "choice_a": "Cloud Composer",
    "choice_b": "Cloud Dataflow",
    "choice_c": "Apache Flink",
    "choice_d": "Cloud Dataproc",
    "choice_e": "Cloud Storage",
    "correct_answer": "A",
    "chapter": 3,
    "chapter_name": "Designing Data Pipelines",
    "topic": "Cloud Composer",
    "reason": "It is a good practice to migrate HBase databases to Bigtable, which provides consistent, scalable performance.",
    "point": 1,
    "timer": 90,
    "page": 112,
    "multiple_choices": false
  },

  {
    "question_id": 51,
    "question": "The business owners of a data warehouse have determined that the current design of the data warehouse is not meeting their needs. In addition to having data about the state of systems at certain points in time, they need to know about all the times that data changed between those points in time. What kind of data warehousing pipeline should be used to meet this new requirement?",
    "choice_a": "ETL",
    "choice_b": "ELT",
    "choice_c": "Extraction and load",
    "choice_d": "Change Data Capture",
    "correct_answer": "D",
    "chapter": 3,
    "chapter_name": "Designing Data Pipelines",
    "topic": "Data Warehousing Pipelines",
    "reason": "In Change Data Capture approach, each change in a source system is captured and recorded in a data store. This is helpful in cases where it is important to know all changes over time and not just the state of the database at the time of data extraction",
    "point": 1,
    "timer": 150,
    "page": 98,
    "multiple_choices": false
  },

  {
    "question_id": 52,
    "question": " In a data warehousing scenario where the transformation logic is complex, involving business rules and calculations, and the processed data is loaded into a structured schema, which pipeline approach is generally more appropriate?",
    "choice_a": "ETL",
    "choice_b": "ELT",
    "choice_c": "Extraction and load",
    "choice_d": "Change Data Capture",
    "correct_answer": "A",
    "chapter": 3,
    "chapter_name": "Designing Data Pipelines",
    "topic": "Data Warehousing Pipelines",
    "reason": "ETL, or Extract, Transform, Load, is suitable when complex transformation logic is required before loading data into a structured schema.",
    "point": 1,
    "timer": 150,
    "page": 98,
    "multiple_choices": false
  },

  {
    "question_id": 53,
    "question": "Consider a situation where your data warehouse is designed to store raw, unaltered data from source systems, and the transformations are performed directly in the data warehouse. Which pipeline approach aligns with this design philosophy?",
    "choice_a": "ETL",
    "choice_b": "ELT",
    "choice_c": "Extraction and load",
    "choice_d": "Change Data Capture",
    "correct_answer": "B",
    "chapter": 3,
    "chapter_name": "Designing Data Pipelines",
    "topic": "Data Warehousing Pipelines",
    "reason": "ELT is preferred when the data warehouse is designed to serve as a powerful processing engine, capable of transforming raw data into meaningful insights.",
    "point": 1,
    "timer": 150,
    "page": 98,
    "multiple_choices": false
  },

  {
    "question_id": 54,
    "question": "In a scenario where the data warehouse primarily focuses on loading raw data from various source systems into a centralized storage without significant transformation, which data warehousing pipeline approach is most suitable?",
    "choice_a": "ETL",
    "choice_b": "ELT",
    "choice_c": "Extraction and load",
    "choice_d": "Change Data Capture",
    "correct_answer": "C",
    "chapter": 3,
    "chapter_name": "Designing Data Pipelines",
    "topic": "Data Warehousing Pipelines",
    "reason": "Extraction and Load (Extraction and Load). This approach involves extracting data from source systems and loading it directly into the data warehouse without extensive transformations, making it suitable for scenarios where the emphasis is on consolidating raw data for storage.",
    "point": 1,
    "timer": 150,
    "page": 98,
    "multiple_choices": false
  },

  {
    "question_id": 55,
    "question": "A startup is designing a data processing pipeline for its IoT platform. Data from sensors will stream into a pipeline running in GCP. As soon as data arrives, a validation process, written in Python, is run to verify data integrity. If the data passes the validation, it is ingested; otherwise, it is discarded. What services would you use to implement the validation check and ingestion?",
    "choice_a": "Cloud Storage and Cloud Pub/Sub",
    "choice_b": "Cloud Functions and Cloud Pub/Sub",
    "choice_c": "Cloud Functions and BigQuery",
    "choice_d": "Cloud Storage and BigQuery",
    "correct_answer": "B",
    "chapter": 4,
    "chapter_name": "Designing a Data Processing Solution",
    "topic": "Design Infrastructure > Cloud Engines",
    "reason": "Cloud Functions can be written using JavaScript, Python and Go. Cloud Functions is a good option when you need to execute code in response to events that can occur at any time, such as uploading a file or calling a webhook. They are also useful when ingesting data using Cloud Pub/Sub, such as in an IoT ingestion pipeline.",
    "point": 1,
    "timer": 90,
    "page": 123,
    "multiple_choices": false
  },

  {
    "question_id": 56,
    "question": "To ensure high availability of a mission-critical application, your team has determined that it needs to run the application in multiple regions. If the application becomes unavailable in one region, traffic from that region should be routed to another region. Since you are designing a solution for this set of requirements, what would you expect to include?",
    "choice_a": "Cloud Storage bucket",
    "choice_b": "Cloud Pub/Sub topic",
    "choice_c": "Global load balancer",
    "choice_d": "HA VPN",
    "correct_answer": "C",
    "chapter": 4,
    "chapter_name": "Designing a Data Processing Solution",
    "topic": "Designing Infrastructure > ARS of Infrastructure",
    "reason": "Using Compute Engine means you are responsible for ensuring high availability and scalability. In GCP’s Compute Engine,  a load balancer is a service that automatically distributes incoming network traffic across multiple instances of your virtual machines (VMs) to ensure no single VM becomes overwhelmed with too much traffic. Load balancing helps improve the availability and reliability of your applications by distributing the load and preventing any individual instance from being a bottleneck.\nThere are 2 main types of load balancers in GCP Compute Engine:\n\uF06ERegional Load Balancer: single-region, distribute traffic across multiple instances within that regions\n\uF06EGlobal Load Balancer: designed to distribute traffic across multiple regions, providing a higher level of availability and redundancy",
    "point": 1,
    "timer": 90,
    "page": 124,
    "multiple_choices": false
  },

  {
    "question_id": 57,
    "question": "Your finance department is migrating a third-party application from an on-premises physical server. The system was written in C, but only the executable binary is available. After the migration, data will be extracted from the application database, transformed, and stored in a BigQuery data warehouse. The application is no longer actively supported by the original developer, and it must run on an Ubuntu 14.04 operating system that has been configured with several required packages. Which compute platform would you use?",
    "choice_a": "Compute Engine",
    "choice_b": "Kubernetes Engine",
    "choice_c": "App Engine Standard",
    "choice_d": "Cloud Functions",
    "correct_answer": "A",
    "chapter": 4,
    "chapter_name": "Designing a Data Processing Solution",
    "topic": "Designing Infrastructure > Compute Engine",
    "reason": "This scenario calls for full control over the choice of the operating system, and the application is moving from a physical server so that it is not containerized. Compute Engine can run the application in a VM configured with Ubuntu 14.04 and the additional packages",
    "point": 1,
    "timer": 90,
    "page": 121,
    "multiple_choices": false
  },

  {
    "question_id": 58,
    "question": "A team of developers has been tasked with rewriting the ETL process that populates an enterprise data warehouse. They plan to use a microservices architecture. Each microservice will run in its own Docker container. The amount of data processed during a run can vary, but the ETL process must always finish within one hour of starting. You want to minimize the amount of DevOps tasks the team needs to perform, but you do not want to sacrifice efficient utilization of compute resources. What GCP compute service would you recommend?",
    "choice_a": "Compute Engine",
    "choice_b": "Kubernetes Engine",
    "choice_c": "App Engine Standard",
    "choice_d": "Cloud Functions",
    "correct_answer": "B",
    "chapter": 4,
    "chapter_name": "Designing a Data Processing Solution",
    "topic": "Designing Infrastructure > Kubernetes Engine",
    "reason": "One of the advantages of Kubernetes is that users can precisely tune the allocation of cluster resources to each container. This is especially useful when applications are designed as a set of microservices. \nA microservice might need only a small fraction of a CPU. In that case, a container running that microservice can be allocated only the amount of CPU needed.\nThis allows for more efficient use of compute resources.",
    "point": 1,
    "timer": 90,
    "page": 121,
    "multiple_choices": false
  },

  {
    "question_id": 59,
    "question": "Your consulting company is contracted to help an enterprise customer negotiate a contract with a SaaS provider. Your client wants to ensure that they will have access to the SaaS service and it will be functioning correctly with only minimal downtime. What metric would you use when negotiating with the SaaS provider to ensure that your client’s reliability requirements are met?",
    "choice_a": "Average CPU utilization",
    "choice_b": "A combination of CPU and memory utilization",
    "choice_c": "Mean time between failure",
    "choice_d": "Mean time to recovery",
    "correct_answer": "C",
    "chapter": 4,
    "chapter_name": "Designing a Data Processing Solution",
    "topic": "Designing Infrastructure > ARS of Infrastructure",
    "reason": "Availability is defined as the ability of a use to access a resource at a specific time. Availability is a function of reliability, which is defined as the probability that a system will meet service-level objectives for some duration of time\n\uF06EReliability is often measured as the mean time between failures",
    "point": 1,
    "timer": 90,
    "page": 123,
    "multiple_choices": false
  },

  {
    "question_id": 60,
    "question": "Sensors on manufacturing machines send performance metrics to a cloud-based service that uses the data to build models that predict when a machine will break down. Metrics are sent in messages. Messages include a sensor identifier, a timestamp, a machine type, and a set of measurements. Different machine types have different characteristics related to failures, and machine learning engineers have determined that for highest accuracy, each machine type should have its own model. Once messages are written to a message broker, how should they be routed to instances of a machine learning service?",
    "choice_a": "Route randomly to any instance that is building a machine learning model",
    "choice_b": "Route based on the sensor identifier so that identifiers in close proximity are used in the same model",
    "choice_c": "Route based on machine type so that only data from one machine type is used for each model",
    "choice_d": "Route based on timestamp so that metrics close in time to one another are used in the same model",
    "correct_answer": "C",
    "chapter": 4,
    "chapter_name": "Designing a Data Processing Solution",
    "topic": "Designing Infrastructure > ARS of Infrastructure",
    "reason": "Machines of different types have different failure characteristics and therefore will have their own models",
    "point": 1,
    "timer": 90,
    "page": 124,
    "multiple_choices": false
  },

  {
    "question_id": 61,
    "question": "As part of a cloud migration effort, you are tasked with compiling an inventory of existing applications that will move to the cloud. One of the attributes that you need to track for each application is a description of its architecture. An application used by the finance department is written in Java, deployed on virtual machines, has several distinct services, and uses the SOAP protocol for exchanging messages. How would you categorize this architecture?",
    "choice_a": "Monolithic",
    "choice_b": "Service-oriented architecture (SOA)",
    "choice_c": "Microservice",
    "choice_d": "Serverless functions",
    "correct_answer": "B",
    "chapter": 4,
    "chapter_name": "Designing a Data Processing Solution",
    "topic": "Designing for Distributed Processing > Services",
    "reason": "The architecture of the application used by the finance department can be categorized as “Service-Oriented Architecture (SOA)”. Here’s why:\n\uF06EWritten in Java - This app in Java indicates the technology stack being used\n\uF06EDeployed on Virtual Machines - suggests a level of infrastructure abstraction, and the application is likely hosted on virtualized servers\n\uF06ESeveral Distinct Services - this aligns with the principles of a service-oriented architecture. In SOA, applications are structured as a set of loosely coupled services that communicate with each other.\n\uF06EUses the SOAP (Simple Object Access Protocol) - often associated with web services in a SOA",
    "point": 1,
    "timer": 90,
    "page": 131,
    "multiple_choices": false
  },

  {
    "question_id": 62,
    "question": "As part of a migration to the cloud, your department wants to restructure a distributed application that currently runs several services on a cluster of virtual machines. Each service implements several functions, and it is difficult to update one function without disrupting operations of the others. Some of the services require third-party libraries to be installed. Your company has standardized on Docker containers for deploying new services. What kind of architecture would you recommend?",
    "choice_a": "Monolithic",
    "choice_b": "Hub-and-spoke",
    "choice_c": "Microservices",
    "choice_d": "Pipeline architecture",
    "correct_answer": "C",
    "chapter": 4,
    "chapter_name": "Designing a Data Processing Solution",
    "topic": "Designing for Distributed Processing > Services",
    "reason": "A microservices architecture would be a suitable recommendation. Here’s why:\n\uF06EDocker containers - provide a lightweight and portable way to package applications and their dependencies. They ensure consistency across different environments, making it easier to manage dependencies and deploy services consistently.\n\uF06EMicroservices - for this type of architecture, an application is decomposed into a set of small and independent services, each running in its own container. Each service is responsible for a set of functions\n\uF06EUpdating functions independently - microservices allow for independent development, deployment, and scaling of each service. This means that updating 1 function within a service does not necessarily disrupt the operation of other services or functions\n\uF06EThird-party libraries - microservices can encapsulate third-party libraries within their respective containers",
    "point": 1,
    "timer": 90,
    "page": 131,
    "multiple_choices": false
  },

  {
    "question_id": 63,
    "question": "The CTO of your company is concerned about the rising costs of maintaining your company’s enterprise data warehouse. Some members of your team are advocating to migrate to a cloud-based data warehouse such as BigQuery. What is the first step for migrating from the on-premises data warehouse to a cloud-based data warehouse?",
    "choice_a": "Assessing the current state of the data warehouse",
    "choice_b": "Designing the future state of the data warehouse",
    "choice_c": "Migrating data, jobs, and access controls to the cloud",
    "choice_d": "Validating the cloud data warehouse",
    "correct_answer": "A",
    "chapter": 4,
    "chapter_name": "Designing a Data Processing Solution",
    "topic": "Migrating a Data Warehouse ",
    "reason": "One of the first things to do in a data warehouse migration is to identify existing use cases, including information about the data needed, such as its source, frequency of updating, ETL jobs used with the data, and access controls applied in the use case",
    "point": 1,
    "timer": 90,
    "page": 132,
    "multiple_choices": false
  },

  {
    "question_id": 64,
    "question": "When gathering requirements for a data warehouse migration, which of the following would you include in a listing of technical requirements?",
    "choice_a": "Data sources, data model, and ETL scripts",
    "choice_b": "Data sources, data model, and business sponsor roles",
    "choice_c": "Data sources only",
    "choice_d": "Data model, data catalog, ETL scripts, and business sponsor roles",
    "correct_answer": "A",
    "chapter": 4,
    "chapter_name": "Designing a Data Processing Solution",
    "topic": "Migrating a Data Warehouse",
    "reason": "First thing to do when doing the Data Warehouse migration is to identify existing use cases, including information about the data needed. Gathering technical requirements should include the following:\n\uF06EA list of data sources including metadata about data sources\n\uF06EA data catalog \n\uF06EA data model including schemas, tables, views, indexes\n\uF06EA list of reports and visualization generated from the data in warehouse\nA list of roles and associated access controls, including admins, developers, and end users (NOT BUSINESS SPONSORS AND THEIR ROLES)",
    "point": 1,
    "timer": 90,
    "page": 133,
    "multiple_choices": false
  },

  {
    "question_id": 65,
    "question": "In addition to concerns about the rising costs of maintaining an on-premises data warehouse, the CTO of your company has complained that new features and reporting are not being rolled out fast enough. The lack of adequate business intelligence has been blamed for a drop in sales in the last quarter. Your organization is incurring what kind of cost because of the backlog?",
    "choice_a": "Capital",
    "choice_b": "Operating",
    "choice_c": "Opportunity",
    "choice_d": "Fiscal",
    "correct_answer": "C",
    "chapter": 4,
    "chapter_name": "Designing a Data Processing Solution",
    "topic": "Migrating a Data Warehouse > Business Benefits",
    "reason": "The company is incurring an opportunity cost because if they had migrated to a modern cloud-based data warehouse, the team would have had opportunities to develop new reports",
    "point": 1,
    "timer": 90,
    "page": 133,
    "multiple_choices": false
  },

  {
    "question_id": 66,
    "question": "The data modelers who built your company’s enterprise data warehouse are asking for your guidance to migrate the data warehouse to BigQuery. They understand that BigQuery is an analytical database that uses SQL as a query language. They also know that BigQuery supports joins, but reports currently run on the data warehouse are consuming significant amounts of CPU because of the number and scale of joins. What feature of BigQuery would you suggest they consider in order to reduce the number of joins required?",
    "choice_a": "Colossus filesystem",
    "choice_b": "Columnar data storage",
    "choice_c": "Nested and repeated fields",
    "choice_d": "Federated storage",
    "correct_answer": "C",
    "chapter": 4,
    "chapter_name": "Designing a Data Processing Solution",
    "topic": "Migrating a Data Warehouse > Designing a Future State",
    "reason": "As part of designing for the future state, you should consider how you can take advantage of BigQuery features for data warehousing, including:\n\uF06EBigQuery uses the columnar format, which supports nested and repeated fields (this is a feature) - help denormalize data models, thus reducing the need for joins\n\uF06EData stored on the Colossus filesystem, which stores redundant blocks of data on multiple physical disks - ensure availability and durability\n\uF06EFederate storage - used to query data stored in Cloud Storage, Bigtable or Google Drive",
    "point": 1,
    "timer": 90,
    "page": 133,
    "multiple_choices": false
  },

  {
    "question_id": 67,
    "question": "While the CTO is interested in having your enterprise data warehouse migrated to the cloud as quickly as possible, the CTO is particularly risk averse because of errors in reporting in the past. Which prioritization strategy would you recommend?",
    "choice_a": "Exploiting current opportunities",
    "choice_b": "Migrating analytical workloads first",
    "choice_c": "Focusing on the user experience first",
    "choice_d": "Prioritizing low-risk use cases first",
    "correct_answer": "D",
    "chapter": 4,
    "chapter_name": "Designing a Data Processing Solution",
    "topic": "Migrating a Data Warehouse > Migrating Data, Jobs, and Access Controls",
    "reason": "When a team of data warehouse developers is new to GCP and BigQuery, it may be best to start with low-risk use cases. These would be use cases that do not have other systems depending on them, and they are not needed for high-priority business processes that have to be available and reliable",
    "point": 1,
    "timer": 90,
    "page": 134,
    "multiple_choices": false
  },

  {
    "question_id": 68,
    "question": "The enterprise data warehouse has been migrated to BigQuery. The CTO wants to shut down the on-premises data warehouse but first wants to verify that the new cloud-based data warehouse is functioning correctly. What should you include in the verification process?",
    "choice_a": "Verify that schemas are correct and that data is loaded",
    "choice_b": "Verify schemas, data loads, transformations, and queries",
    "choice_c": "Verify that schemas are correct, data is loaded, and the backlog of feature requests is prioritized",
    "choice_d": "Verify schemas, data loads, transformations, queries, and that the backlog of feature requests is prioritize",
    "correct_answer": "B",
    "chapter": 4,
    "chapter_name": "Designing a Data Processing Solution",
    "topic": "Migrating a Data Warehouse > Validating the Data Warehouse",
    "reason": "The last step of migrating a data warehouse is to validate the migration. This includes testing and verifying that:\n\uF06ESchemas are defined correctly and completely\n\uF06EAll data expected to be in the data warehouse is actually loaded\n\uF06ETransformations are applied correctly and data quality checks pass\n\uF06EQueries, reports, and visualizations run as expected\n\uF06EAccess control policies are in place\n\uF06EOther governance practices are in place\nThe backlog of feature requests is important but not relevant to verifying the migration",
    "point": 1,
    "timer": 90,
    "page": 135,
    "multiple_choices": false
  },

  {
    "question_id": 69,
    "question": "What are the order of the step for migrating from the on-premises data warehouse to a cloud-based data warehouse? Pick the correct order for the stages:\n1 - Migrating data, jobs, and access controls to the cloud\n2 - Validating the cloud data warehouse\n3 - Designing the future state of the data warehouse\n4 - Assessing the current state of the data warehouse",
    "choice_a": "3, 4, 1, 2",
    "choice_b": "2, 4, 1, 3",
    "choice_c": "4, 3, 1, 2",
    "choice_d": "4, 1, 3, 2",
    "correct_answer": "C",
    "chapter": 4,
    "chapter_name": "Designing a Data Processing Solution",
    "topic": "Migrating a Data Warehouse ",
    "reason": "Order of the steps for migrating from on-premises data warehouse to a cloud-based data warehouse:\nDesigning the future state of the data warehouse\nAssessing the current state of the data warehouse\nMigrating data, jobs, and access controls to the cloud\nValidating the cloud data warehouse",
    "point": 1,
    "timer": 90,
    "page": 132,
    "multiple_choices": false
  },

  {
    "question_id": 70,
    "question": "What does a hybrid cloud architecture involve?",
    "choice_a": "Combining services only from public clouds",
    "choice_b": "Utilizing only private cloud infrastructure",
    "choice_c": "Integrating services and resources from both public and private clouds",
    "choice_d": "Exclusively relying on on-premises infrastructure",
    "correct_answer": "C",
    "chapter": 4,
    "chapter_name": "Designing a Data Processing Solution",
    "topic": "Designing Infrastructure > Hybrid Cloud and Edge Computing",
    "reason": "A Hybrid Cloud refers to an integrated cloud computing environment that combines services and resources from both public clouds and private clouds. It allows data and applications to be shared between them",
    "point": 1,
    "timer": 90,
    "page": 126,
    "multiple_choices": false
  },

  {
    "question_id": 71,
    "question": "What is a key characteristic of edge computing?",
    "choice_a": "Centralized data processing in the cloud",
    "choice_b": "Processing data closer to the source of data generation",
    "choice_c": "Exclusive reliance on on-premises servers",
    "choice_d": "Sending all data to the cloud for processing",
    "correct_answer": "B",
    "chapter": 4,
    "chapter_name": "Designing a Data Processing Solution",
    "topic": "Designing Infrastructure > Hybrid Cloud and Edge Computing",
    "reason": "Edge Computing involves processing data closer to the source of data generation rather than relying solely on centralized cloud servers. It brings computation and data storage closer to the devices where it is needed, reducing latency and enabling real-time data processing.",
    "point": 1,
    "timer": 90,
    "page": 127,
    "multiple_choices": false
  },

  {
    "question_id": 72,
    "question": "Imagine a multinational e-commerce company that operates both online and physical stores worldwide. They manage customer data, inventory, and sales analytics. The company is concerned about data security and compliance regulations in different regions. Describe how a hybrid cloud architecture could benefit this company.\nHow could a hybrid cloud architecture address the concerns of the multinational e-commerce company regarding data security and compliance regulations in various regions?",
    "choice_a": "By centralizing all data in a public cloud for easy management",
    "choice_b": "By using only on-premises servers to ensure data security",
    "choice_c": "By leveraging private cloud infrastructure for sensitive data and public cloud for scalability",
    "choice_d": "By relying solely on public cloud resources to reduce costs",
    "correct_answer": "C",
    "chapter": 4,
    "chapter_name": "Designing a Data Processing Solution",
    "topic": "Designing Infrastructure > Hybrid Cloud and Edge Computing",
    "reason": "A hybrid cloud approach allows the e-commerce company to store sensitive customer data in a private cloud, ensuring control and compliance with regional regulations. This addresses security concerns. Simultaneously, leveraging the public cloud for non-sensitive data provides scalability during peak demand without compromising security. The flexibility of a hybrid model accommodates varying compliance requirements across regions.",
    "point": 1,
    "timer": 90,
    "page": 126,
    "multiple_choices": false
  },

  {
    "question_id": 73,
    "question": "Consider a smart city initiative where various sensors and cameras are deployed throughout the city to monitor traffic, pollution, and public safety in real-time. The city aims to optimize traffic flow and respond swiftly to emergencies. In the context of the smart city initiative with sensors and cameras monitoring traffic and public safety, how could edge computing contribute to the optimization of traffic flow and rapid emergency response?",
    "choice_a": "By processing data locally at the edge to reduce latency and improve real-time decision-making",
    "choice_b": "By transmitting all sensor data to a single on-premises server for analysis",
    "choice_c": "By relying on centralized cloud servers for immediate data processing",
    "choice_d": "By implementing a hybrid cloud architecture for improved scalability",
    "correct_answer": "A",
    "chapter": 4,
    "chapter_name": "Designing a Data Processing Solution",
    "topic": "Designing Infrastructure > Hybrid Cloud and Edge Computing",
    "reason": "Edge computing minimizes latency by processing data locally, enhancing real-time decision-making. In the smart city scenario, analyzing sensor and camera data at the edge enables instant responses for traffic optimization and emergency situations. This approach ensures that decisions are made locally without relying on distant data centers, contributing to more efficient and timely responses in the smart city initiative.",
    "point": 1,
    "timer": 90,
    "page": 127,
    "multiple_choices": false
  },

  {
    "question_id": 74,
    "question": "Your development team is working on a microservices-based architecture for a new e-commerce platform. The services need to be deployed, scaled independently, and managed efficiently. Additionally, the team requires orchestration and automated scaling features. Which compute platform would you recommend for deploying and managing these microservices?",
    "choice_a": "Compute Engine",
    "choice_b": "Kubernetes Engine",
    "choice_c": "App Engine Standard",
    "choice_d": "Cloud Functions",
    "correct_answer": "B",
    "chapter": 4,
    "chapter_name": "Designing a Data Processing Solution",
    "topic": "Designing Infrastructure > Kubernetes Engine",
    "reason": "Kubernetes Engine is well-suited for deploying and managing containerized applications in a microservices architecture. It provides container orchestration, automated scaling, and easy management of containerized workloads.",
    "point": 1,
    "timer": 90,
    "page": 121,
    "multiple_choices": false
  },

  {
    "question_id": 75,
    "question": "Your company is developing a web application that requires auto-scaling based on demand, and you want to focus on writing code without dealing with infrastructure management. The application needs to be deployed quickly with minimal configuration. Which compute platform would be the most suitable for this scenario?",
    "choice_a": "Compute Engine",
    "choice_b": "Kubernetes Engine",
    "choice_c": "App Engine Standard",
    "choice_d": "Cloud Functions",
    "correct_answer": "C",
    "chapter": 4,
    "chapter_name": "Designing a Data Processing Solution",
    "topic": "Designing Infrastructure > App Engine",
    "reason": "App Engine Standard is a fully managed platform-as-a-service (PaaS) offering that allows developers to deploy and scale applications without managing the underlying infrastructure. It is suitable for applications with varying demand and provides automatic scaling based on traffic.",
    "point": 1,
    "timer": 90,
    "page": 122,
    "multiple_choices": false
  },

  {
    "question_id": 76,
    "question": "You are developing a serverless architecture for processing events triggered by changes in a cloud storage bucket. The processing involves lightweight functions that execute in response to these events. The workload is event-driven, and you want to minimize infrastructure management efforts. Which compute platform aligns with this serverless architecture?",
    "choice_a": "Compute Engine",
    "choice_b": "Kubernetes Engine",
    "choice_c": "App Engine Standard",
    "choice_d": "Cloud Functions",
    "correct_answer": "D",
    "chapter": 4,
    "chapter_name": "Designing a Data Processing Solution",
    "topic": "Designing Infrastructure > Cloud Functions",
    "reason": "Cloud Functions is a serverless compute platform designed for executing lightweight functions in response to events. It is ideal for event-driven architectures where the focus is on writing code, and infrastructure management is abstracted away.",
    "point": 1,
    "timer": 90,
    "page": 123,
    "multiple_choices": false
  },

  {
    "question_id": 77,
    "question": "Your company is planning to launch a new e-commerce platform that is expected to experience fluctuating levels of traffic. The platform needs to be available at all times, handle increased demand during promotional events, and ensure reliable performance. Which characteristic is most crucial for ensuring a seamless user experience in this scenario?",
    "choice_a": "Availability",
    "choice_b": "Scalability",
    "choice_c": "Reliability",
    "choice_d": "Redundancy",
    "correct_answer": "B",
    "chapter": 4,
    "chapter_name": "Designing a Data Processing Solution",
    "topic": "Designing Infrastructure > ARS of Infrastructure",
    "reason": "In the context of an e-commerce platform with fluctuating levels of traffic, scalability is crucial for ensuring a seamless user experience. Scalability allows the infrastructure to handle increased demand by efficiently allocating resources, such as adding more servers or adjusting computing power, to accommodate peak loads during promotional events.",
    "point": 1,
    "timer": 90,
    "page": 123,
    "multiple_choices": false
  },

  {
    "question_id": 78,
    "question": "Your company is responsible for managing a critical database that stores sensitive customer information. The database is a key component of various applications within the organization. In the event of a hardware failure, it is paramount to minimize downtime and data loss. What infrastructure characteristic is most suitable for ensuring data availability and continuity in case of hardware failures?",
    "choice_a": "Availability",
    "choice_b": "Scalability",
    "choice_c": "Reliability",
    "choice_d": "Redundancy",
    "correct_answer": "D",
    "chapter": 4,
    "chapter_name": "Designing a Data Processing Solution",
    "topic": "Designing Infrastructure > ARS of Infrastructure",
    "reason": "To ensure data availability and continuity in the event of hardware failures, implementing redundancy is crucial. Redundancy involves having duplicate systems or components in place, so if one fails, the other can seamlessly take over. In the context of a critical database, having redundant servers, storage, or a failover mechanism ensures that there's a backup system ready to handle operations, minimizing downtime and potential data loss.",
    "point": 1,
    "timer": 90,
    "page": 123,
    "multiple_choices": false
  },

  {
    "question_id": 79,
    "question": "A group of data scientists wants to pre-process a large dataset that will be delivered in batches. The data will be written to Cloud Storage and processed by custom applications running on Compute Engine instances. They want to process the data as quickly as possible when it arrives and are willing to pay the cost of running up to 10 instances at a time. When a batch is finished, they’d like to reduce the number of instances to 1 until the next batch arrives. The batches do not arrive on a known schedule. How would you recommend that they provision Compute Engine instances?",
    "choice_a": "Use a Cloud Function to monitor Stackdriver metrics, add instances when CPU utilization peaks, and remove them when demand drops.",
    "choice_b": "Use a script running on one dedicated instance to monitor Stackdriver metrics, add instances when CPU utilization peaks, and remove them when demand drops.",
    "choice_c": "Use managed instance groups with a minimum of 1 instance and a maximum of 10.",
    "choice_d": "Use Cloud Dataproc with an auto-scaling policy set to have a minimum of 1 instance and a maximum of 10.",
    "correct_answer": "C",
    "chapter": 5,
    "chapter_name": "Building and Operationalizing Processing Infrastructure",
    "topic": "Provisioning and Adjusting Compute Engine",
    "reason": "When creating an instance group, you can specify the name of the instance template along with a target CPU utilization that will trigger adding instances to the group up to the maximum of instances allowed, you can also set the minimum number of instances so it would never goes back to 0 instances any point of time (stalling).",
    "point": 1,
    "timer": 90,
    "page": 147,
    "multiple_choices": false
  },

  {
    "question_id": 80,
    "question": "You are running a high-performance computing application in a managed instance group. You notice that the throughput of one instance is significantly lower than that for other instances. The poorly performing instance is terminated, and another instance is created to replace it. What feature of managed instance groups is at work here?",
    "choice_a": "Autoscaling",
    "choice_b": "Autohealing",
    "choice_c": "Redundancy",
    "choice_d": "Eventual consistency",
    "correct_answer": "B",
    "chapter": 5,
    "chapter_name": "Building and Operationalizing Processing Infrastructure",
    "topic": "Provisioning and Adjusting Compute Engine > Provisioning MIGs",
    "reason": "The ability to enhance the reliability and availability of your application by automatically replacing failed instances within a managed instance group is called [Autohealing].\nWhile the other 3 are also features of MIGs in Compute Engine but they serve different purposes:\n\uF06EAutoscaling: automatically adjust the number of VM instance to distribute the workload\n\uF06ERedundancy: creating duplicate of backup components to provide fault tolerance and prevent a single point of failure\nEventual Consistency: a property of distributed systems that indicates that, given enough time, all replicas of piece of data in the system will converge to the same value",
    "point": 1,
    "timer": 90,
    "page": 145,
    "multiple_choices": false
  },

  {
    "question_id": 81,
    "question": "A new engineer in your group asks for your help with creating a managed instance group. The engineer knows the configuration and the minimum and maximum number of instances in the MIG. What is the next thing the engineer should do to create the desired MIG?",
    "choice_a": "Create each of the initial members of the instance group using gcloud compute instance create commands",
    "choice_b": "Create each of the initial members of the instance group using the cloud console",
    "choice_c": "Create an instance template using the gcloud compute instance-templates create command",
    "choice_d": "Create an instance template using the cbt create instance-template command",
    "correct_answer": "C",
    "chapter": 5,
    "chapter_name": "Building and Operationalizing Processing Infrastructure",
    "topic": "Provisioning and Adjusting Compute Engine > Provisioning MIGs",
    "reason": "Once the engineer knows the configuration and the min-max number of instances in the MIG, he will have to create an instance from this template, by using the gcloud compute instances create commands.",
    "point": 1,
    "timer": 90,
    "page": 145,
    "multiple_choices": false
  },

  {
    "question_id": 82,
    "question": "Your team is migrating applications from running on bare-metal servers and virtual machines to running in containers. You would like to use Kubernetes Engine to run those containers. One member of the team is unfamiliar with Kubernetes and does not understand why they cannot find a command to deploy a container. How would you describe the reason why there is no deploy container command?",
    "choice_a": "Kubernetes uses pods as the smallest deployable unit, and pods have usually one but possibly multiple containers that are deployed as a unit.",
    "choice_b": "Kubernetes uses deployments as the smallest deployable unit, and pods have usually one but possibly multiple containers that are deployed as a unit.",
    "choice_c": "Kubernetes uses replicas as the smallest deployable unit, and pods have usually one but possibly multiple containers that are deployed as a unit.",
    "choice_d": "Kubernetes calls containers “pods,” and the command to deploy is kubectl deploy pod.",
    "correct_answer": "A",
    "chapter": 5,
    "chapter_name": "Building and Operationalizing Processing Infrastructure",
    "topic": "Provisioning and Adjusting Kubernetes Engine > Architecture",
    "reason": "There is no deploy container command in Kubernetes Engine because it has “pods” as the smallest computation unit, and each pod contain one or more containers. So specifically, that member cannot find a command to deploy a container because they can only work with Pods, and containers are the off-spring of pods.",
    "point": 1,
    "timer": 90,
    "page": 149,
    "multiple_choices": false
  },

  {
    "question_id": 83,
    "question": "A Kubernetes administrator wants to improve the performance of an application running in Kubernetes. They have determined that the four replicas currently running are not enough to meet demand and want to increase the total number of replicas to six. The name of the deployment is my-app-123. What command should they use?",
    "choice_a": "kubectl scale deployment my-app-123 --replicas 6",
    "choice_b": "kubectl scale deployment my-app-123 --replicas 2",
    "choice_c": "gcloud containers scale deployment my-app-123 --replicas 6",
    "choice_d": "gcloud containers scale deployment my-app-123 --replicas 2",
    "correct_answer": "A",
    "chapter": 5,
    "chapter_name": "Building and Operationalizing Processing Infrastructure",
    "topic": "Provisioning and Adjusting Kubernetes Engine > Adjusting Resources",
    "reason": "In Kubernetes Engine, to adjust the number of replicas manually, you can use kubectl, which is the command-line utility for interacting with Kubernetes clusters. Note that this is NOT a gcloud container command. Let’s take a look at an example kubect command to adjust the number of replicas for a deployment named my-app-123:\n< kubect scale deployment my-app-123 --replicas 6 >\nThis command will set the number of desired replicas for the my-app-123 deployment to 6.",
    "point": 1,
    "timer": 90,
    "page": 151,
    "multiple_choices": false
  },

  {
    "question_id": 84,
    "question": "A Cloud Bigtable instance with one cluster is not performing as expected. The instance was created for analytics. Data is continuously streamed from thousands of sensors, and statistical analysis programs run continually in a batch. What would you recommend to improve performance?",
    "choice_a": " Use a write-optimized operating system on the nodes",
    "choice_b": "Use a read-optimized operating system on the nodes",
    "choice_c": "Add a cluster, run batch processing on one cluster, and have writes routed to the other cluster",
    "choice_d": "Add another node pool to the cluster in each zone that already has a node pool or that cluster",
    "correct_answer": "C",
    "chapter": 5,
    "chapter_name": "Building and Operationalizing Processing Infrastructure",
    "topic": "Provisioning and Adjusting Cloud Bigtable > Replication in Bigtable",
    "reason": "Multiple clusters can help improve read performances when running large batch analytic workloads along with write-intensive operations, users can experience degraded write performance. With multiple clusters, read operations can be routed to one cluster and write traffic to another cluster.",
    "point": 1,
    "timer": 90,
    "page": 156,
    "multiple_choices": false
  },

  {
    "question_id": 85,
    "question": "A Cloud Dataproc cluster is running with a single master node. You have determined that the cluster needs to be highly available. How would you increase the number of master nodes to 3?",
    "choice_a": "Use the gcloud dataproc clusters update command with parameter --num-masters 3",
    "choice_b": "Use the gcloud dataproc clusters update command with parameter --add-masters 2",
    "choice_c": "Use the cbt dataproc clusters update command with parameter --add-masters 2",
    "choice_d": "The number of master nodes cannot be changed. A new cluster would have to be deployed with three master nodes.",
    "correct_answer": "D",
    "chapter": 5,
    "chapter_name": "Building and Operationalizing Processing Infrastructure",
    "topic": "Provisioning and Adjusting Cloud Dataproc",
    "reason": "In Google Cloud Dataproc, the number of master nodes in a cluster is determined at the time of cluster creation and CANNOT be dynamically increased or decreased after the cluster is running. When you create a Dataproc cluster, you specify the number of master and worker nodes, and these settings remain fixed for the lifetime of the cluster.",
    "point": 1,
    "timer": 90,
    "page": 158,
    "multiple_choices": false
  },

  {
    "question_id": 86,
    "question": "You have provisioned a Kubernetes Engine cluster and deployed an application. The application load varies during the day, and you have configured auto-scaling to add replicas when CPU utilization exceeds 60 percent. How is that CPU utilization calculated?",
    "choice_a": "Based on all CPUs used by the deployment",
    "choice_b": "Based on all CPUs in the cluster",
    "choice_c": "Based on all CPU utilization of the most CPU-intensive pod",
    "choice_d": "Based on the CPU utilization of the least CPU-intensive pod",
    "correct_answer": "A",
    "chapter": 5,
    "chapter_name": "Building and Operationalizing Processing Infrastructure",
    "topic": "Provisioning and Adjusting Kubernetes > Adjusting Resources",
    "reason": "The total CPU utilization by the deployment is used as the basis for making scaling decisions.\n> B is incorrect, some CPUs in the cluster may be used by other deployments\n> C and D are incorrect, because the decision is based on overall utilization, not any individual pod",
    "point": 1,
    "timer": 90,
    "page": 152,
    "multiple_choices": false
  },

  {
    "question_id": 87,
    "question": "A team of data scientists wants to run a Python application in a Docker container. They want to minimize operational overhead, so they decide to use App Engine. They want to run the application in a Python 3.4 environment. Which configuration file would they modify to specify that runtime?",
    "choice_a": "app.yaml",
    "choice_b": "queue.yaml",
    "choice_c": "dispatch.yaml",
    "choice_d": "cron.yaml",
    "correct_answer": "A",
    "chapter": 5,
    "chapter_name": "Building and Operationalizing Processing Infrastructure",
    "topic": "Configuring Managed Serverless Processing Services",
    "reason": "You can configure App Engine as well as supporting services by specifying 3 files:\n\uF06EApp.yaml - required 3 parameters: runtime (specify the runtime environment, such as Python 3), handlers and threadsafe\n\uF06ECron.yaml - configure scheduled tasks for an application\n\uF06EDispatch.yaml - specify routing rules to send incoming requests to a specific service based on URL",
    "point": 1,
    "timer": 90,
    "page": 159,
    "multiple_choices": false
  },

  {
    "question_id": 88,
    "question": "Your team is experimenting with Cloud Functions to build a pipeline to process images uploaded to Cloud Storage. During the development stage, you want to avoid sudden spikes in Cloud Functions use because of errors in other parts of the pipeline, particularly the test code that uploads test images to Cloud Storage. How would you reduce the risk of running large numbers of Cloud Functions at one time?",
    "choice_a": "Use the --limit parameter when deploying the function",
    "choice_b": "Use the --max-instances parameter when deploying the function",
    "choice_c": "Set a label with the key max-instances and the value of the maximum number of instances",
    "choice_d": "Set a language-specific parameter in the function to limit the number of instances",
    "correct_answer": "B",
    "chapter": 5,
    "chapter_name": "Building and Operationalizing Processing Infrastructure",
    "topic": "Configuring Cloud Functions",
    "reason": "The --max-instances parameter limits the number of concurrently executing function instances\nIf you want to reduce the risk of running large numbers of Cloud Functions simultaneously, you can use the max-instances parameter to control the scaling behavior and resource allocation of your functions. “max-instances” is the number of Cloud Function instances that will exist at any one time.",
    "point": 1,
    "timer": 90,
    "page": 160,
    "multiple_choices": false
  },

  {
    "question_id": 89,
    "question": "Audit control requirements at your company require that all logs be kept for at least 365 days. You prefer to keep logs and log entries in Stackdriver logging, but you understand that logs with predefined retention periods of less than 1 year will require you to set up an export to another storage system, such as Cloud Storage. Which of the following logs would you need to set up exports for to meet the audit requirement?",
    "choice_a": "Admin activity audit logs",
    "choice_b": "System event audit logs",
    "choice_c": "Access transparency logs",
    "choice_d": "Data access audit logs",
    "correct_answer": "D",
    "chapter": 5,
    "chapter_name": "Building and Operationalizing Processing Infrastructure",
    "topic": "Monitoring Processing Resources > Stackdriver Logging",
    "reason": "Logs are maintained in Stackdriver for a specific period of time known as the retention period. If you want to keep them longer, you will need to export the logs before the end of retention period. The retention varies by log type:\n\uF06EAdmin activity audit logs: 400 days\n\uF06ESystem event audit logs: 400 days\n\uF06EAccess transparency logs: 400 days\n\uF06EData access audit logs: 30 days\nThe company requires logs to be kept for at least 365 days. The only log that needs to be set up exports is “Data Access audit logs”, because it can only be kept for 30 days.",
    "point": 1,
    "timer": 90,
    "page": 161,
    "multiple_choices": false
  },

  {
    "question_id": 90,
    "question": "You would like to collect data on the memory utilization of instances running in a particular managed instance group. What Stackdriver service would you use?",
    "choice_a": "Stackdriver Debugger",
    "choice_b": "Stackdriver Logging",
    "choice_c": "Stackdriver Monitoring",
    "choice_d": "Stackdriver Trace",
    "correct_answer": "C",
    "chapter": 5,
    "chapter_name": "Building and Operationalizing Processing Infrastructure",
    "topic": "Monitoring Processing Resources ",
    "reason": "Stackdriver Monitoring collects metrics on the performance of infrastructure resources and applications (including memory utilization of instances in a MIG)",
    "point": 1,
    "timer": 90,
    "page": 160,
    "multiple_choices": false
  },

  {
    "question_id": 91,
    "question": "You would like to view information recorded by an application about events prior to the application crashing. What Stackdriver service would you use?",
    "choice_a": "Stackdriver Debugger",
    "choice_b": "Stackdriver Logging",
    "choice_c": "Stackdriver Monitoring",
    "choice_d": "Stackdriver Trace",
    "correct_answer": "B",
    "chapter": 5,
    "chapter_name": "Building and Operationalizing Processing Infrastructure",
    "topic": "Monitoring Processing Resources",
    "reason": "Stackdriver Logging collects semi-structured data about events and aggregates logs from various services and applications. Useful for troubleshooting, auditing, and monitoring the health of your applications ",
    "point": 1,
    "timer": 90,
    "page": 161,
    "multiple_choices": false
  },

  {
    "question_id": 92,
    "question": "Customers are complaining of long waits while your e-commerce site processes orders. There are many microservices in your order processing system. You would like to view information about the time each microservice takes to run. What Stackdriver service would you use?",
    "choice_a": "Stackdriver Debugger",
    "choice_b": "Stackdriver Logging",
    "choice_c": "Stackdriver Monitoring",
    "choice_d": "Stackdriver Trace",
    "correct_answer": "D",
    "chapter": 5,
    "chapter_name": "Building and Operationalizing Processing Infrastructure",
    "topic": "Monitoring Processing Resources",
    "reason": "Stackdriver Trace is a distributed tracing system designed to collect data on how long it takes to process requests to services. This is especially useful when you’re using microservices architectures.",
    "point": 1,
    "timer": 90,
    "page": 161,
    "multiple_choices": false
  },

  {
    "question_id": 93,
    "question": "You have created a test environment for a group of business analysts to run several Cloud Dataflow pipelines. You want to limit the processing resources any pipeline can consume. What execution parameter would you specify to limit processing resources?",
    "choice_a": "numWorkers",
    "choice_b": "maxNumWorkers",
    "choice_c": "streaming",
    "choice_d": "maxResources",
    "correct_answer": "B",
    "chapter": 5,
    "chapter_name": "Building and Operationalizing Processing Infrastructure",
    "topic": "",
    "reason": "",
    "point": 1,
    "timer": 90,
    "page": 158,
    "multiple_choices": false
  },

  {
    "question_id": 94,
    "question": "During off-peak hours, you want to minimize costs by reducing the number of running instances. Which feature of Compute Engine allows you to dynamically adjust the instance count based on low demand and scale down resources automatically?",
    "choice_a": "Autoscaling",
    "choice_b": "Autohealing",
    "choice_c": "Redundancy",
    "choice_d": "Eventual consistency",
    "correct_answer": "A",
    "chapter": 5,
    "chapter_name": "Building and Operationalizing Processing Infrastructure",
    "topic": "Provisioning and Adjusting Compute Engine > Provisioning MIGs",
    "reason": "The feature being referred to in this question is Autoscaling. During off-peak hours, when the demand for resources is lower, you want to optimize your costs by reducing the number of running instances.Autoscaling is a feature that allows you to dynamically adjust the instance count based on predefined metrics, such as low demand or reduced traffic.",
    "point": 1,
    "timer": 90,
    "page": 145,
    "multiple_choices": false
  },

  {
    "question_id": 95,
    "question": "While designing the data consistency model for your distributed application, you need to consider how to handle situations where users in different regions might see slightly different versions of the data for a short period. Which feature or concept becomes crucial in addressing this challenge and ensuring a coherent user experience?",
    "choice_a": "Autoscaling",
    "choice_b": "Autohealing",
    "choice_c": "Redundancy",
    "choice_d": "Eventual consistency",
    "correct_answer": "D",
    "chapter": 5,
    "chapter_name": "Building and Operationalizing Processing Infrastructure",
    "topic": "Provisioning and Adjusting Compute Engine > Provisioning MIGs",
    "reason": "The feature or concept being referred to in this question is Eventual Consistency. In a globally distributed application where users in different regions access shared data, network latency and occasional connectivity issues can lead to delays in synchronizing updates across all replicas.",
    "point": 1,
    "timer": 90,
    "page": 145,
    "multiple_choices": false
  },

  {
    "question_id": 96,
    "question": "A development team is building a microservices architecture using Kubernetes Engine on Google Cloud. They need to ensure that certain pods within a deployment have priority access to resources over others. Which Kubernetes resource can the team configure to assign priority to specific pods within a deployment in Google Kubernetes Engine?",
    "choice_a": "app.yaml",
    "choice_b": "queue.yaml",
    "choice_c": "dispatch.yaml",
    "choice_d": "cron.yaml",
    "correct_answer": "B",
    "chapter": 5,
    "chapter_name": "Building and Operationalizing Processing Infrastructure",
    "topic": "Configuring Managed Serverless Processing Services",
    "reason": "In Google Cloud Tasks, the queue.yaml file is used to configure and define properties for Cloud Tasks queues. It allows the team to set parameters such as the maximum number of concurrent tasks and the rate at which tasks are processed for a particular queue.",
    "point": 1,
    "timer": 90,
    "page": 159,
    "multiple_choices": false
  },

  {
    "question_id": 97,
    "question": "An operations team is responsible for managing the routing rules for incoming requests to different services in a microservices architecture. They want to customize the routing behavior based on URL patterns. Which configuration file should the operations team modify to define URL patterns and corresponding routing rules for their services?",
    "choice_a": "app.yaml",
    "choice_b": "queue.yaml",
    "choice_c": "dispatch.yaml",
    "choice_d": "cron.yaml",
    "correct_answer": "C",
    "chapter": 5,
    "chapter_name": "Building and Operationalizing Processing Infrastructure",
    "topic": "Configuring Managed Serverless Processing Services",
    "reason": "The dispatch.yaml file in Google App Engine is used to define URL patterns and corresponding routing rules. It allows the operations team to customize how requests are routed to different services based on the specified URL patterns.",
    "point": 1,
    "timer": 90,
    "page": 159,
    "multiple_choices": false
  },

  {
    "question_id": 98,
    "question": "A team is developing a web application with scheduled tasks that need to run at specified intervals. They want to configure and manage the cron jobs for these tasks in Google Cloud. Which configuration file should the team modify to define the schedule and parameters for their cron jobs?",
    "choice_a": "app.yaml",
    "choice_b": "queue.yaml",
    "choice_c": "dispatch.yaml",
    "choice_d": "cron.yaml",
    "correct_answer": "D",
    "chapter": 5,
    "chapter_name": "Building and Operationalizing Processing Infrastructure",
    "topic": "Configuring Managed Serverless Processing Services",
    "reason": "In Google App Engine, the cron.yaml file is used to define and configure cron jobs. It allows the team to specify the schedule, target URL, and other parameters for tasks that need to be executed at regular intervals.",
    "point": 1,
    "timer": 90,
    "page": 159,
    "multiple_choices": false
  },

  {
    "question_id": 99,
    "question": "In Google Cloud, which are the benefits and features that contribute to the efficient management, scaling, and availability of application?",
    "choice_a": "Autohealing based on application-specific health checks, which replace non-functioning Instances",
    "choice_b": "Support for multizone groups that provide for availability in spite of zone-level failures",
    "choice_c": "Integrated backup and restore capabilities for data stored on instances",
    "choice_d": "Load balancing to distribute workload across all instances in the group",
    "choice_e": "Autoscaling, which adds or removes instances in the group to accommodate increases and decreases in workloads",
    "choice_f": "Real-time monitoring and alerting for custom metrics and logs",
    "choice_g": "Automatic, incremental updates to reduce disruptions to workload processing",
    "choice_h": "Support for custom encryption keys to enhance data security and compliance",
    "correct_answer": ["A","B","D","E","G"],
    "chapter": 5,
    "chapter_name": "Building and Operationalizing Processing Infrastructure",
    "topic": "Provisioning and Adjusting Compute Engine",
    "reason": "MIGs have several useful properties, including:\n\uF06EAutohealing based on application-specific health checks, which replaces non-functioning instances\n\uF06ESupport for multi-zone groups that provide for availability in spite of zone-level failures\n\uF06ELoad balancing to distribute workload across all instances in the group\n\uF06EAutoscaling, which adds or removes instances in the group to accommodate increases and decreases in workloads\nAutomatic, incremental updates to reduce disruptions to workload processing",
    "point": 1,
    "timer": 90,
    "page": 145,
    "multiple_choices": true
  },

  {
    "question_id": 100,
    "question": "Bigtable instance can be provisioned using some pieces of technology, which technology CANNOT provision Bigtable?",
    "choice_a": "Cloud Console",
    "choice_b": "Stackdriver Monitoring",
    "choice_c": "Command-line SDK",
    "choice_d": "REST API",
    "correct_answer": "B",
    "chapter": 5,
    "chapter_name": "Building and Operationalizing Processing Infrastructure",
    "topic": "Provisioning and Adjusting Cloud Bigtable",
    "reason": "Bigtable instance can be provisioned using the cloud console, command-line SDK, and REST API",
    "point": 1,
    "timer": 90,
    "page": 154,
    "multiple_choices": false
  },

  {
    "question_id": 101,
    "question": "You have been tasked with creating a pilot project in GCP to demonstrate the feasibility of  migrating workloads from an on-premises Hadoop cluster to Cloud Dataproc. Three other engineers will work with you. None of the data that you will use contains sensitive information. You want to minimize the amount of time that you spend on administering the development environment. What would you use to control access to resources in the development  environment?",
    "choice_a": "Predefined roles",
    "choice_b": "Custom roles",
    "choice_c": "Primitive roles",
    "choice_d": "Access control lists",
    "correct_answer": "C",
    "chapter": 6,
    "chapter_name": "Designing for Security and Compliance",
    "topic": "Cloud IAM Access Management > Primitive Roles",
    "reason": "Primitive roles can be used where coarse-grained access controls are acceptable, you could use primitive roles to grant access to developers in a development environment, since the developers would be responsible for administering the development environment.",
    "point": 1,
    "timer": 90,
    "page": 170,
    "multiple_choices": false
  },

  {
    "question_id": 102,
    "question": "The auditors for your company have determined that several employees have more permissions than needed to carry out their job responsibilities. All the employees have users accounts on GCP that have been assigned predefined roles. You have concluded that the optimal way to meet the auditors’ recommendations is by using custom roles. What permission is needed to create a custom role?",
    "choice_a": "iam.roles.create",
    "choice_b": "iam.custom.roles",
    "choice_c": "roles/iam.custom.create",
    "choice_d": "roles/iam.create.custom",
    "correct_answer": "A",
    "chapter": 6,
    "chapter_name": "Designing for Security and Compliance",
    "topic": "Cloud IAM Access Management > Custom Roles",
    "reason": "Users MUST have the iam.roles.create permission to be able to create a custom role",
    "point": 1,
    "timer": 90,
    "page": 172,
    "multiple_choices": false
  },

  {
    "question_id": 103,
    "question": "You have created a managed instance group in Compute Engine to run a high-performance computing application. The application will read source data from a Cloud Storage bucket and write results to another bucket. The application will run whenever new data is uploaded to Cloud Storage via a Cloud Function that invokes the script to start the job. You will need to assign the role roles/storage.objectCreator to an identity so that the application can write the output data to Cloud Storage. To what kind of identity would you assign the roles?",
    "choice_a": "User",
    "choice_b": "Group",
    "choice_c": "Service account",
    "choice_d": "You wouldn’t. The role would be assigned to the bucket",
    "correct_answer": "C",
    "chapter": 6,
    "chapter_name": "Designing for Security and Compliance",
    "topic": "IAM Storage & Processing Services > 5 Standard Roles are used with Cloud Storage",
    "reason": "roles/storage.objectCreator is 1 of the 5 standard roles are used with Cloud Storage. A service account associated with the application should have this role assigned to it.",
    "point": 1,
    "timer": 90,
    "page": 177,
    "multiple_choices": false
  },

  {
    "question_id": 104,
    "question": "Your company has implemented an organizational hierarchy consisting of two layers of folders and tens of projects. The top layer of folders corresponds to a department, and the second layer of folders are working groups within a department. Each working group has one or more projects in the resource hierarchy. You have to ensure that all projects comply with regulations, so you have created several policies. Policy A applies to all departments. B, C, D, and E are department specific. At what level of the resource hierarchy would you assign each policy?",
    "choice_a": "Assign policies A, B, C, D, and E to each folder",
    "choice_b": "Assign policy A to the organizational hierarchy and policies B, C, D, and E to each department’s corresponding folder",
    "choice_c": "Assign policy A to the organizational hierarchy and policies B, C, D, and E to each department’s corresponding projects",
    "choice_d": "Assign policy A to each department’s folder and policies B, C, D, and E to each project",
    "correct_answer": "B",
    "chapter": 6,
    "chapter_name": "Designing for Security and Compliance",
    "topic": "IAM Storage & Processing Services > Access Control > Hierarchy Policy",
    "reason": "Policies are inherited through the resource hierarchy. Folders inherit the policies of the organization. If a folder is created within another folder, it inherits the policies of the encapsulating folder. Projects inherit the policies of the organization and any higher-level folders. Resources inherit the policies of the organization, folders, and projects above them in the hierarchy. The combination of policies directly assigned to a resource, and the policies inherited from ancestors in the resource hierarchy, is called the effective policy.",
    "point": 1,
    "timer": 90,
    "page": 176,
    "multiple_choices": false
  },

  {
    "question_id": 105,
    "question": "Your startup is developing a mobile app that takes an image as input and produces a list of names of objects in the image. The image file is uploaded from the mobile device to a Cloud Storage bucket. A service account is associated with the server-side application that will retrieve the image. The application will not perform any other operation on the file or the bucket. Following the principle of least privilege, what role would you assign to the service account?",
    "choice_a": "roles/storage.objectViewer",
    "choice_b": "roles/storage.objectAdmin",
    "choice_c": "roles/storage.objectCreator",
    "choice_d": "roles/storage.objectViewer and roles/storage.objectCreator",
    "correct_answer": "A",
    "chapter": 6,
    "chapter_name": "Designing for Security and Compliance",
    "topic": "IAM Storage & Processing Services > 5 Standard Roles are used with Cloud Storage",
    "reason": "objectViewer is the role with the least privilege, this role cannot perform any tasks except for viewing the object. While the role of Admin can do all kind of operations, and objectCreator can do the creation tasks.",
    "point": 1,
    "timer": 90,
    "page": 177,
    "multiple_choices": false
  },

  {
    "question_id": 106,
    "question": "A data analyst asks for your help on a problem that users are having that involves BigQuery. The data analyst has been granted permissions to read the tables in a particular dataset. However, when the analyst runs a query, an error message is returned. What role would you think is missing from the users’ assigned roles?",
    "choice_a": "roles/BigQuery.admin",
    "choice_b": "roles/BigQuery.jobUser",
    "choice_c": "roles/BigQuery.metadataViewer",
    "choice_d": "roles/BigQuery.queryRunner",
    "correct_answer": "B",
    "chapter": 6,
    "chapter_name": "Designing for Security and Compliance",
    "topic": "Cloud IAM Access Management > Predefined Roles > BigQuery",
    "reason": "The role jobUser allows users to run jobs, including queries.",
    "point": 1,
    "timer": 90,
    "multiple_choices": false
  },

  {
    "question_id": 107,
    "question": "Your company is subject to financial industry regulations that require all customer data to be encrypted when persistently stored. Your CTO has tasked you with assessing options for encrypting the data. What must you do to ensure that applications processing protected data encrypt it when it is stored on disk or SSD?",
    "choice_a": "Configure a database to use database encryption",
    "choice_b": "Configure persistent disks to use disk encryption",
    "choice_c": "Configure the application to use application encryption",
    "choice_d": "Nothing. Data is encrypted at rest by default",
    "correct_answer": "D",
    "chapter": 6,
    "chapter_name": "Designing for Security and Compliance",
    "topic": "Data Security > Encryption At Rest",
    "reason": "Data at rest (at any given time) is encrypted by default in Google Cloud Platform.",
    "point": 1,
    "timer": 90,
    "page": 180,
    "multiple_choices": false
  },

  {
    "question_id": 108,
    "question": "Data can be encrypted at multiple levels, such as at the platform, infrastructure, and device levels. At the device level, how is data encrypted in the Google Cloud Platform?",
    "choice_a": "AES256 or AES128 encryption",
    "choice_b": "Elliptic curve cryptography",
    "choice_c": "Data Encryption Standard (DES)",
    "choice_d": "Blowfish",
    "correct_answer": "A",
    "chapter": 6,
    "chapter_name": "Designing for Security and Compliance",
    "topic": "Data Security - Encryption At Res",
    "reason": "In Device / Hardware level, data encrypted apply AES256 and AES128 encryption",
    "point": 1,
    "timer": 90,
    "page": 180,
    "multiple_choices": false
  },

  {
    "question_id": 109,
    "question": "In GCP, each data chunk written to a storage system is encrypted with a data encryption key. How does GCP protect the data encryption key so that an attacker who gained access to the storage system storing the key could not use it to decrypt the data chunk?",
    "choice_a": "GCP writes the data encryption key to a hidden location on disk.",
    "choice_b": "GCP encrypts the data encryption key with a key encryption key.",
    "choice_c": "GCP stores the data encryption key in a secure Cloud SQL database.",
    "choice_d": "GCP applies an elliptic curve encryption algorithm for each data encryption key.",
    "correct_answer": "B",
    "chapter": 6,
    "chapter_name": "Designing for Security and Compliance",
    "topic": "Data Security > Encryption At Rest",
    "reason": "When Google Cloud stores data in the storage system, it stores it in sub-file chunks that can be up to several gigabytes in size. Each chunk is encrypted with its own key, known as a data encryption key (DEK) . If a chunk is updated, a new key is used. Keys are not used for more than one chunk. Also, each chunk has a unique identifier that is referenced by access control lists (ACLs) to limit access to the chunks, which are stored in different locations to make it even more difficult for a malicious actor to piece them together.",
    "point": 1,
    "timer": 90,
    "page": 180,
    "multiple_choices": false
  },

  {
    "question_id": 110,
    "question": "The CTO has asked you to participate in a prototype project to provide better privacy controls. The CTO asks you to run a risk analysis job on a text file that has been inspected by the Data Loss Prevention API. What is the CTO interested in knowing?",
    "choice_a": "The number of times sensitive information is redacted",
    "choice_b": "The percentage of text that is redacted",
    "choice_c": "The likelihood that the data can be re-identified",
    "choice_d": "What InfoType patterns were detected",
    "correct_answer": "C",
    "chapter": 6,
    "chapter_name": "Designing for Security and Compliance",
    "topic": "Data Security > Data Loss Prevention API  > Running Data Loss Prevention Jobs",
    "reason": "Risk Analysis Jobs in Data Loss Prevention API - calculate the likelihood that data could be re-identified.",
    "point": 1,
    "timer": 90,
    "page": 184,
    "multiple_choices": false
  },

  {
    "question_id": 111,
    "question": "Your company is about to start a huge project to analyze a large number of documents to redact sensitive information. You would like to follow Google-recommended best practices. What would you do first?",
    "choice_a": "Identify InfoTypes to use",
    "choice_b": "Prioritize the order of scanning, starting with the most at-risk data",
    "choice_c": "Run a risk analysis job first",
    "choice_d": "Extract a sample of data and apply all InfoTypes to it",
    "correct_answer": "B",
    "chapter": 6,
    "chapter_name": "Designing for Security and Compliance",
    "topic": "Data Security > Data Loss Prevention API  > Inspection Best Practices",
    "reason": "This is the Google-recommended best practices for using the Data Loss Prevention API\n\uF076Step 1: you should inventory and prioritize the content you wish to scan. This is especially true if you have a large backlog of content that needs to be scanned. The data that is most at risk should be scanned first\n\uF076Step 2: Make sure that the cloud DLP service account has the correct roles to access to your services\n\uF076Step 3: Sampling and using simple matching criteria\nStep 4: Schedule scans using job triggers",
    "point": 1,
    "timer": 90,
    "page": 185,
    "multiple_choices": false
  },

  {
    "question_id": 112,
    "question": "Your startup is creating an app to help students with math homework. The app will track assignments, how long the student takes to answer a question, the number of incorrect answers, and so on. The app will be used by students ages 9 to 14. You expect to market the app in the United States. With which of the following regulations must you comply?",
    "choice_a": "HIPAA",
    "choice_b": "GDPR",
    "choice_c": "COPPA",
    "choice_d": "FedRAMP",
    "correct_answer": "C",
    "chapter": 6,
    "chapter_name": "Designing for Security and Compliance",
    "topic": "Data Security - Legal Compliance",
    "reason": "Each Regulation serve a unique purposes. COPPA stands for Children’s Online Privacy Protection Act, which is exactly the regulation that the app with children in the age of 9-14 needs.",
    "point": 1,
    "timer": 90,
    "page": 186,
    "multiple_choices": false
  },

  {
    "question_id": 113,
    "question": "You are investigating long latencies in Cloud Bigtable query response times. Most queries finish in less than 20 ms, but the 99th percentile queries can take up to 400 ms. You examine a Key Visualizer heatmap and see two areas with bright colors indicating hotspots. What could be causing those hotspots?",
    "choice_a": "Improperly used secondary index",
    "choice_b": "Less than optimal partition key",
    "choice_c": "Improperly designed row-key",
    "choice_d": "Failure to use a read replica",
    "correct_answer": "C",
    "chapter": 7,
    "chapter_name": "Designing Databases for Reliability, Scalability and Availability",
    "topic": "Designing Cloud Bigtable > Designing Row-keys",
    "reason": "In Cloud Bigtable, row-key designing is one of the most important design choices. The reason for this is that Cloud Bigtable scales best when read and write operations are distributed across nodes and tablets. If read and write operations are concentrated in a small number of nodes and tablets, the overall performance is limited to the performance of those few resources. Hotspots occur when a row-key design does not adequately distribute read/write load for a given query pattern.",
    "point": 1,
    "timer": 90,
    "page": 196,
    "multiple_choices": false
  },

  {
    "question_id": 114,
    "question": "An IoT startup has hired you to review their Cloud Bigtable design. The database stores data generated by over 100,000 sensors that send data every 60 seconds. Each row contains all the data for one sensor sent during an hour. Hours always start at the top of the hour. The row-key is the sensor ID concatenated to the hour of the day followed by the date. What change, if any, would you recommend to this design?",
    "choice_a": "Use one row per sensor and 60-second datasets instead of storing multiple datasets in a single row.",
    "choice_b": "Start the row key-row-key with the day and hour instead of the sensor ID.",
    "choice_c": "Allow hours to start an any arbitrary time to accommodate differences in sensor clocks.",
    "choice_d": "No change is recommended.",
    "correct_answer": "A",
    "chapter": 7,
    "chapter_name": "Designing Databases for Reliability, Scalability and Availability",
    "topic": "Designing Cloud Bigtable > Designing Row-keys > Time Series",
    "reason": "Keep it tall and narrow so it makes querying easier",
    "point": 1,
    "timer": 90,
    "page": 198,
    "multiple_choices": false
  },

  {
    "question_id": 115,
    "question": "Your company has a Cloud Bigtable database that requires strong consistency, but it also requires high availability. You have implemented Cloud Bigtable replication and specified single-cluster routing in the app profile for the database. Some users have noted that they occasionally receive query results inconsistent with what they should have received. The problem seems to correct itself within a minute. What could be the cause of this problem?",
    "choice_a": "Secondary indexes are being updated during the query and return incorrect results when a secondary index is not fully updated.",
    "choice_b": "You have not specified an app configuration file that includes single-cluster routing and use of replicas only for failover.",
    "choice_c": "Tablets are being moved between nodes, which can cause inconsistent query results.",
    "choice_d": "The row-key is not properly designed.",
    "correct_answer": "B",
    "chapter": 7,
    "chapter_name": "Designing Databases for Reliability, Scalability and Availability",
    "topic": "Designing Cloud Bigtable > Use Replication for Availability and Scalability",
    "reason": "App Profiles are configurations that specify how to handle client requests. If you want strong consistency, you would have to specify single-cluster routing in the app configuration, and you must not use the other clusters except for failover.",
    "point": 1,
    "timer": 90,
    "page": 199,
    "multiple_choices": false
  },

  {
    "question_id": 116,
    "question": "You have been tasked with migrating a MongoDB database to Cloud Spanner. MongoDB is a document database, similar to Cloud Firestore. You would like to maintain some of the document organization of the MongoDB design. What data type, available in Cloud Spanner, would you use to define a column that can hold a document-like structure?",
    "choice_a": "Array",
    "choice_b": "String",
    "choice_c": "STRUCT",
    "choice_d": "JSON",
    "correct_answer": "C",
    "chapter": 7,
    "chapter_name": "Designing Databases for Reliability, Scalability and Availability",
    "topic": "Designing Cloud Spanner > Relational Database Features",
    "reason": "Struct is one of the data types supported by Cloud Spanner, it is the container of ordered typed fields. While JSON is the file type, not a data type.",
    "point": 1,
    "timer": 90,
    "page": 201,
    "multiple_choices": false
  },

  {
    "question_id": 117,
    "question": "An application using a Cloud Spanner database has several queries that are taking longer to execute than the users would like. You review the queries and notice that they all involve joining three or more tables that are all related hierarchically. What feature of Cloud Spanner would you try in order to improve the query performance?",
    "choice_a": "Replicated clusters",
    "choice_b": "Interleaved tables",
    "choice_c": "STORING clause",
    "choice_d": "Execution plans",
    "correct_answer": "B",
    "chapter": 7,
    "chapter_name": "Designing Databases for Reliability, Scalability and Availability",
    "topic": "Designing Cloud Spanner > Interleaved Tables",
    "reason": "One important performance feature of Cloud Spanner is its ability to interleave data from related tables. This is done through a parent-child relationship in which parent data, such as row from the order table, is stored with child data, such as order line items. This makes retrieving data simultaneously from both tables more efficient than if the data were stored separately and is especially helpful when performing joins.",
    "point": 1,
    "timer": 90,
    "page": 202,
    "multiple_choices": false
  },

  {
    "question_id": 118,
    "question": "A Cloud Spanner database is using a natural key as the primary key for a large table. The natural key is the preferred key by users because the values are easy to relate to other data. Database administrators notice that these keys are causing hotspots on Cloud Spanner nodes and are adversely affecting performance. What would you recommend in order to improve performance?",
    "choice_a": "Keep the data of the natural key in the table but use a hash of the natural key as the primary key",
    "choice_b": "Keep the natural key and let Cloud Spanner create more splits to improve performance",
    "choice_c": "Use interleaved tables",
    "choice_d": "Use more secondary indexes",
    "correct_answer": "A",
    "chapter": 7,
    "chapter_name": "Designing Databases for Reliability, Scalability and Availability",
    "topic": "Designing Cloud Spanner > Primary Keys and Hotspots",
    "reason": "In Cloud Spanner, when designing a database structure, there are several recommended ways of defining primary keys to avoid hotspots, which are:\n\uF076Using a Hash of the Natural Key\n\uF076Swapping the Order of Columns in Keys to Promote High-Cardinality Attributes\n\uF076Using a Universal Unique Identifier\n\uF076Using Bit-Reverse Sequential Values",
    "point": 1,
    "timer": 90,
    "page": 203,
    "multiple_choices": false
  },

  {
    "question_id": 119,
    "question": "You are using a UUID as the primary key in a Cloud Spanner database. You have noticed hotspotting that you did not anticipate. What could be the cause?",
    "choice_a": "You have too many secondary indexes.",
    "choice_b": "You have too few secondary indexes.",
    "choice_c": "You are using a type of UUID that has sequentially ordered strings at the beginning of the UUID.",
    "choice_d": "You need to make the maximum length of the primary key longer.",
    "correct_answer": "C",
    "chapter": 7,
    "chapter_name": "Designing Databases for Reliability, Scalability and Availability",
    "topic": "Designing Cloud Spanner > Primary Keys and Hotspots",
    "reason": "When using Universally Unique Identifier, you must use UUID version 4 or later. Because some older UUIDs store timestamps in the high-order bits, that can leads to hotspotting",
    "point": 1,
    "timer": 90,
    "page": 203,
    "multiple_choices": false
  },

  {
    "question_id": 120,
    "question": "You are working for a financial services firm on a Cloud Bigtable database. The database stores equity and bond trading information from approximately 950 customers. Over 10,000 equities and bonds are tracked in the database. New data is received at a rate of 5,000 data points per minute. What general design pattern would you recommend?",
    "choice_a": "Tall and narrow table",
    "choice_b": "One table for each customer",
    "choice_c": "One table for equities and one for bonds",
    "choice_d": "Option A and Option B",
    "choice_e": "Option A and Option C",
    "correct_answer": "E",
    "chapter": 7,
    "chapter_name": "Designing Databases for Reliability, Scalability and Availability",
    "topic": "Designing Cloud Bigtable > Designing Row-keys > Time Series",
    "reason": "The table should always be “tall and narrow” so that it has the best performance in querying. One table for each customer would not be ideal since it required creating a new table whenever a new customer joined. Instead, we create a table for equities and another table for bonds data, so the tables could relate altogether easily, without ever creating a newer table, keep the number of tables as lowest as possible.",
    "point": 1,
    "timer": 90,
    "page": 198,
    "multiple_choices": false
  },

  {
    "question_id": 121,
    "question": "You have been brought into a large enterprise to help with a data warehousing initiative. The first project of the initiative is to build a repository for all customer-related data, including sales, finance, inventory, and logistics. It has not yet been determined how the data will be used. What Google Cloud storage system would you recommend that the enterprise use to store that data?",
    "choice_a": "Cloud Bigtable",
    "choice_b": "BigQuery",
    "choice_c": "Cloud Spanner",
    "choice_d": "Cloud Storage",
    "correct_answer": "D",
    "chapter": 7,
    "chapter_name": "Designing Databases for Reliability, Scalability and Availability",
    "topic": "",
    "reason": "",
    "point": 1,
    "timer": 90,
    "multiple_choices": false
  },

  {
    "question_id": 122,
    "question": "Data is streaming into a BigQuery table. As the data arrives, it is added to a partition that was automatically created that day. Data that arrives the next day will be written to a different partition. The data modeler did not specify a column to use as a partition key. What kind of partition is being used?",
    "choice_a": "Ingestion time partitioned tables",
    "choice_b": "Timestamp partitioned tables",
    "choice_c": "Integer range partitioned tables",
    "choice_d": "Clustered tables",
    "correct_answer": "A",
    "chapter": 7,
    "chapter_name": "Designing Databases for Reliability, Scalability and Availability",
    "topic": "Designing BigQuery > Clustered and Partitioned Tables",
    "reason": "When a table is created with ingestion time partitioning, BigQuery loads data into a daily partition and creates new partitions each day. Timestamp partitioned tables partition based on a DATE or TIMESTAMP column in a table. Integer range partition tables are partitioned based on a column with an INTEGER data type.",
    "point": 1,
    "timer": 90,
    "page": 209,
    "multiple_choices": false
  },

  {
    "question_id": 123,
    "question": "You are designing a BigQuery database with multiple tables in a single dataset. The data stored in the dataset is measurement data from sensors on vehicles in the company’s fleet. Data is collected on each vehicle and downloaded at the end of each shift. After that, it is loaded into a partitioned table. You want to have efficient access to the most interesting data, which you define as a particular measurement having a value greater than 100.00. You want to cluster on that measurement column, which is a FLOAT64. When you define the table with a timestamped partitioned table and clustering on the measurement column, you receive an error. What could that error be?",
    "choice_a": "You cannot use clustering on an external table.",
    "choice_b": "You cannot use clustering with a FLOAT64 column as the clustering key.",
    "choice_c": "The table is not the FLOAT64 partition type.",
    "choice_d": "The clustering key must be an integer or timestamp.",
    "correct_answer": "B",
    "chapter": 7,
    "chapter_name": "Designing Databases for Reliability, Scalability and Availability",
    "topic": "Designing BigQuery > Clustered and Partitioned Tables",
    "reason": "Clustering columns must be one of the following data types:\n\uF076Date\n\uF076Bool\n\uF076Geography\n\uF076INT64\n\uF076Numeric\n\uF076String\n\uF076Timestamp",
    "point": 1,
    "timer": 90,
    "page": 210,
    "multiple_choices": false
  },

  {
    "question_id": 124,
    "question": "What data formats are supported for external tables in Cloud Storage and Google Drive?",
    "choice_a": "Comma-separated values only",
    "choice_b": "Comma-separated values and Avro",
    "choice_c": "Comma-separated values, Avro, and newline-delimited JSON",
    "choice_d": "Comma-separated values, Avro, newline-delimited JSON, and Parquet",
    "correct_answer": "C",
    "chapter": 7,
    "chapter_name": "Designing Databases for Reliability, Scalability and Availability",
    "topic": "Designing BigQuery > External Data Access",
    "reason": "BigQuery can access data in external resources, known as federated sources. External sources can be Cloud Bigtable, Cloud Storage and Google Drive.\nBigQuery supports several formats of Cloud Storage data:\n\uF076Comma-separated values\n\uF076Newline-delimited JSON\n\uF076Avro\n\uF076Optimized Row Columnar (ORC)\n\uF076Parquet\n\uF076Datastore exports\n\uF076Firestore exports\nBigQuery supports several format of Google Drive\n\uF076Comma-separated values\n\uF076Newline-delimited JSON\n\uF076Avro\nGoogle Sheets",
    "point": 1,
    "timer": 90,
    "page": 212,
    "multiple_choices": false
  },

  {
    "question_id": 125,
    "question": "Analysts and data scientists at your company ask for your help with data preparation. They currently spend significant amounts of time searching for data and trying to understand the exact definition of the data. What GCP service would you recommend that they use? ",
    "choice_a": "Cloud Composer",
    "choice_b": "Data Catalog",
    "choice_c": "Cloud Dataprep",
    "choice_d": "Data Studio",
    "correct_answer": "B",
    "chapter": 8,
    "chapter_name": "Understanding Data Operations for Flexibility and Portability",
    "topic": "Data Catalog",
    "reason": "Cloud Catalog is designed to help data consumers understand what data is available, what it means, and how it can be used. Data Catalog is a GCP metadata service for data management. Its purpose is to:\n\uF06EData Discovery and Exploration\n\uF06ECollaboration and Knowledge Sharing\n\uF06EData Governance and Compliance\n\uF06EData Quality Monitoring\n\uF06EMetadata Integration\n\uF06EMachine Learning and AI support",
    "point": 1,
    "timer": 90,
    "page": 220,
    "multiple_choices": false
  },

  {
    "question_id": 126,
    "question": "Machine learning engineers have been working for several weeks on building a recommendation system for your company’s e-commerce platform. The model has passed testing and validation, and it is ready to be deployed. The model will need to be updated every day with the latest data. The engineers want to automate the model building process that includes running several Bash scripts, querying databases, and running some custom Python code. What GCP service would you recommend that they use? ",
    "choice_a": "Cloud Composer",
    "choice_b": "Data Catalog",
    "choice_c": "Cloud Dataprep",
    "choice_d": "Data Studio",
    "correct_answer": "A",
    "chapter": 8,
    "chapter_name": "Understanding Data Operations for Flexibility and Portability",
    "topic": "Orchestrating Workflows with Cloud Composer",
    "reason": "If you want to automate the model building process, including running Bash scripts, querying databases, and executing custom Python code, Cloud Composer would be the ideal choice. Cloud Composer provides a fully managed and scalable workflow orchestration service, allowing you to define, schedule, and automate complex workflows that involve various tasks and dependencies. You can create workflows that include running Bash scripts, interacting with databases, and executing Python code, providing a comprehensive solution for automating your ML model building process.",
    "point": 1,
    "timer": 90,
    "page": 231,
    "multiple_choices": false
  },

  {
    "question_id": 127,
    "question": "A business intelligence analyst has just acquired several new datasets. They are unfamiliar with the data and are especially interested in understanding the distribution of data in each column as well as the extent of missing or misconfigured data. What GCP service would you recommend they use?",
    "choice_a": "Cloud Composer",
    "choice_b": "Cloud Catalog",
    "choice_c": "Cloud Dataprep",
    "choice_d": "Data Studio",
    "correct_answer": "C",
    "chapter": 8,
    "chapter_name": "Understanding Data Operations for Flexibility and Portability",
    "topic": "Data Preprocessing with Dataprep > Discovering Data",
    "reason": "Understanding the distribution of data in each column, assessing the extent of missing or misconfigured data, and gaining insights into the quality of the data fall under the “Data Discovery”. A GCP Service that can be used for Data Preprocessing (including Cleansing Data, Discovering Data, Enriching Data, Import/Export Data, Structuring and Validating Data) is Cloud Dataprep\nNote: Dataprep is short for “Data Preprocessing”",
    "point": 1,
    "timer": 90,
    "page": 224,
    "multiple_choices": false
  },

  {
    "question_id": 128,
    "question": "Line-of-business managers have asked your team for additional reports from data in a data warehouse. They want to have a single report that can act as a dashboard that shows key metrics using tabular data as well as charts. What GCP service would you recommend? ",
    "choice_a": "Cloud Composer",
    "choice_b": "Data Catalog",
    "choice_c": "Cloud Dataprep",
    "choice_d": "Data Studio",
    "correct_answer": "D",
    "chapter": 8,
    "chapter_name": "Understanding Data Operations for Flexibility and Portability",
    "topic": "Visualization with Data Studio ",
    "reason": "Data Studio is a reporting and visualization tool. This tool is organized around reports, and it reads data from data sources and formats the data into table and charts",
    "point": 1,
    "timer": 90,
    "page": 226,
    "multiple_choices": false
  },

  {
    "question_id": 129,
    "question": "You are using Cloud Dataprep to prepare datasets for machine learning. Another team will be using the data that you prepare, and they have asked you to export your data from Cloud Dataprep. The other team is concerned about file size and asks you to compress the files using GZIP. What formats can you use in the export file?",
    "choice_a": "CSV only",
    "choice_b": "CSV and JSON only",
    "choice_c": "CSV and AVRO only ",
    "choice_d": "JSON and AVRO only",
    "correct_answer": "B",
    "chapter": 8,
    "chapter_name": "Understanding Data Operations for Flexibility and Portability",
    "topic": "Data Preprocessing with Dataprep > Importing and Exporting Data",
    "reason": "In Dataprep, data can be exported includes:\n\uF06ECSV\n\uF06EJSON\n\uF06EAvro\n\uF06EBigQuery Tables\nUsers can write compressed [CSV] and [JSON] files using GZIP or BZIP",
    "point": 1,
    "timer": 90,
    "page": 225,
    "multiple_choices": false
  },

  {
    "question_id": 130,
    "question": "The finance department in your company is using Data Studio for data warehouse reporting. Their existing reports have all the information they need, but the time required to update charts and tables is longer than expected. What kind of data source would you try to improve the query performance?",
    "choice_a": "Live data source",
    "choice_b": "Extracted data source",
    "choice_c": "Compound data source",
    "choice_d": "Blended data source",
    "correct_answer": "B",
    "chapter": 8,
    "chapter_name": "Understanding Data Operations for Flexibility and Portability",
    "topic": "Visualizing with Data Studio > Connecting to Data Sources",
    "reason": "Extracted Data sources work with a static snapshot of a dataset, which can be updated on demand. Extracted data sources involve pre-aggregating or pre-processing data and storing it in a separate location, such as a data warehouse or data table. This can significantly improve query performance because the data is already processed and optimized for reporting.\nReason for not Option D: Blended data source: While blending data from multiple sources can provide a unified view, it might introduce additional complexity, and the impact on query performance depends on the nature of the blending and the efficiency of the underlying data processing.",
    "point": 1,
    "timer": 90,
    "page": 228,
    "multiple_choices": false
  },

  {
    "question_id": 131,
    "question": "A DevOps team in your company uses Data Studio to display application performance data. Their top priority is timely data. What kind of connection would you recommend they use to have data updated in reports automatically?",
    "choice_a": "Live data source",
    "choice_b": "Extracted data source",
    "choice_c": "Compound or blended data source ",
    "choice_d": "Extracted or live data source",
    "correct_answer": "A",
    "chapter": 8,
    "chapter_name": "Understanding Data Operations for Flexibility and Portability",
    "topic": "Visualizing with Data Studio > Connecting to Data Sources",
    "reason": "When the top priority is timely data, Live Connection Data Sources would be the most recommended. Because these data sources are automatically updated with changes to the underlying data sources.\nExtracted Data Sources only work with a static snapshot of a dataset, which is not updated timely.",
    "point": 1,
    "timer": 90,
    "page": 228,
    "multiple_choices": false
  },

  {
    "question_id": 132,
    "question": "A machine learning engineer is using Data Studio to build models in Python. The engineer has decided to use a statistics library that is not installed by default. How would you suggest that they install the missing library?",
    "choice_a": "Using conda install or pip install from a Cloud shell",
    "choice_b": "Using conda install or pip install from within a Jupyter Notebook",
    "choice_c": "Use the Linux package manager from within a Cloud shell",
    "choice_d": "Download the source from GitHub and compile locally",
    "correct_answer": "B",
    "chapter": 8,
    "chapter_name": "Understanding Data Operations for Flexibility and Portability",
    "topic": "Exploring Data with Cloud Datalab > Adding Libraries to Cloud Datalab Instances",
    "reason": "When user needs to add other libraries/packages, they can use the “conda install command” or the “pip install” command. “conda install” or “pip install” can be run from within a Jupyter Notebook",
    "point": 1,
    "timer": 90,
    "page": 230,
    "multiple_choices": false
  },

  {
    "question_id": 133,
    "qsuetion": "9.A DevOps engineer is working with you to build a workflow to load data from an on-premises database to Cloud Storage and then run several data preprocessing and analysis programs. After those are run, the output is loaded into a BigQuery table, an email is sent to managers indicating that new data is available in BigQuery, and temporary files are deleted. What GCP service would you use to implement this workflow?",
    "choice_a": "Cloud Dataprep",
    "choice_b": "Cloud Dataproc",
    "choice_c": "Cloud Composer",
    "choice_d": "Data Studio",
    "correct_answer": "C",
    "chapter": 8,
    "chapter_name": "Understanding Data Operations for Flexibility and Portability",
    "topic": "Orchestrating Workflows with Cloud Composer",
    "reason": "After the data runs through multiple processes, the output is loaded to a BigQuery table, then an email is sent to indicate the appearance of a new table, then temporary files are deleted >> these tasks perfectly describe the automation workflows. When talking about workflows and scheduling, the service related would be “Apache Airflow”, implemented by “Cloud Composer”",
    "point": 1,
    "timer": 90,
    "page": 231,
    "multiple_choices": false
  },

  {
    "question_id": 134,
    "question": "You have just received a large dataset. You have comprehensive documentation on the dataset and are ready to start analyzing. You will do some visualization and data filtering, but you also want to be able to run custom Python functions. You want to work interactively with the data. What GCP service would you use?",
    "choice_a": "Cloud Dataproc",
    "choice_b": "Cloud Datalab",
    "choice_c": "Cloud Composer",
    "choice_d": "Data Studio",
    "correct_answer": "B",
    "chapter": 8,
    "chapter_name": "Understanding Data Operations for Flexibility and Portability",
    "topic": "Exploring Data with Cloud Datalab",
    "reason": "When you want to do some visualization and data filtering, and also be able to run custom Python functions, work interactively with the data then Cloud Datalab would be the perfect service.Cloud Datalab is an interactive tool for exploring and transforming data",
    "point": 1,
    "timer": 90,
    "page": 228,
    "multiple_choices": false
  },

  {
    "question_id": 135,
    "question": "In Dataprep, what are the flat file formats that the service can import?",
    "choice_a": "ZIP compressed file",
    "choice_b": "CSV",
    "choice_c": "Plain Text",
    "choice_d": "Avro",
    "choice_e": "Parquet",
    "choice_f": "JSON, including Nested",
    "choice_g": "TSV",
    "choice_h": "Excel",
    "correct_answer": ["B","C","E","F","G","H"],
    "chapter": 8,
    "chapter_name": "Understanding Data Operations for Flexibility and Portability",
    "topic": "Data Preprocessing with Dataprep > Importing and Exporting Data",
    "reason": "Cloud Dataprep can import a number of flat file formats, including\n■ Microsoft Excel format (XLS/XLSX)\n■ CSV\n■ JSON, including nested\n■ Plain text\n■ Tab-separated values (TSV)\n■ Parquet",
    "point": 1,
    "timer": 90,
    "page": 225,
    "multiple_choices": true
  },

  {
    "question_id": 136,
    "question": "You are working on a machine learning project where you need to preprocess and analyze large datasets using Python. The preprocessing steps involve cleaning, transforming, and feature engineering. Additionally, you need to leverage machine learning libraries like TensorFlow and Scikit-learn to build and train models. You want a flexible and interactive environment where you can iteratively develop and test your code. Which Google Cloud Platform (GCP) service would you choose?",
    "choice_a": "Cloud Dataprep",
    "choice_b": "Cloud Datalab",
    "choice_c": "Cloud Dataproc",
    "choice_d": "AI Platform Notebooks",
    "correct_answer": "B",
    "chapter": 8,
    "chapter_name": "Understanding Data Operations for Flexibility and Portability",
    "topic": "Exploring Data with Cloud Datalab",
    "reason": "Cloud Datalab provides an interactive and flexible environment for data exploration, analysis, and machine learning tasks. It integrates with various data processing and analysis tools and allows you to run custom Python code seamlessly",
    "point": 1,
    "timer": 90,
    "page": 228,
    "multiple_choices": false
  },

  {
    "question_id": 137,
    "question": "The marketing team in your company needs to create real-time dashboards for monitoring website traffic and user engagement. The data is constantly changing, and up-to-the-minute information is crucial for their analysis. Which type of data source would you recommend to optimize the real-time updates?",
    "choice_a": "Live data source",
    "choice_b": "Extracted data source",
    "choice_c": "Compound data source",
    "choice_d": "Blended data source",
    "correct_answer": "A",
    "chapter": 8,
    "chapter_name": "Understanding Data Operations for Flexibility and Portability",
    "topic": "Visualizing with Data Studio > Connecting to Data Sources",
    "reason": "In scenarios where real-time updates are crucial, such as monitoring website traffic, a live data source is preferred. It allows for direct and immediate access to the latest data without the need for extraction or pre-processing delays.",
    "point": 1,
    "timer": 90,
    "page": 228,
    "multiple_choices": false
  },

  {
    "question_id": 138,
    "question": "Your company is running a marketing campaign that involves tracking user interactions from various sources such as social media, email campaigns, and website forms. The marketing team needs a unified view that combines data from these different sources to assess the overall campaign performance. Which type of data source would best serve this requirement?",
    "choice_a": "Live data source",
    "choice_b": "Extracted data source",
    "choice_c": "Compound data source",
    "choice_d": "Blended data source",
    "correct_answer": "C",
    "chapter": 8,
    "chapter_name": "Understanding Data Operations for Flexibility and Portability",
    "topic": "Visualizing with Data Studio > Connecting to Data Sources",
    "reason": "When combining data from multiple sources to assess overall campaign performance, a compound data source is suitable. It allows for a unified view by incorporating data from different systems or platforms.",
    "point": 1,
    "timer": 90,
    "page": 228,
    "multiple_choices": false
  },

  {
    "question_id": 139,
    "question": "The HR department is working on a report that requires combining employee data from an HR system with survey results from an external vendor. The goal is to analyze the correlation between employee satisfaction and various HR metrics. Which type of data source would you recommend to seamlessly integrate data from different systems?",
    "choice_a": "Live data source",
    "choice_b": "Extracted data source",
    "choice_c": "Compound data source",
    "choice_d": "Blended data source",
    "correct_answer": "D",
    "chapter": 8,
    "chapter_name": "Understanding Data Operations for Flexibility and Portability",
    "topic": "Visualizing with Data Studio > Connecting to Data Sources",
    "reason": "Blended data sources are ideal when you need to seamlessly integrate data from different systems. In this case, blending HR system data with external survey results ensures a comprehensive dataset for correlation analysis without the need for complex ETL processes.",
    "point": 1,
    "timer": 90,
    "page": 228,
    "multiple_choices": false
  },

  {
    "question_id": 140,
    "question": "You have been tasked with helping to establish ML pipelines for your department. The models will be trained using data from several sources, including several enterprise transaction processing systems and third-party data provider datasets. Data will arrive in batches. Although you know the structure of the data now, you expect that it will change, and you will not know about the changes until the data arrives. You want to ensure that your ingestion process can store the data in whatever structure it arrives in. After the data is ingested, you will transform it as needed. What storage system would you use for batch ingestion?",
    "choice_a": "Cloud Storage",
    "choice_b": "Cloud Spanner",
    "choice_c": "Cloud Dataprep",
    "choice_d": "Cloud Pub/Sub",
    "correct_answer": "A",
    "chapter": 9,
    "chapter_name": "Deploying Machine Learning Pipelines",
    "topic": "Cloud Storage",
    "reason": "Cloud Storage is a fully-managed object storage service that allows you to store whatever structure it arrives in. While Cloud Spanner only supports structured, and Cloud Pub/Sub + Dataprep are not storage.",
    "point": 1,
    "timer": 90,
    "multiple_choices": false
  },

  {
    "question_id": 141,
    "question": "A startup company is building an anomaly detection service for manufacturing equipment. IoT sensors on manufacturing equipment transmit data on machine performance every 30 seconds. The service will initially support up to 5,000 sensors, but it will eventually grow to millions of sensors. The data will be stored in Cloud Bigtable after it is preprocessed and transformed by a Cloud Dataflow workflow. What service should be used to ingest the IoT data?",
    "choice_a": "Cloud Storage",
    "choice_b": "Cloud Bigtable",
    "choice_c": "BigQuery Streaming Insert",
    "choice_d": "Cloud Pub/Sub",
    "correct_answer": "D",
    "chapter": 9,
    "chapter_name": "Deploying Machine Learning Pipelines",
    "topic": "Cloud Pub/Sub",
    "reason": "Cloud Pub/Sub is the most suitable for ingesting streaming data that can grow exponentially, this service supports ingesting unlimited data stream, provide scaling ability (grow to millions of sensors). And also, Cloud Pub/Sub can integrates with other Cloud services, such as Cloud Dataflow. While BigQuery Streaming Insert cannot integrate with any Cloud service but itself.",
    "point": 1,
    "timer": 90,
    "multiple_choices": false
  },

  {
    "question_id": 142,
    "question": "A machine learning engineer has just started a new project. The engineer will be building a recommendation engine for an e-commerce site. Data from several services will be used, including data about products, customers, and inventory. The data is currently available in a data lake and stored in its raw, unprocessed form. What is the first thing you would recommend the machine learning engineer do to start work on the project?",
    "choice_a": "Ingest the data into Cloud Storage",
    "choice_b": "Explore the data with Cloud Dataprep",
    "choice_c": "Transform the data with Cloud Dataflow",
    "choice_d": "Transform the data with BigQuery",
    "correct_answer": "B",
    "chapter": 9,
    "chapter_name": "Deploying Machine Learning Pipelines",
    "topic": "Structure of ML Pipelines > Data Preparation",
    "reason": "Since the data is currently available in a data lake, we know that the Ingestion process has been done, and data is stored in its raw, unprocessed form, so the next stage for the data would be “Data Preparation”, in which, there would be some actions such as:\n\uF06EData Exploration: working with new data source, understand the distribution of data and overall quality\n\uF06EData Transformation: process raw and unprocessed data into usable data\n\uF06EFeature Engineering: adding or modifying the representation of features to make implicit patterns more explicit\nFor Data Preparation, the most suitable service would be “Dataprep” (used for Data Preprocessing including explore, transform and feature engineering)",
    "point": 1,
    "timer": 90,
    "page": 240,
    "multiple_choices": false
  },

  {
    "question_id": 143,
    "question": "A machine learning engineer is in the process of building a model for classifying fraudulent transactions. They are using a neural network and need to decide how many nodes and layers to use in the model. They are experimenting with several different combinations of number of nodes and number of layers. What data should they use to evaluate the quality of models being developed with each combination of settings?",
    "choice_a": "Training data",
    "choice_b": "Validation data",
    "choice_c": "Test data",
    "choice_d": "Hyperparameter data",
    "correct_answer": "B",
    "chapter": 9,
    "chapter_name": "Deploying Machine Learning Pipelines",
    "topic": "Structure of ML Pipelines > Data Segregation",
    "reason": "Validation data, in the context of machine learning, refers to a subset of the dataset that is used to assess the performance and generalization ability of a trained machine learning model. It is distinct from the training data, which is used to train the model, and the test data, which is used to evaluate the model's performance on unseen example.",
    "point": 1,
    "timer": 90,
    "page": 243,
    "multiple_choices": false
  },

  {
    "question_id": 144,
    "question": "A developer with limited knowledge of machine learning is attempting to build a machine learning model. The developer is using data collected from a data lake with minimal data preparation. After models are built, they are evaluated. Model performance is poor. The developer has asked for your help to reduce the time needed to train the model and increase the quality of model predictions. What would you do first with the developer? ",
    "choice_a": "Explore the data with the goal of feature engineering",
    "choice_b": "Create visualizations of accuracy, precision, recall, and F measures",
    "choice_c": "Use tenfold cross-validation",
    "choice_d": "Tune hyperparameters",
    "correct_answer": "A",
    "chapter": 9,
    "chapter_name": "Deploying Machine Learning Pipelines",
    "topic": "Structure of ML Pipelines > Data Preparation",
    "reason": "When the developer has limited knowledge of machine learning, and they did minimal effort in data preparation, the first thing you - as the senior developer do, is to go back from the first steps after Data Ingesting process. The developer must explore the data with the goal of feature engineering even if the fresh developer had done it, because the poor model performance mainly because of the lack of effort in data preparation.",
    "point": 1,
    "timer": 90,
    "page": 240,
    "multiple_choices": false
  },

  {
    "question_id": 145,
    "question": "A developer has built a machine learning model to predict the category of new stories. The possible values are politics, economics, business, health, science, and local news. The developer has tried several algorithms, but the model accuracy is poor even when evaluating the model on using the training data. This is an example of what kind of potential problem with a machine learning model?",
    "choice_a": "Overfitting",
    "choice_b": "Underfitting",
    "choice_c": "Too much training data",
    "choice_d": "Using tenfold cross-validation for evaluation",
    "correct_answer": "B",
    "chapter": 9,
    "chapter_name": "Deploying Machine Learning Pipelines",
    "topic": "Structure of ML Pipelines > Model Training > Underfitting",
    "reason": "When the model accuracy is poor, a potential problem with the machine learning model might be overfitting or underfitting. But if the model accuracy is even poor when evaluating the model on using the training data, the data does absolutely have the “Underfitting” problem.\nUnderfitting: The model is too simple to capture the underlying patterns in the data",
    "point": 1,
    "timer": 90,
    "page": 245,
    "multiple_choices": false
  },

  {
    "question_id": 146,
    "question": "A developer has built a machine learning model to predict the category of new stories. The possible values are politics, economics, business, health, science, and local news. The developer has tried several algorithms, but the model accuracy is quite high when evaluating the model using the training data but quite low when evaluating using test data. What would you recommend to correct this problem?",
    "choice_a": "Use confusion matrices for evaluation",
    "choice_b": "Use L1 or L2 regularization when evaluating",
    "choice_c": "Use L1 or L2 regularization when training",
    "choice_d": "Tune the hyperparameters more",
    "correct_answer": "C",
    "chapter": 9,
    "chapter_name": "Deploying Machine Learning Pipelines",
    "topic": "Structure of ML Pipelines > Model Training > Regularization",
    "reason": "The model accuracy is high when evaluating the model using the training data, but perform poorly when using test data => This indicates the potential problem of data is “Overfitting”\nTo prevent the data result from “Overfitting”, Regularization (both L1 and L2) should be done during the model training phase",
    "point": 1,
    "timer": 90,
    "page": 246,
    "multiple_choices": false
  },

  {
    "question_id": 147,
    "question": "Your e-commerce company deployed a product recommendation system six months ago. The system uses a machine learning model trained using historical sales data from the previous year. The model performed well initially. When customers were shown product recommendations, the average sale value increased by 14 percent. In the past month, the model has generated an average increase of only 2 percent. The model has not changed since it was deployed six months ago. What could be the cause of the decrease in effectiveness, and what would you recommend to correct it?",
    "choice_a": "The model is overfitting—use regularization.",
    "choice_b": "The data used to train the model is no longer representative of current sales data, and the model should be retrained with more recent data.",
    "choice_c": "The model should be monitored to collect performance metrics to identity the root cause of the decreasing effectiveness of the model.",
    "choice_d": "The model is underfitting—train with more data.",
    "correct_answer": "B",
    "chapter": 9,
    "chapter_name": "Deploying Machine Learning Pipelines",
    "topic": "Structure of ML Pipelines > Model Monitoring",
    "reason": "If the machine learning model performed very well at first and it also had good performance when working with train/test set, and the model decrease in effectiveness as time goes, there can only be 1 cause of this: The data used to train the model is too out-dated, it is no longer representative of current sales data. The solution for the model would be retrained with more recent data, and it should be updated frequently to keep the effectiveness\nNote that the model is a perfect fit, not overfitting, not underfitting, just outdated.",
    "point": 1,
    "timer": 90,
    "page": 249,
    "multiple_choices": false
  },

  {
    "question_id": 148,
    "question": "A startup company is developing software to analyze images of traffic in order to understand congestion patterns better and how to avoid them. The software will analyze images that are taken every minute from hundreds of locations in a city. The software will need to identify cars, trucks, cyclists, pedestrians, and buildings. The data on object identities will be used by analysis algorithms to detect daily patterns, which will then be used by traffic engineers to devise new traffic flow patterns. What GCP service would you use for this?",
    "choice_a": "AutoML Vision Object Detection",
    "choice_b": "AutoML Vision Edge - Object Detection",
    "choice_c": "AutoML Video Intelligence Classification",
    "choice_d": "Auto ML Video Intelligence Object Tracking",
    "correct_answer": "A",
    "chapter": 9,
    "chapter_name": "Deploying Machine Learning Pipelines",
    "topic": "GCP Options for Deploying Machine Learning Pipeline",
    "reason": "The GCP service that can be used to enable users to train their own machine learning models to classify images by using previously-taken images is the “AutoML Vision Classification”. The images is taken every minute, but it does not require on-device ML processing, so it cannot be option B “AML Vision Edge - Object Detection”",
    "point": 1,
    "timer": 90,
    "page": 250,
    "multiple_choices": false
  },

  {
    "question_id": 149,
    "question": "An analyst would like to build a machine learning model to classify rows of data in a dataset. There are two categories into which the rows can be grouped: Type A and Type B. The dataset has over 1 million rows, and each row has 32 attributes or features. The analyst does not know which features are important. A labeled training set is available with a sufficient number of rows to train a model. The analyst would like the most accurate model possible with the least amount of effort on the analyst’s part. What would you recommend? ",
    "choice_a": "Kubeflow",
    "choice_b": "Spark MLib",
    "choice_c": "AutoML Tables",
    "choice_d": "AutoML Natural Language",
    "correct_answer": "C",
    "chapter": 9,
    "chapter_name": "Deploying Machine Learning Pipelines",
    "topic": "GCP Options for Deploying Machine Learning Pipeline > Cloud AutoML",
    "reason": "Cloud AutoML is a Machine Learning service designed for developers who want to incorporate ML into their application without having to learn much about the details of ML, lead to the minimum effort. And AutoML Tables builds machine learning models based on Structured Data (as the question said: a dataset with 1 million rows and 32 features/columns). \nWhile AutoML Natural Language can be worked on “Unstructured Data”. Kubeflow and Spark MLlib requires much more setup, which result to more efforts on every part",
    "point": 1,
    "timer": 90,
    "page": 250,
    "multiple_choices": false
  },

  {
    "question_id": 150,
    "question": "The chief financial officer of your company would like to build a program to predict which customers will likely be late paying their bills. The company has an enterprise data warehouse in BigQuery containing all the data related to customers, billing, and payments. The company does not have anyone with machine learning experience, but it does have analysts and data scientists experienced in SQL, Python, and Java. The analysts and data scientists will generate and test a large number of models, so they prefer fast model building. What service would you recommend using to build the model?",
    "choice_a": "Kubeflow",
    "choice_b": "Spark MLib",
    "choice_c": "BigQuery ML",
    "choice_d": "AutoML Tables",
    "correct_answer": "C",
    "chapter": 9,
    "chapter_name": "Deploying Machine Learning Pipelines",
    "topic": "GCP Options for Deploying Machine Learning Pipeline > BigQuery ML",
    "reason": "BigQuery ML is a machine learning service provided by GCP that enables users to build and deploy machine learning models directly within BigQuery using SQL queries. It is well-suited for users who are familiar with SQL, including Data Analysts and Data Scientists in this scenario. BigQuery ML is designed for fast model building and is integrated directly into the BQ data warehouse.\nSince the company has an enterprise data warehouse in BigQuery containing all the relevant data, using BigQuery ML allows for a seamless integration of machine learning into their existing data infrastructure.",
    "point": 1,
    "timer": 90,
    "page": 251,
    "multiple_choices": false
  },

  {
    "question_id": 151,
    "question": "A team of researchers is analyzing buying patterns of customers of a national grocery store chain. They are especially interested in sets of products that customers frequently by together. The researchers plan to use association rules for this frequent pattern mining. What machine learning option in GCP would you recommend?",
    "choice_a": "Cloud Dataflow",
    "choice_b": "Spark MLib",
    "choice_c": "BigQuery ML",
    "choice_d": "AutoML Tables",
    "correct_answer": "B",
    "chapter": 9,
    "chapter_name": "Deploying Machine Learning Pipelines",
    "topic": "GCP Options for Deploying Machine Learning Pipeline > Spark Machine Learning",
    "reason": "Spark MLib algorithms includes:\n■ Support vector machines for classification\n■ Linear regression for forecasting\n■ Decision trees, random forests, and gradient-boosted trees for classification\n■ Naive Bayes for classification\n■ K-means clustering and streaming k-means for segmenting\n■ Latent Dirichlet allocation for segmenting\n■ Singular value decomposition and principal component analysis for dimensionality \nreduction\n■ Frequent Pattern (FP)–growth and association rules for frequent pattern mining",
    "point": 1,
    "timer": 90,
    "page": 252,
    "multiple_choices": false
  },

  {
    "question_id": 152,
    "question": "A data scientist has developed a machine learning model to identify fraudulent transactions in a credit card dataset. The model achieves near-perfect accuracy on the training data but performs poorly when applied to a new set of credit card transactions. What is the likely issue with the machine learning model?",
    "choice_a": "Overfitting",
    "choice_b": "Underfitting",
    "choice_c": "Too much training data",
    "choice_d": "Using tenfold cross-validation for evaluation",
    "correct_answer": "A",
    "chapter": 9,
    "chapter_name": "Deploying Machine Learning Pipelines",
    "topic": "Structure of ML Pipelines > Model Training > Overfitting",
    "reason": "The model achieving near-perfect accuracy on the training data but performing poorly on new data suggests that the model has memorized the training data too well, including noise and specific details that do not generalize to other instances. This is a classic sign of overfitting.\nOverfitting models are highly tailored to the training data and struggle to generalize to unseen data, leading to a drop in performance on new examples",
    "point": 1,
    "timer": 90,
    "page": 245,
    "multiple_choices": false
  },

  {
    "question_id": 153,
    "question": "A data scientist is working on a binary classification task to predict whether customers will churn or not based on historical data. Despite efforts to train a machine learning model, the accuracy is not as high as expected, and the predictions lack robustness. What is a potential issue that the data scientist should investigate?",
    "choice_a": "Insufficient model complexity",
    "choice_b": "Inadequate training duration",
    "choice_c": "Lack of feature engineering",
    "choice_d": "Inconsistent labeling in the training data",
    "correct_answer": "C",
    "chapter": 9,
    "chapter_name": "Deploying Machine Learning Pipelines",
    "topic": "Structure of ML Pipelines > Model Training > Feature Selection",
    "reason": "If the accuracy is not as high as expected, and the predictions lack robustness, it suggests that the features used in the model may not be effectively capturing the underlying patterns related to customer churn. Feature engineering involves creating new features, transforming existing ones, or selecting the most relevant features to enhance the model's predictive performance. In this scenario, the data scientist should investigate whether the choice and engineering of features are sufficient to represent the complexity of the problem",
    "point": 1,
    "timer": 90,
    "page": 245,
    "multiple_choices": false
  },

  {
    "question_id": 154,
    "question": "A data scientist is developing a machine learning model to predict housing prices based on various features such as square footage, number of bedrooms, and location. After training the model, it performs well on the training data but exhibits a significant drop in performance when evaluated on a separate test dataset. What would you recommend to improve the model's generalization?",
    "choice_a": "Increase the complexity of the model architecture",
    "choice_b": "Use L1 or L2 regularization when evaluating",
    "choice_c": "Feature engineering on the existing variables",
    "choice_d": "Tune the hyperparameters more",
    "correct_answer": "D",
    "chapter": 9,
    "chapter_name": "Deploying Machine Learning Pipelines",
    "topic": "Structure of ML Pipelines > Model Training",
    "reason": "In this scenario, where the model performs well on the training data but poorly on the test data, the issue might be related to the model's hyperparameters. Tuning the hyperparameters, such as adjusting learning rates, regularization strengths, or model complexity parameters, can help improve the model's generalization to new, unseen data\n>> Note that the solution for this could also be “Use L1 or L2 regularization” but when training and not evaluating, so option B is wrong.",
    "point": 1,
    "timer": 90,
    "page": 245,
    "multiple_choices": false
  },

  {
    "question_id": 155,
    "question": "A manufacturing company is implementing a quality control system where cameras are placed on the production line to inspect products for defects. The goal is to identify and classify defects in real-time directly on the production floor. Which GCP service would you recommend for this scenario?",
    "choice_a": "AutoML Vision Object Detection",
    "choice_b": "AutoML Vision Edge - Object Detection",
    "choice_c": "AutoML Video Intelligence Classification",
    "choice_d": "AutoML Video Intelligence Object Tracking",
    "correct_answer": "B",
    "chapter": 9,
    "chapter_name": "Deploying Machine Learning Pipelines",
    "topic": "GCP Options for Deploying Machine Learning Pipeline",
    "reason": "In a manufacturing setting where real-time processing on the production floor is required, AutoML Vision Edge - Object Detection is suitable. This service is designed for on-device inference, making it well-suited for scenarios where processing needs to happen directly at the edge, close to the source of data. This ensures low latency and efficient handling of the quality control process without relying on cloud-based processing.",
    "point": 1,
    "timer": 90,
    "page": 250,
    "multiple_choices": false
  },

  {
    "question_id": 156,
    "question": "A wildlife conservation organization is deploying cameras in a nature reserve to monitor the movement and behavior of endangered species. The organization aims to track the paths and interactions of specific animals over time. Which GCP service would you recommend for this scenario?",
    "choice_a": "AutoML Vision Object Detection",
    "choice_b": "AutoML Vision Edge - Object Detection",
    "choice_c": "AutoML Video Intelligence Classification",
    "choice_d": "AutoML Video Intelligence Object Tracking",
    "correct_answer": "D",
    "chapter": 9,
    "chapter_name": "Deploying Machine Learning Pipelines",
    "topic": "GCP Options for Deploying Machine Learning Pipeline",
    "reason": "For the scenario involving tracking the movement and behavior of specific animals over time, AutoML Video Intelligence Object Tracking is the appropriate choice. This service is designed to track and trace specific objects within a video sequence, making it well-suited for applications where the goal is to monitor and analyze the trajectories of objects (in this case, endangered species) over different frames in the video.",
    "point": 1,
    "timer": 90,
    "page": 250,
    "multiple_choices": false
  },

  {
    "question_id": 157,
    "question": "A media company is building a platform to analyze video content for content moderation purposes. The goal is to identify and categorize potentially inappropriate content within user-uploaded videos. Which GCP service would you recommend for this scenario?",
    "choice_a": "AutoML Vision Object Detection",
    "choice_b": "AutoML Vision Edge - Object Detection",
    "choice_c": "AutoML Video Intelligence Classification",
    "choice_d": "AutoML Video Intelligence Object Tracking",
    "correct_answer": "C",
    "chapter": 9,
    "chapter_name": "Deploying Machine Learning Pipelines",
    "topic": "GCP Options for Deploying Machine Learning Pipeline",
    "reason": "For content moderation purposes where the focus is on analyzing video content for potential inappropriate content, AutoML Video Intelligence Classification is suitable. This service is designed to classify and categorize video content based on its overall content. It allows for the identification of specific objects or scenes within videos, making it effective for content moderation applications where the goal is to understand the content of the entire video.",
    "point": 1,
    "timer": 90,
    "page": 250,
    "multiple_choices": false
  },

  {
    "question_id": 158,
    "question": "A technology startup is looking to deploy and manage machine learning workflows efficiently, utilizing Kubernetes for scalability. The team has skilled DevOps engineers and data scientists who are proficient in Python and want a platform that supports end-to-end machine learning workflows. What machine learning option in GCP would you recommend?",
    "choice_a": "Kubeflow",
    "choice_b": "Spark MLib",
    "choice_c": "BigQuery ML",
    "choice_d": "AutoML Tables",
    "correct_answer": "A",
    "chapter": 9,
    "chapter_name": "Deploying Machine Learning Pipelines",
    "topic": "GCP Options for Deploying Machine Learning Pipeline > Kubeflow",
    "reason": "Kubeflow is an open-source platform for deploying, monitoring, and managing machine learning workflows on Kubernetes. It is well-suited for organizations that prioritize scalability, containerization, and efficient management of machine learning pipelines using Kubernetes.\nIn this scenario, where the team has skilled DevOps engineers and values scalability, Kubeflow would provide a powerful platform for deploying and managing machine learning workflows efficiently.",
    "point": 1,
    "timer": 90,
    "page": 251,
    "multiple_choices": false
  },

  {
    "question_id": 159,
    "question": "A large enterprise has a Hadoop-based data lake and is dealing with massive amounts of data. The data science team is familiar with Apache Spark and is looking for a machine learning library that integrates seamlessly with Spark for distributed computing. What machine learning option in GCP would you recommend?",
    "choice_a": "Kubeflow",
    "choice_b": "Spark MLib",
    "choice_c": "BigQuery ML",
    "choice_d": "AutoML Tables",
    "correct_answer": "B",
    "chapter": 9,
    "chapter_name": "Deploying Machine Learning Pipelines",
    "topic": "GCP Options for Deploying Machine Learning Pipeline > Spark Machine Learning",
    "reason": "Spark MLib is part of the Apache Spark ecosystem and is designed for distributed machine learning tasks. It integrates seamlessly with Spark, making it a suitable choice for organizations dealing with massive amounts of data stored in a Hadoop-based data lake.\nIn this scenario, where the data science team is already familiar with Apache Spark and requires distributed computing capabilities, Spark MLib would be the recommended choice.",
    "point": 1,
    "timer": 90,
    "page": 252,
    "multiple_choices": false
  },

  {
    "question_id": 160,
    "question": "A data science team is working on a regression problem where the dataset contains a large number of features, and there is a concern about the possibility of some features being irrelevant or redundant. The team wants to apply a regularization technique that not only helps in preventing overfitting but also has the ability to perform feature selection by driving some feature coefficients to exactly zero. Which regularization technique would you recommend for this scenario?",
    "choice_a": "Lasso Regularization",
    "choice_b": "Ridge Regularization",
    "choice_c": "Elastic Net Regularization",
    "choice_d": "Feature Scaling",
    "correct_answer": "A",
    "chapter": 9,
    "chapter_name": "Deploying Machine Learning Pipelines",
    "topic": "Structure of ML Pipelines > Model Training > Regularization",
    "reason": "Lasso Regularization, also known as L1 regularization, introduces a penalty term based on the absolute values of the coefficients. This has the effect of driving some coefficients to exactly zero, effectively performing feature selection.\nIn scenarios where there are concerns about irrelevant or redundant features and the desire is to perform feature selection, Lasso is the appropriate choice.",
    "point": 1,
    "timer": 90,
    "page": 246,
    "multiple_choices": false
  },

  {
    "question_id": 161,
    "question": "In a machine learning project, the team is working on a linear regression model with multiple features. The dataset has a potential issue of multi-collinearity, where some of the features are highly correlated. The team wants to apply a technique that can mitigate this issue by penalizing large coefficients and encourage a more balanced solution. Which technique would you recommend for handling multi-collinearity in this scenario?",
    "choice_a": "Lasso Regularization",
    "choice_b": "Ridge Regularization",
    "choice_c": "Principal Component Analysis (PCA)",
    "choice_d": "Feature Scaling",
    "correct_answer": "B",
    "chapter": 9,
    "chapter_name": "Deploying Machine Learning Pipelines",
    "topic": "Structure of ML Pipelines > Model Training > Regularization",
    "reason": "Ridge Regression (L2 regularization) introduces a penalty term based on the squared values of the coefficients. This penalization helps to mitigate multi-collinearity by reducing the impact of highly correlated features",
    "point": 1,
    "timer": 90,
    "page": 246,
    "multiple_choices": false
  },

  {
    "question_id": 162,
    "question": "A data science team is building a machine learning pipeline using Spark to preprocess data and apply a specific transformation to convert categorical features into numerical representations. Which Spark MLlib component would you primarily use for this task?",
    "choice_a": "Data Frames",
    "choice_b": "Transformers",
    "choice_c": "Estimators",
    "choice_d": "Parameters",
    "correct_answer": "B",
    "chapter": 9,
    "chapter_name": "Deploying Machine Learning Pipelines",
    "topic": "GCP Options for Deploying Machine Learning Pipeline > Spark Machine Learning",
    "reason": "In Spark MLlib, Transformers are components that transform one DataFrame into another, often by applying a specific transformation to the features. Transformations such as converting categorical features into numerical representations are typically performed using Transformers.",
    "point": 1,
    "timer": 90,
    "page": 252,
    "multiple_choices": false
  },

  {
    "question_id": 163,
    "question": "A data science team is tasked with building a predictive model for customer churn using Spark MLlib. They need to choose and configure an algorithm for model training. What Spark MLlib component would they primarily use to implement the chosen algorithm?",
    "choice_a": "Data Frames",
    "choice_b": "Transformers",
    "choice_c": "Estimators",
    "choice_d": "Evaluators",
    "correct_answer": "C",
    "chapter": 9,
    "chapter_name": "Deploying Machine Learning Pipelines",
    "topic": "GCP Options for Deploying Machine Learning Pipeline > Spark Machine Learning",
    "reason": "In Spark MLlib, Estimators are components that implement algorithms for model training. When building a predictive model for customer churn, the data science team would typically use an Estimator to fit the model on the training data.",
    "point": 1,
    "timer": 90,
    "page": 252,
    "multiple_choices": false
  },

  {
    "question_id": 164,
    "question": "From Spark Machine Learning, what are the algorithms that Spark MLlib does not support?",
    "choice_a": "K-Means++ Initialization",
    "choice_b": "Linear Regression for forecasting",
    "choice_c": "Latent Dirichlet allocation for segmenting",
    "choice_d": "Singular value decomposition and principal component analysis for dimensionality reduction",
    "choice_e": "Support Vector Machine",
    "choice_f": "Multilayer Perceptron Neural Network",
    "choice_g": "Random Forest Regression",
    "correct_answer": ["A","B","G"],
    "chapter": 9,
    "chapter_name": "Deploying Machine Learning Pipelines",
    "topic": "GCP Options for Deploying Machine Learning Pipeline > Spark Machine Learning",
    "reason": "Spark MLib algorithms includes:\n■ Support vector machines for classification\n■ Linear regression for forecasting\n■ Decision trees, random forests, and gradient-boosted trees for classification\n■ Naive Bayes for classification\n■ K-means clustering and streaming k-means for segmenting\n■ Latent Dirichlet allocation for segmenting\n■ Singular value decomposition and principal component analysis for dimensionality \nreduction\n■ Frequent Pattern (FP)–growth and association rules for frequent pattern mining",
    "point": 1,
    "timer": 90,
    "page": 252,
    "multiple_choices": false
  },

  {
    "question_id": 165,
    "question": "You are in the early stages of developing a machine learning model using a framework that requires high-precision arithmetic and benefits from massive parallelization. Your data set fits within 32 GB of memory. You want to use Jupyter Notebooks to build the model iteratively and analyze results. What kind of infrastructure would you use? ",
    "choice_a": "A TPU pod with at least 32 TPUs",
    "choice_b": "A single TPU",
    "choice_c": "A single server with a GPU",
    "choice_d": "A managed instance group of 4 VMs with 64 GB of memory each",
    "correct_answer": "C",
    "chapter": 10,
    "chapter_name": "Choosing Training and Serving Infrastructure",
    "topic": "Hardware Accelerators > Choosing Between CPUs, GPUs and TPUs",
    "reason": "The requirements call for high-precision arithmetic and parallelization, so that indicates using a GPU. There is a small amount of data, and you want to work with it interactively, so a single machine with a GPU will suffice",
    "point": 1,
    "timer": 90,
    "page": 261,
    "multiple_choices": false
  },

  {
    "question_id": 166,
    "question": "You are developing a machine learning model that will predict failures in high-precision machining equipment. The equipment has hundreds of IoT sensors that send telemetry data every second. Thousands of the machines are in use in a variety of operating conditions. A year’s worth of data is available for model training. You plan to use TensorFlow, a synchronous training strategy, and TPUs. Which of the following strategies would you use?",
    "choice_a": "MirroredStrategy",
    "choice_b": "CentralStorageStrategy",
    "choice_c": "MultiWorkerMirroredStrategy",
    "choice_d": "TPUStrategy",
    "correct_answer": "D",
    "chapter": 10,
    "chapter_name": "Choosing Training and Serving Infrastructure",
    "topic": "Distributed Model Training",
    "reason": "This strategy is tailored for TPUs and can take advantage of their capabilities efficiently. It is suitable for distributed synchronous training across multiple TPUs, which aligns well with your use case involving thousands of machines and telemetry data.",
    "point": 1,
    "timer": 90,
    "page": 263,
    "multiple_choices": false
  },

  {
    "question_id": 167,
    "question": "Your client has developed a machine learning model that detects anomalies in equity trading time-series data. The model runs as a service in a Google Kubernetes Engine (GKE) cluster deployed in the us-west-1 region. A number of financial institutions in New York and London are interested in licensing the technology, but they are concerned that the total time required to make a prediction is longer than they can tolerate. The distance between the serving infrastructure and New York is about 4,800 kilometers, and the distance to London is about 8,000 kilometers. This is an example of what kind of problem with serving a machine learning model?",
    "choice_a": "Overfitting",
    "choice_b": "Underfitting",
    "choice_c": "Latency",
    "choice_d": "Scalability",
    "correct_answer": "C",
    "chapter": 10,
    "chapter_name": "Choosing Training and Serving Infrastructure",
    "topic": "Edge Computing with GCP",
    "reason": "In the context of data traveling over a distance of 8000 km, latency becomes a concern due to the finite speed of data transmission, which is determined by the physical medium (such as fiber-optic cables) and the speed of light. This is an example of a latency problem that might be resolved by serving the model closer to where the data is generated",
    "point": 1,
    "timer": 90,
    "multiple_choices": false
  },

  {
    "question_id": 168,
    "question": "A study of global climate change is building a network of environmental sensors distributed across the globe. Sensors are deployed in groups of 12 sensors and a gateway. An analytics pipeline is implemented in GCP. Data will be ingested by Cloud Pub/Sub and analyzed using the stream processing capabilities of Cloud Dataflow. The analyzed data will be stored in BigQuery for further analysis by scientists. The bandwidth between the gateways and the GCP is limited and sometimes unreliable. The scientists have determined that they need the average temperature, pressure, and humidity measurements of each group of 12 sensors for a one-minute period. Each sensor sends data to the gateway every second. This generates 720 data points (12 sensors × 60 seconds) every minute for each of the three measurements. The scientists only need the one-minute average for temperature, pressure, and humidity. What data processing strategy would you implement? ",
    "choice_a": "Send all 720 data points for each measurement each minute to a Cloud Pub/Sub message, generate the averages using Cloud Dataflow, and write those results to BigQuery.",
    "choice_b": "Average all 720 data points for each measurement each minute, send the average to a Cloud Pub/Sub message, and use Cloud Dataflow and write those results to BigQuery.",
    "choice_c": "Send all 720 data points for each measurement each minute to a BigQuery streaming insert into a partitioned table.",
    "choice_d": "Average all 720 data points for each measurement each minute, send the average to a Cloud Pub/Sub message, and use Cloud Dataflow and write those results to BigQuery",
    "correct_answer": "B",
    "chapter": 10,
    "chapter_name": "Choosing Training and Serving Infrastructure",
    "topic": "Edge Computing with GCP",
    "reason": "720 data points every minute for each of the 3 measurements is a huge amount of data. The perfect processing strategy would be to use just doing the average operation on the sensor side (we can do this through Cloud IoT Edge Computing to just apply the logic directly within the device as soon as the data entered), then send the averages to a Cloud Pub/Sub message service to reduce the amount of data transmitted over the network, lastly with Cloud Dataflow, the job is simplified as it only needs to consume the averaged data points from Cloud Pub/Sub and further process them if needed. The processing logic in Cloud Dataflow would be lighter compared to calculating averages over large datasets. Write the final results (averages) to BigQuery for further analysis.",
    "point": 1,
    "timer": 90,
    "page": 267,
    "multiple_choices": false
  },

  {
    "question_id": 169,
    "question": "Your DevOps team is deploying an IoT system to monitor and control environmental conditions in your building. You are using a standard IoT architecture. Which of the following components would you not use?",
    "choice_a": "Edge devices",
    "choice_b": "Gateways",
    "choice_c": "Repeater",
    "choice_d": "Cloud platform services",
    "correct_answer": "C",
    "chapter": 10,
    "chapter_name": "Choosing Training and Serving Infrastructure",
    "topic": "Edge Computing with GCP",
    "reason": "Repeaters are used in networks to boost signal strength. There is no indication that this is needed, and in any case, that is a network implementation choice and not a comparable part of the IoT architecture of the other components. Within the building range, repeater would not be appropriate, while all the other devices (Edge devices, gateways and cloud platform services) are much more suitable in a IoT environment.",
    "point": 1,
    "timer": 90,
    "page": 267,
    "multiple_choices": false
  },

  {
    "question_id": 170,
    "question": "In the Google Cloud Platform IoT reference model, which of the following GCP services is used for ingestion?",
    "choice_a": "Cloud Storage",
    "choice_b": "BigQuery streaming inserts",
    "choice_c": "Cloud Pub/Sub",
    "choice_d": "Cloud Bigtable",
    "correct_answer": "C",
    "chapter": 10,
    "chapter_name": "Choosing Training and Serving Infrastructure",
    "topic": "Edge Computing with GCP",
    "reason": "In GCP, IoT data can be ingested only using the following services:\n■ Cloud Pub/Sub for scalable ingestion\n■ IoT Core MQTT, a Message Queue Telemetry Transport broker that uses an industry standard binary protocol\n■ Stackdriver Monitoring and Stackdriver Logging, which can also be used to ingest and process metrics and log messages",
    "point": 1,
    "timer": 90,
    "page": 267,
    "multiple_choices": false
  },

  {
    "question_id": 171,
    "question": "A startup is developing a product for autonomous vehicle manufacturers that will enable its vehicles to detect objects better in adverse weather conditions. The product uses a machine learning model built on TensorFlow. Which of the following options would you choose to serve this model?",
    "choice_a": "On GKE using TensorFlow Training (TFJob)",
    "choice_b": "On Compute Engine using managed instance groups",
    "choice_c": "On Edge TPU devices in the vehicles",
    "choice_d": "On GPUs in the vehicles",
    "correct_answer": "C",
    "chapter": 10,
    "chapter_name": "Choosing Training and Serving Infrastructure",
    "topic": "Edge TPU",
    "reason": "The vehicle needs to detect object in a bad weather condition (low-latency is essential), which is exactly a case for Edge TPU devices. Here are all the cases using Edge TPU:\n■ Image processing in a manufacturing environment\n■ Robotic control\n■ Anomaly detection in time series data\n■ Voice recognition and response",
    "point": 1,
    "timer": 90,
    "page": 268,
    "multiple_choices": false
  },

  {
    "question_id": 172,
    "question": "In the Google Cloud Platform IoT reference model, which of the following GCP services is used for stream processing?",
    "choice_a": "Cloud Storage",
    "choice_b": "BigQuery streaming inserts",
    "choice_c": "Cloud Pub/Sub",
    "choice_d": "Cloud Dataflow",
    "correct_answer": "D",
    "chapter": 10,
    "chapter_name": "Choosing Training and Serving Infrastructure",
    "topic": "Cloud IoT",
    "reason": "The reference architecture for Cloud IoT includes the following:\n■ Cloud IoT Core for device management\n■ Cloud ML Engine for training and deploying machine learning models\n■ Cloud Dataflow for streaming and batch analytics\n■ Cloud Pub/Sub for ingestion\n■ BigQuery for data warehousing and ad hoc querying",
    "point": 1,
    "timer": 90,
    "page": 268,
    "multiple_choices": false
  },

  {
    "question_id": 173,
    "question": "You have developed a TensorFlow model using only the most basic TensorFlow operations and no custom operations. You have a large volume of data available for training, but by your estimates it could take several weeks to train the model using a 16 vCPU Compute Engine instance. Which of the following should you try instead?",
    "choice_a": "A 32 vCPU Compute Engine instance",
    "choice_b": "A TPU pod",
    "choice_c": "A GKE cluster using on CPUs",
    "choice_d": "App Engine Second Generation",
    "correct_answer": "B",
    "chapter": 10,
    "chapter_name": "Choosing Training and Serving Infrastructure",
    "topic": "Hardware Accelerators > Tensor Processing Units",
    "reason": "TPUs are recommended for:\n■ Models without custom TensorFlow operations inside the main training loop\n■ Models  that can take weeks or months to train on CPUs or GPUS\n■ Models dominated by matrix multiplications",
    "point": 1,
    "timer": 90,
    "page": 262,
    "multiple_choices": false
  },

  {
    "question_id": 174,
    "question": "You have developed a machine learning model that uses a specialized Fortran library that is optimized for highly parallel, high-precision arithmetic. You only have access to the compiled code and cannot make any changes to source code. You want to use an accelerator to reduce the training time of your model. Which of the following options would you try first?",
    "choice_a": "A Compute Engine instance with GPUs",
    "choice_b": "A TPU pod",
    "choice_c": "A Compute Engine instance with CPUs only ",
    "choice_d": "Cloud Functions",
    "correct_answer": "A",
    "chapter": 10,
    "chapter_name": "Choosing Training and Serving Infrastructure",
    "topic": "Distributed Model Training",
    "reason": "A Fortran library optimized for highly parallel, high-precision arithmetic that runs on GPUs would be a good option for training this model",
    "point": 1,
    "timer": 90,
    "page": 263,
    "multiple_choices": false
  },

  {
    "question_id": 175,
    "question": "You are tasked with deploying a web application that requires high computational power for parallelizable tasks such as image processing. After initial testing, you find that the application response time is not optimal on the current infrastructure, which uses a 16 vCPU Compute Engine instance. The application code is CPU-bound and does not heavily rely on specialized hardware acceleration. What should you consider to improve the application's performance?",
    "choice_a": "Upgrade to a 32 vCPU Compute Engine instance.",
    "choice_b": "Deploy a TPU pod.",
    "choice_c": "Configure a GKE cluster using only CPUs.",
    "choice_d": "Migrate to App Engine Second Generation.",
    "correct_answer": "A",
    "chapter": 10,
    "chapter_name": "Choosing Training and Serving Infrastructure",
    "topic": "Hardware Accelerators",
    "reason": "Increased CPU Resources: Upgrading to a 32 vCPU Compute Engine instance provides more computational power by doubling the number of virtual CPUs. This can potentially lead to improved performance for CPU-bound tasks as more parallel processing can be performed concurrently.\nScalability: This option is a straightforward and scalable solution. If the application's performance is primarily limited by CPU resources, increasing the number of vCPUs is a direct way to address this issue without introducing new complexities.\nCompatibility: The application code is CPU-bound and does not heavily rely on specialized hardware acceleration. Therefore, a straightforward increase in general-purpose CPU resources is likely to yield improvements without the need for specialized hardware like TPUs (Tensor Processing Units).",
    "point": 1,
    "timer": 90,
    "page": 262,
    "multiple_choices": false
  },

  {
    "question_id": 176,
    "question": "A smart city initiative aims to monitor air quality using a network of sensors deployed across various locations. Each sensor measures air pollutants such as particulate matter and carbon monoxide. The sensors are organized into groups of 10, and each group has a dedicated edge device for data aggregation. The data needs to be processed in near real-time to provide timely insights to city officials. The communication between the edge devices and the central cloud infrastructure is limited and occasionally unreliable.\nWhich data processing strategy would be most suitable for this scenario?",
    "choice_a": "Send raw data from each sensor to Cloud Pub/Sub, perform real-time data aggregation using Cloud Dataflow, and store the aggregated results in BigQuery.",
    "choice_b": "Aggregate sensor data at the edge device, compute the average pollutant levels for each group, and send the aggregated results to Cloud Pub/Sub for further analysis using Cloud Dataflow. Store the results in BigQuery.",
    "choice_c": "Stream raw sensor data directly to BigQuery using streaming inserts, and perform real-time analysis and aggregation within BigQuery.",
    "choice_d": "Preprocess the raw sensor data at the edge device, compute the average pollutant levels for each group, and send the preprocessed results to BigQuery for s",
    "correct_answer": "B",
    "chapter": 10,
    "chapter_name": "Choosing Training and Serving Infrastructure",
    "topic": "Edge Computing with GCP",
    "reason": "Option B aligns with the principles of edge computing by performing data aggregation at the edge, minimizing data transmission, and enabling near real-time processing for the smart city air quality monitoring initiative.",
    "point": 1,
    "timer": 90,
    "page": 267,
    "multiple_choices": false
  },

  {
    "question_id": 177,
    "question": "You are working on a project where you need to distribute the training of a machine learning model across multiple GPUs within a single machine. The dataset is large, and synchronous training is required. Each GPU should contribute to the model's parameter updates. Which TensorFlow strategy would you choose?",
    "choice_a": "MirroredStrategy",
    "choice_b": "CentralStorageStrategy",
    "choice_c": "MultiWorkerMirroredStrategy",
    "choice_d": "TPUStrategy",
    "correct_answer": "B",
    "chapter": 10,
    "chapter_name": "Choosing Training and Serving Infrastructure",
    "topic": "Distributed Model Training",
    "reason": "CentralStorageStrategy is suitable for scenarios where you have multiple GPUs within a single machine. It aggregates the gradients across all GPUs and applies updates to the model's parameters on the central CPU device. This is useful when you have a large dataset and want to distribute the training workload across multiple GPUs on a single machine.",
    "point": 1,
    "timer": 90,
    "page": 263,
    "multiple_choices": false
  },

  {
    "question_id": 178,
    "question": "You are part of a team developing a machine learning model for image classification. The dataset is large, and you want to leverage multiple machines for distributed training. Your team has access to a cluster of machines for this purpose. Which TensorFlow strategy would you choose for synchronous training?",
    "choice_a": "MirroredStrategy",
    "choice_b": "CentralStorageStrategy",
    "choice_c": "MultiWorkerMirroredStrategy",
    "choice_d": "TPUStrategy",
    "correct_answer": "C",
    "chapter": 10,
    "chapter_name": "Choosing Training and Serving Infrastructure",
    "topic": "Distributed Model Training",
    "reason": "MultiWorkerMirroredStrategy is designed for distributed training across multiple machines. It allows synchronous training with data parallelism by replicating the model on each device (machine), computing gradients locally, and then aggregating them across all workers. This is suitable for scenarios where you want to distribute the training workload across a cluster of machines.",
    "point": 1,
    "timer": 90,
    "page": 263,
    "multiple_choices": false
  },

  {
    "question_id": 179,
    "question": "13.Serving a Machine Learning Model is the process of making the model available to make predictions for other services. What considerations should be kept in mind when deploying serving models?",
    "choice_a": "Redundancy",
    "choice_b": "Latency",
    "choice_c": "Opportunities",
    "choice_d": "Version Management",
    "choice_e": "Qualitative Requirement",
    "choice_f": "Scalability",
    "choice_g": "Online Versus Batch Prediction",
    "choice_h": "Accelerations",
    "correct_answer": ["B","D","F","G"],
    "chapter": 10,
    "chapter_name": "Choosing Training and Serving Infrastructure",
    "topic": "Serving Models",
    "reason": "When serving models, you need to keep in mind the following:\n■ Latency\n■ Scalability\n■ Version management\n■ Online versus batch prediction",
    "point": 1,
    "timer": 90,
    "page": 264,
    "multiple_choices": false
  },

  {
    "question_id": 180,
    "question": "In the context of edge computing environments, where decentralized devices play a crucial role in processing data, what specific types of information do edge devices contribute?",
    "choice_a": "Metadata",
    "choice_b": "State Information about the status of the device",
    "choice_c": "Graphical Data",
    "choice_d": "Telemetry Data",
    "choice_e": "Binding Data",
    "correct_answer": ["A", "B", "D"],
    "chapter": 10,
    "chapter_name": "Choosing Training and Serving Infrastructure",
    "topic": "Edge Computing",
    "reason": "Edge devices provide three kinds of data:\n■ Metadata, such as the device ID, model number, and serial number\n■ State information about the status of the device\n■ Telemetry, which is data collected by the device",
    "point": 1,
    "timer": 90,
    "page": 267,
    "multiple_choices": true
  },

  {
    "question_id": 181,
    "question": "In the realm of Edge TPU applications, considering its diverse utility in decentralized processing, which scenario is not considered a suitable use case for Edge TPU deployment?",
    "choice_a": "Robotic Control",
    "choice_b": "Anomaly detection ",
    "choice_c": "Voice recognition an response",
    "choice_d": "Image Processing",
    "choice_e": "Real-time Financial Trading Analysis",
    "choice_f": "None of the above",
    "correct_answer": "E",
    "chapter": 10,
    "chapter_name": "Choosing Training and Serving Infrastructure",
    "topic": "Edge TPU",
    "reason": "Edge TPUs are generally well-suited for tasks that involve machine learning inference at the edge, such as image processing, voice recognition, and anomaly detection. However, real-time financial trading analysis often involves complex calculations and data analytics that may be better addressed with specialized hardware or cloud-based solutions. While Edge TPUs excel in certain tasks, they may not be the most optimized solution for the demanding computational requirements of real-time financial trading analysis",
    "point": 1,
    "timer": 90,
    "page": 268,
    "multiple_choices": false
  },

  {
    "question_id": 182,
    "question": "In the context of developing an end-to-end IoT solution on Google Cloud Platform (GCP), which combination of GCP services would be most suitable for device management, training and deploying machine learning models, streaming analytics, and data warehousing?",
    "choice_a": "A combination of services including those for device management, machine learning models, streaming analytics, and data warehousing.",
    "choice_b": "A combination of services with Cloud Pub/Sub for device management, Cloud ML Engine for training and deploying machine learning models, Cloud Dataflow for streaming and batch analytics, and Cloud Storage for data warehousing.",
    "choice_c": "A combination of services including those for device management, training and deploying machine learning models, streaming analytics, and Cloud Firestore for data warehousing.",
    "choice_d": "A combination of services with Cloud Pub/Sub for device management, Cloud Dataflow for training and deploying machine learning models, Cloud IoT Core for streaming and batch analytics, and BigQuery for data warehousing.",
    "correct_answer": "B",
    "chapter": 10,
    "chapter_name": "Choosing Training and Serving Infrastructure",
    "topic": "Cloud IoT",
    "reason": "This combination aligns with the requirements of device management, machine learning models, streaming analytics, and data warehousing, considering the broader context of an end-to-end IoT solution on GCP.\nThe reference architecture for Cloud IoT includes the following:\n■ Cloud IoT Core for device management\n■ Cloud ML Engine for training and deploying machine learning models\n■ Cloud Dataflow for streaming and batch analytics\n■ Cloud Pub/Sub for ingestion\n■ BigQuery for data warehousing and ad hoc querying",
    "point": 1,
    "timer": 90,
    "page": 268,
    "multiple_choices": false
  },

  {
    "question_id": 183,
    "question": "You are building a machine learning model to predict the sales price of houses. You have 7 years of historical data, including 18 features of houses and their sales price. What type of machine learning algorithm would you use?",
    "choice_a": "Classifier ",
    "choice_b": "Regression ",
    "choice_c": "Decision trees",
    "choice_d": "Reinforcement learning",
    "correct_answer": "B",
    "chapter": 11,
    "chapter_name": "Measuring, Monitoring, and Troubleshooting Machine Learning Models",
    "topic": "Machine Learning Algorithms > Regression",
    "reason": "Sales price is a numeric-value data type. Regression algorithms map one set of variables to other continuous variable",
    "point": 1,
    "timer": 90,
    "page": 280,
    "multiple_choices": false
  },

  {
    "question_id": 184,
    "question": "You have been asked to build a machine learning model that will predict if a news article is a story about technology or another topic. Which of the following would you use?",
    "choice_a": "Logistic regression",
    "choice_b": "K-means clustering",
    "choice_c": "Simple linear regression",
    "choice_d": "Multiple linear regression",
    "correct_answer": "A",
    "chapter": 11,
    "chapter_name": "Measuring, Monitoring, and Troubleshooting Machine Learning Models",
    "topic": "Machine Learning Algorithms > Classification",
    "reason": "When tasked with predicting the category of a news article, a classification model from supervised learning is the most suitable approach. Classification models are designed to categorize data into distinct classes or groups. Among the given options, only \"Logistic Regression\" is a specific type of classification algorithm, well-suited for predicting categorical outcomes. On the other hand, \"K-means clustering\" is an unsupervised learning model, and \"Simple Linear Regression\" and \"Multiple Linear Regression\" are regression models, which are not appropriate for predicting categorical data. Therefore, the optimal choice for this task is \"Logistic Regression,\" aligning with the nature of classification problems.",
    "point": 1,
    "timer": 90,
    "page": 277,
    "multiple_choices": false
  },

  {
    "question_id": 185,
    "question": "A startup is collecting IoT data from sensors placed on manufacturing equipment. The sensors send data every five seconds. The data includes a machine identifier, a timestamp, and several numeric values. The startup is developing a model to identify unusual readings. What type of unsupervised learning technique would they use?",
    "choice_a": "Clustering ",
    "choice_b": "K-means ",
    "choice_c": "Anomaly detection",
    "choice_d": "Reinforcement learning",
    "correct_answer": "C",
    "chapter": 11,
    "chapter_name": "Measuring, Monitoring, and Troubleshooting Machine Learning Models",
    "topic": "Machine Learning Algorithms > Anomaly Detection",
    "reason": "Anomaly Detection is the process of identifying unexpected patterns in data. Since the startup is developing an unsupervised model to detect “unusual readings” (or Unexpected Pattern), the type of technique mentioned must be Anomaly Detection",
    "point": 1,
    "timer": 90,
    "page": 282,
    "multiple_choices": false
  },

  {
    "question_id": 186,
    "question": "You want to study deep learning and decide to start with the basics. You build a binary classifier using an artificial neuron. What algorithm would you use to train it? ",
    "choice_a": "Perceptron",
    "choice_b": "SVM",
    "choice_c": "Decision tree",
    "choice_d": "Linear regression",
    "correct_answer": "A",
    "chapter": 11,
    "chapter_name": "Measuring, Monitoring, and Troubleshooting Machine Learning Models",
    "topic": "Deep Learning",
    "reason": "Deep learning uses the concept of an artificial neuron as a building block. A simple, single neuron model is known as a perceptron and is trained using the perceptron algorithm. The perceptron algorithm is a supervised learning algorithm for binary classification tasks. It is a type of artificial neuron or a single-layer neural network.",
    "point": 1,
    "timer": 90,
    "page": 283,
    "multiple_choices": false
  },

  {
    "question_id": 187,
    "question": "A group of machine learning engineers has been assigned the task of building a machine learning model to predict the price of gold on the open market. Many features could be used, and the engineers believe that the optimal model will be complex. They want to understand the minimum predictive value of a model that they can build from the data that they have. What would they build?",
    "choice_a": "Multiclass classifier",
    "choice_b": "K clusters",
    "choice_c": "Baseline model",
    "choice_d": "Binary classifier",
    "correct_answer": "C",
    "chapter": 11,
    "chapter_name": "Measuring, Monitoring, and Troubleshooting Machine Learning Models",
    "topic": "Engineering Machine Learning Models > F1 Score",
    "reason": "Baseline Model is a simple, often naive, model that serves as a reference point for comparing the performance of more complex models. It helps set a minimum standard (minimum predictive value) that any more sophisticated model should surpass. Baseline models also demonstrate the minimal performance that you should expect from a more complex model",
    "point": 1,
    "timer": 90,
    "page": 290,
    "multiple_choices": false
  },

  {
    "question_id": 188,
    "question": "You are preparing a dataset to build a classifier. The data includes several continuous values, each in the range 0.00 to 100.00. You’d like to have a discrete feature derive each continuous value. What type of feature engineering would you use?",
    "choice_a": "Bucketing",
    "choice_b": "Dimension reduction",
    "choice_c": "Principal component analysis",
    "choice_d": "Gradient descent",
    "correct_answer": "A",
    "chapter": 11,
    "chapter_name": "Measuring, Monitoring, and Troubleshooting Machine Learning Models",
    "topic": "Engineering Machine Learning Models > Feature Engineering",
    "reason": "If the data to build the classifier model included several fields with lots of continuous values (float from 0 to 100), consider using a data preparation technique called “Bucketing” (or “binning”). It is the process of converting a continuous feature into ranges values, and it is used to reduce the impact of minor observation errors.",
    "point": 1,
    "timer": 90,
    "page": 286,
    "multiple_choices": false
  },

  {
    "question_id": 189,
    "question": "You have been tasked with developing a classification model. You have reviewed the data that you will use for training and testing and realize that there are a number of outliers that you think might lead to overfitting. What technique would you use to reduce the impact of those outliers on the model?",
    "choice_a": "Gradient descent",
    "choice_b": "Large number of epochs",
    "choice_c": "L2 regularization",
    "choice_d": "Backpropagation",
    "correct_answer": "C",
    "chapter": 11,
    "chapter_name": "Measuring, Monitoring, and Troubleshooting Machine Learning Models",
    "topic": "Engineering Machine Learning Models  > Training Models",
    "reason": "L2 regularization (also known as weight decay) is a technique used to prevent overfitting by adding a penalty term to the loss function based on the squared magnitudes of the model's weights. This penalty discourages overly large weights, making the model less sensitive to outliers and noise in the training data.",
    "point": 1,
    "timer": 90,
    "page": 288,
    "multiple_choices": false
  },

  {
    "question_id": 190,
    "question": "You have built a deep learning neural network that has 8 layers, and each layer has 100 fully connected nodes. The model fits the training data quite well with an F1 score of 98 out of 100. The model performs poorly when the test data is used, resulting in an F1 score of 62 out of 100. What technique would you use to try to improve performance of this model?",
    "choice_a": "User more epochs",
    "choice_b": "Dropout",
    "choice_c": "Add more layers",
    "choice_d": "ReLU",
    "correct_answer": "B",
    "chapter": 11,
    "chapter_name": "Measuring, Monitoring, and Troubleshooting Machine Learning Models",
    "topic": "Deep Learning",
    "reason": "Deep learning models can make use of dropout, which is a form of regularization. During training, a random number of nodes are ignored when calculating weighted sums. This simulates removing a node from a network and, in effect, reduces the network’s ability to learn and tends to produce models less likely to overfit the training data.",
    "point": 1,
    "timer": 90,
    "page": 283,
    "multiple_choices": false
  },

  {
    "question_id": 191,
    "question": "Your team is building a classifier to identify counterfeit products on an e-commerce site. Most of the products on the site are legitimate, and only about 3 percent of the products are counterfeit. You are concerned that, as is, the dataset will lead to a model that always predicts that products are legitimate. Which of the following techniques could you use to prevent this?",
    "choice_a": "Undersampling",
    "choice_b": "Dropout",
    "choice_c": "L1 regularization",
    "choice_d": "AUC",
    "correct_answer": "A",
    "chapter": 11,
    "chapter_name": "Measuring, Monitoring, and Troubleshooting Machine Learning Models",
    "topic": "Common Sources of Error in Machine Learning Models > Unbalanced Training Sets",
    "reason": "If the ratio between classes in classification model is 97 to 3 (97% legitimate, 3% counterfeit), the training data is heavily unbalanced. One technique could be used to scale the data is “Undersampling” method, where you would use fewer instances of the majority class than you would get with random sampling.",
    "point": 1,
    "timer": 90,
    "page": 292,
    "multiple_choices": false
  },

  {
    "question_id": 192,
    "question": "You are reviewing a dataset and find that the data is relatively high quality. There are no missing values and only a few outliers. You build a model based on the dataset that has high accuracy, precision, and recall when applied to the test data. When you use the model in production, however, it renders poor results. What might have caused this condition?",
    "choice_a": "Applying L1 regularization",
    "choice_b": "Applying dropout",
    "choice_c": "Reporting bias",
    "choice_d": "Automation bias",
    "correct_answer": "C",
    "chapter": 11,
    "chapter_name": "Measuring, Monitoring, and Troubleshooting Machine Learning Models",
    "topic": "Common Sources of Error in Machine Learning Models >  Types of Bias",
    "reason": "When the data is high quality, no missing values, only a minor number of outliers, and the modeling of the data is also high accuracy, precision and recall, it means that the data does not reflects of the world, this problem is called “Reporting Bias”. Additional data should be collected until the distribution of data in a dataset refl ects the \ndistribution in the population as a whole.",
    "point": 1,
    "timer": 90,
    "page": 293,
    "multiple_choices": false
  },

  {
    "question_id": 193,
    "question": "You are tasked with developing a machine learning model to classify emails as either spam or not spam based on various email features. The dataset consists of labeled examples, indicating whether each email is spam or not. What type of machine learning algorithm would you use?",
    "choice_a": "Classifier",
    "choice_b": "Regression",
    "choice_c": "Decision trees",
    "choice_d": "Random Forest",
    "correct_answer": "A",
    "chapter": 11,
    "chapter_name": "Measuring, Monitoring, and Troubleshooting Machine Learning Models",
    "topic": "Machine Learning Algorithms > Classification",
    "reason": "Classification algorithms are well-suited for tasks where the goal is to categorize data into distinct classes or labels based on learned patterns.",
    "point": 1,
    "timer": 90,
    "page": 277,
    "multiple_choices": false
  },

  {
    "question_id": 194,
    "question": "You are provided with a dataset containing images of handwritten digits (0 to 9) along with corresponding labels indicating the correct digit for each image. Your task is to train a model that can accurately predict the digit in a given image. What type of machine learning would you apply?",
    "choice_a": "Supervised Learning",
    "choice_b": "Unsupervised Learning",
    "choice_c": "Reinforcement Learning",
    "choice_d": "Semi-Supervised Learning",
    "correct_answer": "A",
    "chapter": 11,
    "chapter_name": "Measuring, Monitoring, and Troubleshooting Machine Learning Models",
    "topic": "Machine Learning Algorithms > Supervised Learning",
    "reason": "In this scenario, the dataset contains labeled examples where each image is associated with the correct digit label. The task is to predict the digit in new, unseen images based on the patterns learned from the labeled data. This aligns with the characteristics of Supervised Learning",
    "point": 1,
    "timer": 90,
    "page": 277,
    "multiple_choices": false
  },

  {
    "question_id": 195,
    "question": "You have a dataset with customer purchase history, and your goal is to identify groups of customers with similar purchasing behavior without using any predefined labels. What type of machine learning would you use for this task?",
    "choice_a": "Supervised Learning",
    "choice_b": "Unsupervised Learning",
    "choice_c": "Reinforcement Learning",
    "choice_d": "Semi-Supervised Learning",
    "correct_answer": "B",
    "chapter": 11,
    "chapter_name": "Measuring, Monitoring, and Troubleshooting Machine Learning Models",
    "topic": "Machine Learning Algorithms > Unsupervised Learning",
    "reason": "In this case, there are no predefined labels indicating customer groups. The objective is to discover patterns or clusters within the data based on the inherent structure of the purchase history. This aligns with the characteristics of Unsupervised Learning",
    "point": 1,
    "timer": 90,
    "page": 281,
    "multiple_choices": false
  },

  {
    "question_id": 196,
    "question": "In a simulated environment, you are training a robotic arm to perform specific tasks by providing feedback on its actions. The goal is for the robotic arm to learn optimal actions based on trial and error. What type of machine learning is most suitable for this scenario?",
    "choice_a": "Supervised Learning",
    "choice_b": "Unsupervised Learning",
    "choice_c": "Reinforcement Learning",
    "choice_d": "Semi-Supervised Learning",
    "correct_answer": "C",
    "chapter": 11,
    "chapter_name": "Measuring, Monitoring, and Troubleshooting Machine Learning Models",
    "topic": "Machine Learning Algorithms > Reinforcement Learning",
    "reason": "In this scenario, the robotic arm learns through trial and error, receiving feedback in the form of rewards or penalties based on its actions. This aligns with the characteristics of Reinforcement Learning",
    "point": 1,
    "timer": 90,
    "page": 282,
    "multiple_choices": false
  },

  {
    "question_id": 197,
    "question": "You are working on a project to develop a neural network for image recognition. During the model evaluation, you notice that the training loss is decreasing, but the validation loss has started to increase, indicating potential overfitting. What technique would you employ to address the overfitting issue and improve the model's generalization performance?",
    "choice_a": "L1 Regularization",
    "choice_b": "Feature scaling",
    "choice_c": "Data augmentation",
    "choice_d": "Backpropagation",
    "correct_answer": "D",
    "chapter": 11,
    "chapter_name": "Measuring, Monitoring, and Troubleshooting Machine Learning Models",
    "topic": "Engineering Machine Learning Models  > Training Models",
    "reason": "In this scenario, the primary concern is overfitting, as evidenced by the increasing validation loss while the training loss is decreasing. Overfitting occurs when the model becomes too complex and starts to memorize noise in the training data, leading to poor generalization. \nBackpropagation is the primary algorithm used to train neural networks by adjusting weights based on the computed gradients. While backpropagation itself is essential for training, the key here is likely to explore techniques within the backpropagation framework, such as introducing regularization terms (e.g., dropout, L2 regularization) to prevent overfitting.",
    "point": 1,
    "timer": 90,
    "page": 288,
    "multiple_choices": false
  },

  {
    "question_id": 198,
    "question": "You are working on a project where the goal is to predict the genre of a given movie based on its features such as plot summary, cast, and director. The dataset contains multiple genres, and each movie can belong to only one genre. What type of machine learning model would you use for this task?",
    "choice_a": "Binary Classification Models",
    "choice_b": "Multiclass Classification Models",
    "choice_c": "Regression Models",
    "choice_d": "Clustering Models",
    "correct_answer": "B",
    "chapter": 11,
    "chapter_name": "Measuring, Monitoring, and Troubleshooting Machine Learning Models",
    "topic": "Machine Learning Algorithms > Classifications",
    "reason": "The task is to predict the genre of a movie, and the dataset contains multiple genres. Since each movie can belong to only one genre, this is a typical multiclass classification problem where the model needs to assign each instance to one of several predefined classes.",
    "point": 1,
    "timer": 90,
    "page": 288,
    "multiple_choices": false
  },

  {
    "question_id": 199,
    "question": "You are given a dataset of images, each belonging to one of three categories: cats, dogs, or birds. Your task is to build a model that can accurately classify these images into their respective categories. What machine learning algorithm would be suitable for this problem?",
    "choice_a": "K-Nearest Neighbor",
    "choice_b": "Linear Regression",
    "choice_c": "Support Vector Machine",
    "choice_d": "K-Means Clustering",
    "correct_answer": "C",
    "chapter": 11,
    "chapter_name": "Measuring, Monitoring, and Troubleshooting Machine Learning Models",
    "topic": "Machine Learning Algorithms > Classifications",
    "reason": "The task involves classifying images into three distinct categories: cats, dogs, or birds. Support Vector Machine (SVM) is a suitable algorithm for multiclass classification tasks. SVM can handle multiple classes and is effective in scenarios where there are clear decision boundaries between classes, making it a good fit for image classification tasks.",
    "point": 1,
    "timer": 90,
    "page": 288,
    "multiple_choices": false
  },

  {
    "question_id": 200,
    "question": "You are working on a machine learning project tasked with predicting the likelihood of flight delays based on historical data. The dataset includes various features such as departure time, airline, airport, and weather conditions. As you delve into the analysis, you notice that the existing features might not fully capture the complexity of factors influencing flight delays. The departure time, for instance, is represented as a single timestamp, and weather conditions are provided in a raw format. What approach would you consider to enhance the predictive power of your model and uncover hidden patterns within the data?",
    "choice_a": "Hyperparameter Tuning",
    "choice_b": "Cross-Validation",
    "choice_c": "Feature Engineering",
    "choice_d": "Ensemble Learning",
    "correct_answer": "C",
    "chapter": 11,
    "chapter_name": "Measuring, Monitoring, and Troubleshooting Machine Learning Models",
    "topic": "Engineering Machine Learning Models > Feature Engineering",
    "reason": "Feature Engineering involves creating new features or transforming existing ones to provide more meaningful information to the model. This might include breaking down the departure time into relevant components, extracting relevant information from the raw weather data, or combining certain features to create interactions that the model can leverage for better predictions.",
    "point": 1,
    "timer": 90,
    "page": 286,
    "multiple_choices": false
  },

  {
    "question_id": 201,
    "question": "In a dataset containing information about customer preferences, you observe that the same category is represented in multiple ways, such as 'Electronics,' 'Electrical Appliances,' and 'Gadgets.' This inconsistent use of codes and categories poses a challenge in analyzing and understanding the data. What type of data quality issue does this represent?",
    "choice_a": "Missing data",
    "choice_b": "Invalid values",
    "choice_c": "Inconsistent use of codes and categories",
    "choice_d": "Data that is not representative of the population at large",
    "correct_answer": "C",
    "chapter": 11,
    "chapter_name": "Measuring, Monitoring, and Troubleshooting Machine Learning Models",
    "topic": "Common Sources of Error in Machine Learning Models > Data Quality",
    "reason": "The inconsistent representation of the same category in multiple ways, such as different codes or categories for similar concepts, indicates a data quality issue related to Inconsistent use of codes and categories.",
    "point": 1,
    "timer": 90,
    "page": 292,
    "multiple_choices": false
  },

  {
    "question_id": 202,
    "question": "During a review of a healthcare dataset, you discover that some patient records contain entries like '999' for blood pressure, which is medically impossible. This introduces inaccuracies and challenges in drawing meaningful insights from the data. What type of data quality issue is most evident in this case?",
    "choice_a": "Missing data",
    "choice_b": "Wrong data types",
    "choice_c": "Inconsistent use of codes and categories",
    "choice_d": "Data that is not representative of the population at large",
    "correct_answer": "D",
    "chapter": 11,
    "chapter_name": "Measuring, Monitoring, and Troubleshooting Machine Learning Models",
    "topic": "Common Sources of Error in Machine Learning Models > Data Quality",
    "reason": "Entries like '999' for blood pressure in a healthcare dataset suggest that the data may not accurately reflect the characteristics of the broader patient population. This issue aligns with Data that is not representative of the population at large",
    "point": 1,
    "timer": 90,
    "page": 292,
    "multiple_choices": false
  },

  {
    "question_id": 203,
    "question": "What is not a type of bias in Machine Learning?",
    "choice_a": "Identifier Bias",
    "choice_b": "Reporting Bias",
    "choice_c": "Group Attribution Bias",
    "choice_d": "Selection Bias",
    "correct_answer": "A",
    "chapter": 11,
    "chapter_name": "Measuring, Monitoring, and Troubleshooting Machine Learning Models",
    "topic": "Common Sources of Error in Machine Learning Models >  Types of Bias",
    "reason": "Reporting Bias, Group Attribution Bias and Selection Bias are name of types of bias in Machine Learning",
    "point": 1,
    "timer": 90,
    "page": 293,
    "multiple_choices": false
  },

  {
    "question_id": 204,
    "question": "You are leading a team in developing a recommendation system for an online shopping platform. During the model evaluation phase, you observe that the recommendations tend to be disproportionately skewed toward a specific demographic group, leading to potential disparities in product suggestions. As you investigate further, you discover that the training data inadvertently emphasizes certain characteristics of this group. In the context of this scenario, what type of bias is most likely influencing the recommendation system's outcomes?",
    "choice_a": "Sampling Bias",
    "choice_b": "Selection Bias",
    "choice_c": "Group Attribution Bias",
    "choice_d": "Reporting Bias",
    "correct_answer": "C",
    "chapter": 11,
    "chapter_name": "Measuring, Monitoring, and Troubleshooting Machine Learning Models",
    "topic": "Common Sources of Error in Machine Learning Models >  Types of Bias",
    "reason": "In this situation, where the recommendation system shows a disproportionate preference for a specific demographic group due to unintentional biases in the training data, the type of bias most likely influencing the outcomes is Group Attribution Bias",
    "point": 1,
    "timer": 90,
    "page": 293,
    "multiple_choices": false
  },

  {
    "question_id": 205,
    "question": "You are building a machine learning model to analyze unusual events in traffic through urban areas. Your model needs to distinguish cars, bikes, pedestrians, and buildings. It is especially important that the model be able to identify and track moving vehicles. Video will be streamed to your service from cameras mounted on traffic lights. What GCP service would you use for the object analysis and tracking? ",
    "choice_a": "Cloud Video Intelligence API",
    "choice_b": "Cloud Vision API",
    "choice_c": "Cloud Inference API",
    "choice_d": "Cloud Dataflow",
    "correct_answer": "A",
    "chapter": 12,
    "chapter_name": "Leveraging Prebuilt Models as a Service",
    "topic": "Video AI",
    "reason": "Video Intelligence API or Video AI is a service provided by Google Cloud Platform that can be used to identifies cars, bikes, pedestrians and buildings as they are moving in video recorded. This service has pretrained models that automatically recognize objects in videos",
    "point": 1,
    "timer": 90,
    "page": 300,
    "multiple_choices": false
  },
  
  {
    "question_id": 206,
    "question": "A startup is building an educational support platform for students from ages 5–18. The platform will allow teachers to post assignments and conduct assessments. Students will be able to upload content, including text and images. The founder of the startup wants to make sure that explicit images are not uploaded. What GCP service would you use? ",
    "choice_a": "Cloud Video Intelligence API",
    "choice_b": "Cloud Vision API",
    "choice_c": "Cloud Inference API",
    "choice_d": "Cloud Dataprep",
    "correct_answer": "B",
    "chapter": 12,
    "chapter_name": "Leveraging Prebuilt Models as a Service",
    "topic": "Vision AI",
    "reason": "The service that can be used to analyse and classify images to make sure explicit images never got uploaded is called Vision AI (a service to apply Machine Learning to images)\nFurthermore, some other operations that can be performed by Vision AI are:\n■ Detecting text in images\n■ Detecting handwriting in images\n■ Detecting text in PDF, TIFF, and other types of files\n■ Detecting faces\n■ Detecting hints for suggested vertices for a cropped region of an image\n■ Detecting image properties\n■ Detecting landmarks\n■ Detecting logos\n■ Detecting multiple objects\n■ Detecting explicit content (Safe Search)\n■ Detecting web entities and page",
    "point": 1,
    "timer": 90,
    "page": 298,
    "multiple_choices": false
  },

  
  {
    "question_id": 207,
    "question": "You are using the Cloud Vision API to detect landmarks in images. You are using the batch processing with asynchronous requests. The source images for each batch is in a separate Cloud Storage bucket. There are between 1 and 5,000 images in each bucket. Each batch request processes one bucket. All buckets have the same access controls. Sometimes, the operations succeed and sometimes they fail. What could be the cause of the errors?",
    "choice_a": "Cloud Video Intelligence API",
    "choice_b": "Some buckets have more than 2,000 images.",
    "choice_c": "There is an issue with IAM settings.",
    "choice_d": "Images have to be uploaded directly from a device, not a Cloud Storage bucket.",
    "correct_answer": "B",
    "chapter": 12,
    "chapter_name": "Leveraging Prebuilt Models as a Service",
    "topic": "Vision AI",
    "reason": "There can be up to 2000 images to be submitted with a single API call. Since all the buckets have the same controls (No issue with IAM settings), the only reason for the operations to be succeed and fail sometimes is the limit. If the bucket is found with less than 2000 images, it is succeed, and if there are more than 2000 images, it causes an error (reached the boundary)",
    "point": 1,
    "timer": 90,
    "page": 300,
    "multiple_choices": false
  },
  
  {
    "question_id": 208,
    "question": "Your team is building a chatbot to support customer support. Domain experts from the customer support team have identified several kinds of questions that the system should support, including questions about returning products, getting technical help, and asking for product recommendations. You will use Dialogflow to implement the chatbot. What component of Dialogflow will you configure to support the three question types?",
    "choice_a": "Entities",
    "choice_b": "Fulfillments",
    "choice_c": "Integrations",
    "choice_d": "Intents",
    "correct_answer": "D",
    "chapter": 12,
    "chapter_name": "Leveraging Prebuilt Models as a Service",
    "topic": "Dialogflow",
    "reason": "Intents categorize a speaker’s intention for a single statement, such as asking for recommendation",
    "point": 1,
    "timer": 90,
    "page": 302,
    "multiple_choices": false
  },

  {
    "question_id": 209,
    "question": "A developer asks for your help tuning a text-to-speech service that is used with a health and wellness app. The app is designed to run on watches and other personal devices. The sound quality is not as good as the developer would like. What would you suggest trying to improve the quality of sound?",
    "choice_a": "Change the device specification to optimize for a wearable device ",
    "choice_b": "Change from standard to WaveNet-quality voice",
    "choice_c": "Encode the text in Base64",
    "choice_d": "Options A and B ",
    "choice_e": "Options A, B, and C",
    "correct_answer": "D",
    "chapter": 12,
    "chapter_name": "Leveraging Prebuilt Models as a Service",
    "topic": "Cloud Text-to-Speech API",
    "reason": "WaveNet voice has a higher audio output quality than standard voice, but it costs more than the standard. And also, changing the device specification to optimize for a wearable device would increase the audio quality depending on the devices",
    "point": 1,
    "timer": 90,
    "page": 303,
    "multiple_choices": false
  },

  {
    "question_id": 210,
    "question": "A developer asks for your help tuning a speech-to-text service that is used to transcribe text recorded on a mobile device. The quality of the transcription is not as good as expected. The app uses LINEAR16 encoding and a sampling rate of 12,000 Hz. What would you suggest to try to improve the quality? ",
    "choice_a": "Use WaveNet option",
    "choice_b": "Increase the sampling rate to at least 16,000 Hz",
    "choice_c": "Use Speech Synthesis Markup Language to configure conversion parameters",
    "choice_d": "Options A and B",
    "correct_answer": "B",
    "chapter": 12,
    "chapter_name": "Leveraging Prebuilt Models as a Service",
    "topic": "Cloud Speech-to-Text API",
    "reason": "Google has recommended that the audio should be captured at a sampling rate of 16000 Hz or higher for the best results. WaveNet option and Speech Synthesis Markup are only available for “Cloud Text-to-Speed API”",
    "point": 1,
    "timer": 90,
    "page": 303,
    "multiple_choices": false
  },

  {
    "question_id": 211,
    "question": "You have developed a mobile app that helps travelers quickly find sites of interest. The app uses the GCP Translation service. The initial release of the app used the REST API, but adoption has grown so much that you need higher performance from the API and plan to use gRCP instead. What changes do you need to make to the way that you use the Translation service?",
    "choice_a": "Use the WaveNet option",
    "choice_b": "Use Translation API Basic ",
    "choice_c": "Use Translation API Advanced",
    "choice_d": "Option A or B",
    "correct_answer": "C",
    "chapter": 12,
    "chapter_name": "Leveraging Prebuilt Models as a Service",
    "topic": "Translation",
    "reason": "Translation API Basic does not supports gRCP, but Translation API Advanced supports gRCP.",
    "point": 1,
    "timer": 90,
    "page": 304,
    "multiple_choices": false
  },

  {
    "question_id": 212,
    "question": "You are experimenting with the GCP Translation API. You have created a Jupyter Notebook and plan to use Python 3 to build a proof-of-concept system. What are the first two operations that you would execute in your notebook to start using the Translation API?",
    "choice_a": "Import Translation libraries and create a translation client",
    "choice_b": "Create a translation client and encode text in UTF-8",
    "choice_c": "Create a translation client, and set a variable to TRANSLATE to pass in as a parameter to the API function call",
    "choice_d": "Import Translation libraries, and set a variable to TRANSLATE to pass in as a parameter to the API function call",
    "correct_answer": "D",
    "chapter": 12,
    "chapter_name": "Leveraging Prebuilt Models as a Service",
    "topic": "Translation",
    "reason": "The first two operations that I would execute in my notebook to start using the Translation API is to: Importing Translation Libraries >> Set a variable to TRANSLATE to pass in as a parameter to call the API function",
    "point": 1,
    "timer": 90,
    "page": 304,
    "multiple_choices": false
  },

  {
    "question_id": 213,
    "question": "You have been hired by a law firm to help analyze a large volume of documents related to a legal case. There are approximately 10,000 documents ranging from 1 to 15 pages in length. They are all written in English. The lawyers hiring you want to understand who is mentioned in each document so that they can understand how those individuals worked together. What functionality of the Natural Language API would you use?",
    "choice_a": "Identifying entities ",
    "choice_b": "Analyzing sentiment associated with each entity",
    "choice_c": "Analyzing sentiment of the overall text",
    "choice_d": "Generating syntactic analysis",
    "correct_answer": "A",
    "chapter": 12,
    "chapter_name": "Leveraging Prebuilt Models as a Service",
    "topic": "Natural Language",
    "reason": "In Natural Language Processing (NLP), entities are objects or concepts that hold specific meaning. The API can identify and extract entities from a given text, such as names of people, organizations, locations, dates, product names, etc. The entity recognition feature helps to understand the key elements and topics mentioned in the text.",
    "point": 1,
    "timer": 90,
    "page": 305,
    "multiple_choices": false
  },

  {
    "question_id": 214,
    "question": "As a founder of an e-commerce startup, you are particularly interested in engaging with your customers. You decide to use the GCP Recommendations AI API using the “others you may like” recommendation type. You want to maximize the likelihood that users will engage with your recommendations. What optimization objective would you choose?",
    "choice_a": "Click-through rate (CTR)",
    "choice_b": "Revenue per order",
    "choice_c": "Conversation rate",
    "choice_d": "Total revenue",
    "correct_answer": "A",
    "chapter": 12,
    "chapter_name": "Leveraging Prebuilt Models as a Service",
    "topic": "Recommendations AI API",
    "reason": "For the goal of maximizing the likelihood that users will engage with recommendations in an e-commerce setting, the most suitable optimization objective would be Click-through rate (CTR). Click-through rate measures the ratio of users who click on a recommendation to the total number of users who viewed the recommendations. Choosing CTR as the optimization objective is suitable when the primary goal is to maximize user engagement by encouraging users to click on recommended items.",
    "point": 1,
    "timer": 90,
    "page": 307,
    "multiple_choices": false
  },

  {
    "question_id": 215,
    "question": "Your e-commerce startup has been growing rapidly since its launch six months ago. You are starting to notice that the rate of revenue growth is slowing down. Your board of directors is asking you to develop a strategy to increase revenue. You decide to personalize each customer’s experience. One of the ways in which you plan to implement your strategy is by showing customers products that they are likely to interact with next. What recommendation type would you use?",
    "choice_a": "Others you may like",
    "choice_b": "Frequently bought together",
    "choice_c": "Recommended for you",
    "choice_d": "Recently viewed",
    "correct_answer": "C",
    "chapter": 12,
    "chapter_name": "Leveraging Prebuilt Models as a Service",
    "topic": "Recommendations AI API",
    "reason": "",
    "point": 1,
    "timer": 90,
    "page": 307,
    "multiple_choices": false
  },

  {
    "question_id": 216,
    "question": "You work for an enterprise with a large fleet of vehicles. The vehicles are equipped with several sensors that transmit data about fuel utilization, speed, and other equipment operating characteristics. The chief of operations has asked you to investigate the feasibility of building a predictive maintenance application that can help identify breakdowns before they occur. You decide to prototype an anomaly detection model as a first step. You want to build this as quickly as possible, so you decide to use a machine learning service. Which GCP service would you use?",
    "choice_a": "Cloud Inference API",
    "choice_b": "AutoML Tables",
    "choice_c": "AutoML Vision",
    "choice_d": "Cloud Anomaly Detection API",
    "correct_answer": "A",
    "chapter": 12,
    "chapter_name": "Leveraging Prebuilt Models as a Service",
    "topic": "Cloud Inference API",
    "reason": "The Cloud Inference API is designed for this kind of time-series analysis and anomaly detection. Cloud Inference API is a simple, highly efficient and scalable system that makes it easier for businesses and developers to quickly gather insights from typed time series datasets.",
    "point": 1,
    "timer": 90,
    "page": 308,
    "multiple_choices": false
  },

  {
    "question_id": 217,
    "question": "You are involved in a research project that requires a detailed analysis of the grammatical structure and relationships between words in a set of academic papers. Your objective is to gain insights into the syntax of the content to identify patterns and structures. Which functionality of the Natural Language API would be most suitable for this task?",
    "choice_a": "Identifying entities",
    "choice_b": "Analyzing sentiment associated with each entity",
    "choice_c": "Analyzing sentiment of the overall text",
    "choice_d": "Generating syntactic analysis",
    "correct_answer": "D",
    "chapter": 12,
    "chapter_name": "Leveraging Prebuilt Models as a Service",
    "topic": "Natural Language",
    "reason": "When the objective is to conduct a detailed analysis of the grammatical structure and relationships between words in a set of academic papers, the most relevant functionality is generating syntactic analysis. Syntactic analysis involves parsing the text to understand its grammatical structure, including relationships between words, phrases, and clauses.",
    "point": 1,
    "timer": 90,
    "page": 305,
    "multiple_choices": false
  },

  {
    "question_id": 218,
    "question": "You are working for a marketing agency that is conducting a sentiment analysis of customer reviews for a new product launch. The goal is to understand the overall sentiment expressed in each review and categorize them as positive, negative, or neutral. Which functionality of the Natural Language API would you use to achieve this?",
    "choice_a": "Identifying entities",
    "choice_b": "Analyzing sentiment associated with each entity",
    "choice_c": "Analyzing sentiment of the overall text",
    "choice_d": "Generating syntactic analysis",
    "correct_answer": "C",
    "chapter": 12,
    "chapter_name": "Leveraging Prebuilt Models as a Service",
    "topic": "Natural Language",
    "reason": "In the context of analyzing sentiment of the overall text, the goal is to understand the general sentiment expressed in a document, review, or piece of text. This functionality is particularly useful in scenarios where you want to determine whether the overall tone is positive, negative, or neutral. In the given situational question, the marketing agency is interested in categorizing customer reviews for a new product launch",
    "point": 1,
    "timer": 90,
    "page": 305,
    "multiple_choices": false
  },

  {
    "question_id": 219,
    "question": "The scientists are tasked with improving the chatbot's ability to gather information from users, allowing it to make personalized recommendations based on user preferences. Which component of Dialogflow should they configure to enhance the chatbot's capability to interact with user profiles and preferences?",
    "choice_a": "Entities",
    "choice_b": "Fulfillments",
    "choice_c": "Integrations",
    "choice_d": "Intents",
    "correct_answer": "C",
    "chapter": 12,
    "chapter_name": "Leveraging Prebuilt Models as a Service",
    "topic": "",
    "reason": "",
    "point": 1,
    "timer": 90,
    "page": 302,
    "multiple_choices": false
  },

  {
    "question_id": 220,
    "question": "Your business is enhancing the functionality of a customer support chatbot, focusing on handling various backend processes such as order tracking, inventory management, and updating customer profiles. Which component of Dialogflow should you configure to enable seamless communication with external systems and fulfill these backend tasks?",
    "choice_a": "Entities",
    "choice_b": "Fulfillments",
    "choice_c": "Integrations",
    "choice_d": "Intents",
    "correct_answer": "B",
    "chapter": 12,
    "chapter_name": "Leveraging Prebuilt Models as a Service",
    "topic": "Dialogflow",
    "reason": "In this scenario, configuring \"Fulfillments\" in Dialogflow is crucial for handling backend processes and ensuring effective communication with external systems.",
    "point": 1,
    "timer": 90,
    "page": 302,
    "multiple_choices": false
  },

  {
    "question_id": 221,
    "question": "As the lead data scientist in a health and fitness app development team, you are focused on personalizing workout recommendations for users. To encourage users to explore different exercise routines, you want to suggest workouts similar to the ones they have shown interest in. What recommendation type should you incorporate into the app?",
    "choice_a": "Others you may like",
    "choice_b": "Frequently bought together",
    "choice_c": "Recommended for you",
    "choice_d": "Recently viewed",
    "correct_answer": "A",
    "chapter": 12,
    "chapter_name": "Leveraging Prebuilt Models as a Service",
    "topic": "Recommendations AI API",
    "reason": "To inspire users to try new workout routines similar to their preferences, the most suitable recommendation type is \"Others you may like.\"",
    "point": 1,
    "timer": 90,
    "page": 307,
    "multiple_choices": false
  },

  {
    "question_id": 222,
    "question": "You are the chief marketing officer of a subscription-based digital magazine platform, and you're implementing the GCP Recommendations AI API to suggest additional magazines to users based on their reading preferences. Your primary goal is to encourage users to upgrade their subscription plans during the recommendation interactions. What optimization objective would you choose to align with this objective?",
    "choice_a": "Click-through rate (CTR)",
    "choice_b": "Revenue per order",
    "choice_c": "Conversation rate",
    "choice_d": "Total revenue",
    "correct_answer": "C",
    "chapter": 12,
    "chapter_name": "Leveraging Prebuilt Models as a Service",
    "topic": "Recommendations AI API",
    "reason": "To maximize the likelihood of users upgrading their subscription plans during the recommendation interactions, the appropriate optimization objective is \"Conversion rate.\" This focuses on successfully converting recommendation engagements into upgraded subscriptions.",
    "point": 1,
    "timer": 90,
    "page": 307,
    "multiple_choices": false
  },

  {
    "question_id": 223,
    "question": "In your role as a software developer, you are integrating the GCP Translation service into a language learning app that translates short phrases for users. The initial implementation used gRPC for communication, but due to certain constraints, you decide to switch to a simpler and more straightforward approach. What changes do you need to make to the way you use the Translation service?",
    "choice_a": "Use the WaveNet option",
    "choice_b": "Use Translation API Basic",
    "choice_c": "Use Translation API Advanced",
    "choice_d": "All of the above",
    "correct_answer": "B",
    "chapter": 12,
    "chapter_name": "Leveraging Prebuilt Models as a Service",
    "topic": "Translation",
    "reason": "To simplify the communication approach for the language learning app and maintain compatibility, the appropriate change to make is \"Use Translation API Basic\". This choice ensures a straightforward and effective integration with the Translation service.",
    "point": 1,
    "timer": 90,
    "page": 304,
    "multiple_choices": false
  },

  {
    "question_id": 224,
    "question": "Your company is actively collecting and analyzing time-series data to gain insights into customer behaviors and operational patterns. To perform real-time analysis of this data (JSON format), extract meaningful correlations and listing active datasets, you decide to implement a solution leveraging Google Cloud Platform. Which GCP service is specifically designed for such case?",
    "choice_a": "Cloud Inference API",
    "choice_b": "Cloud Video Intelligence API",
    "choice_c": "AutoML Tables",
    "choice_d": "Cloud Dataflow",
    "choice_e": "BigQuery",
    "correct_answer": "A",
    "chapter": 12,
    "chapter_name": "Leveraging Prebuilt Models as a Service",
    "topic": "Cloud Inference API",
    "reason": "The Cloud Inference API provides for processing time-series datasets, including ingesting from JSON formats, removing data, and listing active datasets. It also supports inference queries over datasets, including correlation queries, variation in frequency over time, and probability of events given evidence of those events in the dataset.",
    "point": 1,
    "timer": 90,
    "page": 308,
    "multiple_choices": false
  },

  {
    "question_id": 225,
    "question": "What is NOT a metric to evaluate the performance of recommendations of the Recommendations AI API?",
    "choice_a": "Recommender-engaged AOV",
    "choice_b": "Total Revenue from all recorded purchase events",
    "choice_c": "AOV",
    "choice_d": "Click-through rate",
    "choice_e": "Revenue per event",
    "correct_answer": "E",
    "chapter": 12,
    "chapter_name": "Leveraging Prebuilt Models as a Service",
    "topic": "Cloud Inference API",
    "reason": "The Recommendations AI API tracks metrics to help you evaluate the performance of recommendations made. The metrics include the following:\n■ Total revenue from all recorded purchase events: This is the sum of all revenue from all purchases.\n■ Recommender-engaged revenue: This is the revenue from purchase events that include at least one recommended item.\n■ Recommendation revenue: This is the revenue from recommended items.\n■ Average order value (AOV): This is the average of orders from all purchase events.\n■ Recommender-engaged AOV: This is the average value of orders that include at least one recommended item.\n■ Click-through rate: This is the number of product views from a recommendation.\n■ Conversion rate: This is the number of times that an item was added to a cart divided by the total number of recommendations. \n■ Revenue from recommendations: This is the total revenue from all recommendations.",
    "point": 1,
    "timer": 90,
    "page": 304,
    "multiple_choices": false
  },

  {
    "question_id": 226,
    "question": "Which is not a name of a Prebuilt Model as a Service from Google Cloud Platform?",
    "choice_a": "Video AI",
    "choice_b": "Video-to-Text API",
    "choice_c": "Dialogflow",
    "choice_d": "Inference API",
    "correct_answer": "B",
    "chapter": 12,
    "chapter_name": "Leveraging Prebuilt Models as a Service",
    "topic": "Chapter 12’s Essentials",
    "reason": "Video AI, Dialogflow, and Inference API are valid Prebuilt Models as a Service from Google Cloud Platform. However, \"Video-to-Text API\" is a fabricated term and does not represent an actual service, making it the correct answer for this question.",
    "point": 1,
    "timer": 90,
    "page": 309,
    "multiple_choices": false
  }
  
]